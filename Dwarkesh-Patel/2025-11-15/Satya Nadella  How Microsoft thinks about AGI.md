---
layout: default
title: "Satya Nadella – How Microsoft thinks about AGI"
channel: "Dwarkesh Patel"
date: 2025-11-12
---

# Satya Nadella – How Microsoft thinks about AGI

**频道**: Dwarkesh Patel
**发布日期**: 2025-11-12
**视频链接**: https://www.youtube.com/watch?v=8-boBsWcr5A

---

## Key Takeaways

## 一句话总结
微软CEO萨提亚·纳德拉深度解读微软在AGI时代的战略布局，强调构建灵活、可扩展的基础设施和平台，以支持多元化模型发展和赋能代理（Agents）的未来，并分享了对AI商业模式、技术演进及人才竞争的深刻见解。

## 内容概览
本视频是对微软CEO萨提亚·纳德拉的深度访谈，聚焦微软对AGI（通用人工智能）的战略思考。视频围绕微软最新、最强大的数据中心Fairwater 2展开，揭示了其在AI基础设施上的巨额投入和前瞻性规划。纳德拉深入探讨了AI技术对经济和社会带来的颠覆性变革，以及微软如何通过构建平台、赋能开发者和代理，来适应这一新时代。访谈还触及了AI商业模式的演变、模型竞争、人才争夺以及微软在AI领域的长期愿景。

## 核心要点

### 1. AGI：工业革命以来最重大的变革，但仍处早期
**背景/问题**: AGI的出现被认为是继工业革命以来最重大的技术变革，但其发展速度和实际落地仍有待观察。
**核心观点**: 纳德拉认为AGI是继工业革命以来最重大的事件，但同时强调目前仍处于“早期阶段”（early innings）。技术突破固然重要，但工程实现和规模化应用同样关键。
**实践启示**: 认识到AGI的巨大潜力，但也要保持务实态度，关注技术落地和实际效益，避免过度炒作。微软的战略是着眼于未来50年，而非仅仅5年。

### 2. 基础设施的战略核心：10x扩容与灵活支撑多元模型
**背景/问题**: 训练大型AI模型需要海量的计算资源，且模型技术迭代迅速，单一模型优化可能导致风险。
**核心观点**: 微软正以18-24个月10倍的速度扩充其AI训练能力，例如Fairwater 2数据中心的设计就是为支持比GPT-5更大的模型训练。关键在于构建能够支持“模型并行”（model parallelism）和“数据并行”（data parallelism）的、高度互联的基础设施，并且这种基础设施必须是通用的，能够支持多种模型和工作负载（训练、数据生成、推理），而非为单一模型优化。
**实践启示**: 投资于能够适应未来不确定性的、可扩展的基础设施是AI战略的关键。避免“赢家诅咒”（winner's curse），即过度投入优化单一模型，而忽略了技术快速迭代的风险。

### 3. 从终端用户工具到代理基础设施的转型
**背景/问题**: AI技术的发展将重塑人机交互和工作方式，传统的终端用户工具模式可能需要转型。
**核心观点**: 微软的业务将从“终端用户工具”（end-user tools）业务，转变为“代理”（agents）完成工作的“基础设施”（infrastructure）业务。这意味着微软将为AI代理提供支持其运行和协作的底层服务。
**实践启示**: 提前布局AI代理所需的基础设施和服务，将是未来微软增长的关键。关注如何为AI代理提供计算、存储、安全、身份验证和可观测性等核心能力。

### 4. 商业模式的多元化与订阅模式的演进
**背景/问题**: AI的高昂计算成本（COGS）对传统的SaaS（软件即服务）商业模式构成了挑战，需要新的盈利模式。
**核心观点**: 微软认为AI时代的商业模式将是多元化的，包括广告、交易、设备利润、以及订阅（消费者和企业）和按使用量付费（consumption）。订阅模式将从“消费权”（entitlement to consumption rights）演变为更精细化的定价，根据包含的消费量进行分级。
**实践启示**: 微软在所有这些商业模式上都有布局，并能将它们组合运用。关键在于通过提供增值服务和优化定价策略，来适应AI时代更高的运营成本和不断变化的用户需求。

### 5. 市场扩张与竞争格局：Copilot的案例
**背景/问题**: AI技术，尤其是代码助手（coding assistant），正在快速改变软件开发领域，并引发激烈的市场竞争。
**核心观点**: GitHub Copilot的成功证明了AI在特定领域的市场扩张能力。尽管竞争对手（如Claude, Cursor, CodeX）迅速涌现，但微软认为这是市场健康发展的标志。GitHub的核心价值在于其作为代码托管平台，能够吸引并服务于所有开发者，无论他们使用何种代码助手。
**实践启示**: 市场扩张是AI领域的重要驱动力。微软通过提供平台级服务（如GitHub），可以从整个生态系统的增长中获益，即使在特定产品（如Copilot）的市场份额受到竞争时。

### 6. GitHub Agent HQ：构建AI代理的协作平台
**背景/问题**: 未来AI代理将需要更复杂的协作、管理和指挥能力，以应对多代理任务和复杂工作流。
**核心观点**: 微软正在构建“Agent HQ”（Agent Headquarters），概念上类似于AI代理的“有线电视”（cable TV），将整合来自不同模型提供商的代理，提供统一的订阅、任务发布、监控和代码管理功能。这包括“Mission Control”等功能，允许用户通过一个界面管理和协调多个AI代理。
**实践启示**: 平台化和生态系统建设是AI代理时代的关键。微软通过GitHub Agent HQ，试图成为AI代理的“操作系统”或“控制平面”，为开发者提供集成化的AI代理开发和使用体验。

### 7. 基础设施层面的竞争与价值
**背景/问题**: 随着AI模型能力的增强，模型本身的重要性可能相对下降，而支撑模型运行的底层基础设施和平台的重要性将更加凸显。
**核心观点**: 微软认为，其核心业务将从“终端用户工具”转向“代理基础设施”。这意味着将为AI代理提供计算、存储、安全、发现、管理等核心服务。例如，Windows 365的增长就来自于AI代理需要为其配置独立的计算环境。
**实践启示**: 微软利用其在数据中心、操作系统、云服务等方面的深厚积累，为AI代理提供高效、安全的底层支撑。这种“计算资源即服务”的模式，将是AI时代基础设施层面的重要价值所在。

### 8. 模型与“脚手架”的价值分配
**背景/问题**: AI价值最终会流向模型提供商还是提供模型“脚手架”（scaffolding）的平台方？
**核心观点**: 纳德拉认为，价值分配将是模型与“脚手架”之间的一种博弈。他强调，如果微软能赢得“脚手架”的竞争，即在处理模型“打磨”（hobbling）和“参差不齐”（jaggedness）的智能问题上取得优势，将能实现垂直整合，因为拥有数据的“流动性”（liquidity）和“上下文工程”（context engineering）能力。同时，他承认开源模型将提供一个基础能力，使得数据和“脚手架”成为关键。
**实践启示**: 微软的策略是同时构建领先的模型能力（通过OpenAI合作和自研）以及强大的基础设施和应用层（“脚手架”）。通过数据优势和对底层技术的深刻理解，微软可以在模型和平台之间争取到有利的价值分配。

### 9. 微软AI战略：多模型、多平台、多层次
**背景/问题**: 如何在AI领域保持领先地位，尤其是在模型能力和人才竞争日益激烈的背景下？
**核心观点**: 微软的AI战略是多维度的。一方面，将最大化利用OpenAI的模型，并通过独有数据进行微调和增强。另一方面，将大力发展自研模型（如MAI），并在图像、音频、文本等领域取得突破。同时，微软将构建支持多种模型（包括开源模型、OpenAI模型以及自研模型）的基础设施，并在安全、知识工作、编码、科学等领域构建“模型优先”（model-forward）的应用。GitHub Agent HQ等平台将整合第三方模型。
**实践启示**: 微软采取的是一种“开放但有战略”（open but strategic）的模式，既拥抱合作，也积极投入自研，并致力于构建一个支持多样化AI生态系统的平台。

### 10. 人才竞争与长期愿景
**背景/问题**: AI领域的顶尖人才争夺激烈，如何吸引和留住世界级AI研究团队是关键。
**核心观点**: 微软已经组建了一个世界级的AI团队，包括Mustafa Suleyman、Karen Simonyan等，并明确了其AI实验室的长期目标。他们将继续在基础设施、模型能力和应用层进行投入，并为应对未来5-8年的技术突破做好准备。
**实践启示**: 微软认识到人才在AI创新中的核心作用，并正在积极吸引顶尖人才。其长期愿景是成为AI领域的领导者，不仅在基础设施和模型层面，更在推动AGI的实现上。

### 11. 持续学习与智能爆炸的潜在风险
**背景/问题**: 如果AI模型能够像人类一样在工作中持续学习并融合经验，可能引发“智能爆炸”。
**核心观点**: 纳德拉承认，如果存在一个被广泛部署、能看到所有数据并持续学习的模型，那将是“game set match”（胜负已定）。但他认为，现实情况是，即使在特定领域（如编码），也存在多种模型并在不同场景下被部署，就像数据库领域一样，不会出现单一模型统治所有领域的情况。
**实践启示**: 尽管存在智能爆炸的理论可能，但现实世界的复杂性意味着多元化和专业化将是常态。微软的策略是构建支持多种模型、跨领域、跨地域的灵活基础设施和平台，以应对这种不确定性。

## 关键概念与资源
**核心概念**:
*   **AGI (Artificial General Intelligence)**: 通用人工智能
*   **Industrial Revolution**: 工业革命
*   **Winner's Curse**: 赢家诅咒
*   **Commoditization**: 商品化
*   **Infrastructure Business**: 基础设施业务
*   **End-User Tools Business**: 终端用户工具业务
*   **Agents**: 代理（AI代理）
*   **Model Parallelism**: 模型并行
*   **Data Parallelism**: 数据并行
*   **Flops (Floating-point Operations Per Second)**: 每秒浮点运算次数
*   **Optics**: 光学器件（指数据中心网络连接）
*   **Network Topology**: 网络拓扑
*   **Scalability**: 可扩展性
*   **Scaling Laws**: 规模法则
*   **Guardian Angel / Cognitive Amplifier**: 守护天使 / 认知放大器（AI的两种比喻）
*   **SaaS (Software as a Service)**: 软件即服务
*   **COGS (Cost of Goods Sold)**: 销货成本
*   **RPO (Revenue Per Occupant)**: 每用户平均收入
*   **Ramp**: 爬坡（业务增长曲线）
*   **TAM (Total Addressable Market)**: 总潜在市场
*   **GitHub Copilot**: 微软的AI代码助手
*   **Agent HQ**: 微软构想的AI代理协作平台
*   **Mission Control**: Agent HQ中的一个功能模块
*   **Scaffolding**: 脚手架（指支撑AI模型运行的平台和工具）
*   **Liquidity of Data**: 数据流动性
*   **Context Engineering**: 上下文工程
*   **Middle Tier**: 中间层（指软件架构中的中间层）
*   **Business Logic**: 业务逻辑
*   **Cognitive Layer**: 认知层
*   **Per User Business**: 按用户计费的业务
*   **Per Agent Business**: 按代理计费的业务
*   **Observability**: 可观测性
*   **Identity System**: 身份系统
*   **Hybrid World**: 混合世界（指人类与AI协作的环境）
*   **Primitives**: 原语（指基础功能模块）
*   **EisDiscovery**: 电子证据发现（推测，与数据管理相关）
*   **MAI (Microsoft AI)**: 微软自研AI模型系列
*   **RL Fine-tuning**: 强化学习微调
*   **Image Arena**: 图像模型评测平台
*   **LM (Language Model)**: 语言模型
*   **Omni Model**: 全能模型（整合多种模态的模型）
*   **Super Intelligence**: 超级智能
*   **Hyperscale**: 超大规模（指云基础设施）
*   **Frontier Class Model**: 领先模型
*   **Vertical Integration**: 垂直整合
*   **ISV (Independent Software Vendor)**: 独立软件供应商
*   **API (Application Programming Interface)**: 应用程序接口
*   **Intelligence Explosion**: 智能爆炸
*   **Network Effects**: 网络效应

**工具/技术**:
*   **Fairwater 2 Data Center**: 微软最新的数据中心
*   **GPT family**: OpenAI的GPT模型系列
*   **MAI models**: 微软自研模型
*   **GitHub**: 代码托管平台
*   **VS Code**: 微软的代码编辑器
*   **Windows 365**: 微软的云PC服务
*   **Office 365 / Microsoft 365**: 微软的办公套件
*   **Excel Agent**: 微软在Excel中集成的AI代理
*   **H100s**: NVIDIA的GPU型号
*   **OpenAI Models**: OpenAI提供的AI模型
*   **Anthropic Models**: Anthropic提供的AI模型
*   **Claude**: Anthropic的模型
*   **Cursor**: AI代码助手
*   **CodeX**: AI代码助手
*   **Grok**: xAI的模型
*   **Libox**: 提供高质量AI训练数据

**推荐资源**:
*   **Raj Reddy**: 图灵奖得主，AI领域的先驱
*   **Dylan Patel (SemiAnalysis)**: 访谈的联合主持人，半导体分析师
*   **Darkest's podcast**: 提及的播客节目，可能与AI和科技话题相关

## 目标受众
**最适合**:
*   **科技公司高管和战略决策者**: 尤其是那些负责AI战略、基础设施建设、产品开发和商业模式创新的决策者。
*   **AI领域的研究人员和工程师**: 了解行业巨头的战略方向，为技术研发和产品设计提供参考。
*   **对AI未来发展趋势感兴趣的投资者和分析师**: 把握AI领域的重要投资机会和市场动态。
*   **对微软AI战略有深入了解需求的开发者和合作伙伴**: 理解微软在AI生态系统中的定位和合作机会。

**价值场景**:
*   **战略规划会议**: 为制定公司AI发展路线图和投资决策提供洞察。
*   **技术趋势研讨**: 了解AGI时代基础设施、模型和平台的核心发展方向。
*   **商业模式创新讨论**: 探索AI时代新的盈利模式和价值分配机制。
*   **了解竞争格局**: 分析主要科技公司在AI领域的竞争策略和优势。

## 延伸思考
1.  **AI代理的“操作系统”之争**: Agent HQ等平台是否会成为AI代理的“操作系统”，微软能否在此领域建立类似Windows或Android的生态优势？其他云厂商和AI公司将如何应对？
2.  **模型迭代与基础设施的“军备竞赛”**: 随着模型能力指数级增长，对算力基础设施的需求将永无止境。微软等巨头如何平衡基础设施投入的规模化与技术迭代的灵活性，以避免“落后一步”的风险？
3.  **AGI的价值分配与社会影响**: 如果AGI真的实现，其创造的巨大价值将如何分配给模型提供商、基础设施提供商、平台方以及社会大众？这对经济结构、就业市场和社会公平将带来哪些深远影响？

---

## 中文文稿

好的，这是为您转录和编辑的中文文稿：

### AGI 时代的微软愿景

**主持人**：或许在工业革命之后，这是最重大的事件了。但同时，我也清楚地认识到，我们仍处于早期阶段。如果你们是一家模型公司，可能会面临“赢家诅咒”。你们可能付出了艰辛的努力，进行了难以置信的创新，但最终却可能因为一个复制品就导致技术被商品化。

**主持人**：我们不想仅仅成为一家公司的托管平台，只与一个客户建立庞大的业务往来。那不是一个可持续的商业模式。你无法为单一模型构建一个优化的基础设施。如果那样做了，一旦出现某个突破性的进展，你的整个网络拓扑结构就会瞬间失效。这确实令人担忧。

**主持人**：我们今天的业务是面向终端用户的工具业务，但未来它将主要转变为支持智能体（agents）工作的基础设施业务。我们必须考虑的不是未来五年会做什么，而是未来五十年会做什么。

**主持人**：今天我们采访的是 Satya Nadella。采访者是我，以及 SemiAnalysis 的创始人 Dylan Patel。Satya，欢迎您。

**Satya Nadella**：谢谢。很高兴来到亚特兰大。

**主持人**：是的，感谢您带我们参观了新的设施，这里非常酷。

**Satya Nadella**：绝对的。

**主持人**：Satya 和微软云与 AI 执行副总裁 Scott Guthrie 带领我们参观了他们全新的 Fairwater 2 数据中心，这是目前世界上最强大的数据中心。

**Satya Nadella**：我们试图在每 18 到 24 个月内将训练能力提升 10 倍。这次升级将是 GPT-5 训练能力的 10 倍。为了让大家有个概念，这个建筑中的网络光模块数量，几乎相当于我们两年前所有 Azure 数据中心网络光模块的总和。

**主持人**：这大约是 500 万个网络连接。

**Satya Nadella**：您拥有了不同区域之间以及两个区域之间巨大的带宽。那么，这是对未来大规模扩展的巨大投入吗？您是否预见到未来会有某个巨大的模型需要两个不同的区域来训练？

**Satya Nadella**：目标是能够为大型训练任务聚合这些浮点运算（flops），并将它们跨站点整合在一起。

**主持人**：没错。

**Satya Nadella**：实际上，你会用它进行训练，然后用于数据生成（data gen），用于推理（inference），以及各种其他方式。它不会永远只用于一种工作负载。

**主持人**：您提到附近还在建设中的 Water 4。

**Satya Nadella**：是的，我们也将为它配备 100 PB 的网络。

**主持人**：嗯。

**Satya Nadella**：这样我们就能以非常高的速率将两者连接起来。然后，我们将通过 IWAN 连接到密尔沃基，我们在那里有多个其他的 Fairwater 数据中心正在建设。

**Satya Nadella**：你可以看到模型并行（model parallelism）和数据并行（data parallelism）。它基本上是为校园内的训练任务、pod 和 super pods 而设计的。通过广域网（WAN），你可以连接到威斯康星州的数据中心，并真正地运行一个训练任务，所有这些资源都能被聚合起来。

**主持人**：我们现在看到的是一个还没有安装服务器和机架的单元（cell）。

**Satya Nadella**：一个单元有多少个机架？

**Satya Nadella**：让我想想。我们通常不直接分享这些数据，但是……

**主持人**：我之所以问，是因为你们 upstairs 可以开始计算了。我们可以让你们计算一下这个建筑里有多少个单元。

**Satya Nadella**：这部分我也不能透露。不过，计算单元数量很简单，对吧？

[音乐]

**主持人**：天哪，声音真大。

**主持人**：您看到这个，是不是觉得“我明白我的钱花在哪里了”。

**Satya Nadella**：这就像我经营一家软件公司。欢迎来到软件公司。

**主持人**：一旦你们决定使用 GB200 和 NVIL，设计空间有多大？还有多少其他决策需要做出？模型架构与物理规划的优化之间存在耦合关系。

**Satya Nadella**：而且这也很令人担忧，因为会有新的芯片出现。比如 Vera Rubin Ultra，它的功耗密度将截然不同，冷却需求也将完全不同。所以，你不能只按照一个规格来建造一切。这又回到了我们稍后要讨论的话题：

**Satya Nadella**：你想实现的是“随时间扩展”（scaling in time），而不是“一次性扩展”（scale once）然后就被固定住了。

**主持人**：回顾过去所有重大的技术转型，无论是铁路、互联网、可替换零件和标准化，还是云计算，每一次革命从技术发现到普及和渗透到经济中的速度都越来越快。许多参加 Dorkesh 播客的嘉宾认为，这是最后一次技术革命或转型，这次非常非常不同。至少到目前为止，在市场上，我们已经看到在三年内，超大规模云服务商（hyperscalers）明年的资本支出将达到 5000 亿美元，这个规模在速度上是前所未有的，而且最终状态似乎也大相径庭。您是如何看待的？您的看法似乎与那些认为 AGI 即将到来、并且对此非常兴奋的“AI 狂热者”的观点截然不同。我希望能更多地了解您的想法。

**Satya Nadella**：是的。我的看法是，我首先感受到的是那种兴奋感，就像在工业革命之后，这是最重大的事件。所以，我从这个前提开始。但同时，我也清楚地认识到，我们仍处于早期阶段。我们已经构建了一些非常有用的东西，看到了出色的特性。这些规模法则（scaling laws）似乎正在奏效，我乐观地认为它们会继续奏效。其中一些……这需要真正的……

好的，这是根据您提供的英文视频字幕转录并整理成的中文文稿：

### 计算的演进与AI的价值

科学的突破固然重要，但其中也包含了大量的工程技术和实践。即便如此，我仍然认为，过去七十年来计算领域的进步，也是一场推动我们前进的宏大进程。

正如我所说，我非常欣赏Raj Reddy提出的关于人工智能（AI）的比喻。他是一位来自卡内基梅隆大学（CMU）的图灵奖得主。他一直以来（甚至在通用人工智能（AGI）出现之前）都认为，AI应该成为我们的“守护天使”或“认知放大器”。我非常喜欢这个比喻，它简单地概括了AI的本质：它最终的“人类效用”是什么？它将成为一个认知放大器，同时也是一个守护天使。

从这个角度来看，我将其视为一种工具。当然，你也可以对此进行一些“神秘化”的解读，认为它“不仅仅是一个工具”，它能做许多过去只有人类才能做的事情。但事实上，许多技术在过去都经历过类似的过程。过去很多事情都只能由人类完成，然后我们发明了工具来帮助我们。

### “Satya Tokens”的经济价值

或许我们不必纠结于具体的定义，但可以这样设想：也许需要五年、十年、二十年，最终机器能够生成“Satya Tokens”（萨提亚代币）。而微软董事会认为这些“Satya Tokens”非常有价值。

那么，通过采访萨提亚（Satya），你们浪费了他多少经济价值呢？

你无法承担“Satya Tokens”的API成本。所以，无论你如何称呼它——“Satya Tokens”是工具还是代理（agent）——目前，如果模型的成本是每百万个token几美元或几美分，那么其利润空间就非常巨大。

“Satya Tokens”的价值何在？这个利润空间会流向何处？以及微软会在多大程度上参与其中？这是我的疑问。我认为，在某种意义上，这又回到了经济增长的图景将如何展开的问题。企业将呈现何种形态？生产力将如何变化？

对我而言，这再次印证了，就像工业革命在经过了大约七十年的普及后才开始显现经济增长一样。这同样是需要记住的一点：即使技术传播速度很快，但要真正出现经济增长，它必须扩散到一定程度，以至于工作内容和工作流程都必须发生改变。

因此，我认为企业内部进行真正的变革管理是不可忽视的。那么，未来人类及其产生的token（代币）是否会获得更高的杠杆作用？无论是“Dark Cesh”还是未来的“Dylan Tokens”。你想想，如果没有技术，你能进行半导体分析或运行这个播客吗？完全不可能，对吧？你所能达到的规模，是无法想象的。

所以，问题是，这种规模能达到多少倍？通过某种新技术的出现，它是否会翻倍？绝对会。因此，在你的收入增长曲线或受众增长曲线中，我认为这就是将要发生的事情。

换句话说，工业革命用了大约七十年才完成的事情，或许在未来二十到二十五年内就能完成。如果幸运的话，我希望将工业革命两百年的历程压缩到二十年内完成。

### 微软的商业模式转型与AI成本

微软历史上一直是，或许是最大的软件公司，最大的软件即服务（SaaS）公司。你们经历过一次转型，从销售Windows光盘和Microsoft Office许可，转变为如今销售Microsoft 365的订阅服务。

从那次转型到你们今天的业务，还有一个新的转型正在发生，对吧？软件即服务（SaaS）的边际用户成本极低，虽然研发投入巨大，客户获取成本也很高。这就是为什么（并非特指微软，而是指SaaS公司）在市场上表现不佳，因为AI的单位成本（COGS）非常高，这完全打破了这些业务模式的运作方式。

作为一家伟大的软件公司，一家SaaS公司，您将如何帮助微软转型到这个单位成本（COGS）至关重要，且边际用户成本截然不同的新时代？因为现在你们对Copilot的收费是每月20美元。

### AI时代的商业模式与定价策略

这是一个非常好的问题。从某种意义上说，我认为业务模式本身的杠杆会保持相似。如果我看看从消费者到更广泛范围的模型菜单，其中会有广告单元、交易、AI设备制造商的设备毛利、消费者和企业的订阅服务，以及按使用量付费。

所以，我认为这些都是衡量指标。至于你的观点，什么是订阅？到目前为止，人们喜欢订阅，因为他们可以为此进行预算。它们本质上是某些消费权利的授权，这些权利被封装在订阅中。

因此，我认为这在某种程度上会成为一个定价问题。你被授权的消费量是多少？如果你看看所有的编码订阅服务，它们大致就是这样运作的，对吧？它们有专业版、标准版等等。所以我认为这就是定价和利润结构将如何分层的方式。

好的，这是根据您提供的英文视频字幕转录并编辑后的高质量中文文稿：

### 微软的AI战略

### 拥抱全方位业务模式

有趣的是，微软恰好处于这样一个业务领域。对我们来说，好消息是我们已经全面投入，并覆盖了所有这些领域。事实上，从产品组合的层面来看，我们几乎拥有所有其他消费类业务的消耗型订阅服务。

时间会告诉我们，哪些模式在哪些领域会更具优势。关于您提到的SaaS（软件即服务）方面，我思考得比较多。以Office 365或Microsoft 365为例。拥有较低的恢复点目标（RPO）非常棒，因为这里有一个有趣的现象。

### 云迁移的经验教训

在从本地服务器迁移到云端的过程中，我们曾问自己一个问题：如果我们只是将那些使用我们Office许可和本地服务器的用户迁移到云端，我们的运营成本（COGS）会怎样？这不仅会压缩我们的利润率，还会让我们变成一个盈利能力大幅下降的公司。

然而，事实恰恰相反。迁移到云端极大地拓展了市场。例如，我们在印度曾售卖过一些服务器，但销量并不大。而在云端，印度的每个人都负担得起按比例购买服务器的费用。IT成本方面，我之前未曾意识到的一个重要方面是，人们在SharePoint底层存储上花费了多少钱。事实上，EMC（易安信）最大的业务板块可能是为SharePoint提供的存储服务器。

所有这些在云端都大大减少了，因为没有人需要去购买。这实际上是营运资金的占用，意味着现金流出。因此，云端极大地拓展了市场。

### AI驱动的市场扩张

AI（人工智能）也将带来类似的影响。以编程为例，我们通过GitHub和VS Code经过数十年积累的技术，现在突然间，编程助手在一年内就取得了如此巨大的成就。我认为，AI也将带来市场的大规模扩张。

### 竞争格局与微软的优势

那么，问题来了：市场会扩张，但触及微软的收入部分是否也会随之扩张？Copilot（微软的AI编程助手）就是一个例子。今年早些时候，根据Dylan的数据，GitHub Copilot的收入大约是5亿美元。当时几乎没有直接竞争对手。

而现在，Claude、Code Cursor和Copilot的收入都接近10亿美元，Codeium（字幕中误写为Codeex）也以7-8亿美元紧随其后。因此，问题在于，在微软能够触及的所有服务中，微软的Copilot等同类产品拥有怎样的优势？

### 创新者的崛起与微软的地位

是的，我非常喜欢这张图表。我喜欢它的原因有很多。首先，我们仍然处于领先地位。其次，图表上列出的所有公司都是在过去四到五年内成立的。对我来说，这是最好的信号。

这意味着我们面临着新的竞争对手和新的生存挑战。当你想到“天哪，现在是Claude要打败我们了，Cursor要打败我们了”，而不是像过去那样担心Borland（一个老牌软件公司）时，你就知道我们走在正确的方向上。

我们从零开始发展到现在的规模，这本身就代表了市场的扩张。这就像云服务一样，编程和AI这个品类，很可能成为最大的品类之一。它是一个“软件工厂”的品类，甚至可能比“知识工作”的品类还要大。

### 保持开放心态与持续竞争

我希望保持开放的心态。我们将面临激烈的竞争，我认为这是您提出的一个非常好的观点。但我们很高兴能够将我们已有的优势转化为现在的局面，现在我们需要去竞争。

在竞争方面，即使在上个季度，我们刚刚公布了季度财报，我们的订阅用户从2000万增长到了2600万。我对我们的订阅用户增长和发展方向感到满意。

### GitHub的战略价值

但更令人兴奋的是，所有那些生成海量代码的其他公司的代码库都去了哪里？它们都去了GitHub。因此，GitHub在代码库创建、PR（Pull Request）等方面都达到了历史新高。从某种意义上说，我们希望保持这种开放性。

这意味着我们希望拥有这种优势，因为我们不希望将其与我们自身的增长混为一谈。有趣的是，我们每秒钟就有一名开发者加入GitHub，这是我记得的一个数据。其中80%的人会直接进入某种GitHub Copilot的工作流程，仅仅因为它们可用。

而且，其中许多人甚至会使用我们的一些代码审查代理，这些代理默认开启，因为它们可以被使用。因此，我们将拥有许多结构性的机会。

### GitHub的未来发展方向

我们还将做的是，就像我们当初对Git所做的那样，围绕Git、Issues（问题跟踪）和Actions（自动化工作流）等GitHub的核心功能进行扩展。这些都是强大而优秀的功能，因为它们都围绕着你的代码库构建。

我们希望扩展这一点。上周在GitHub Universe（GitHub年度开发者大会）上，我们正是这样做的。我们提出了“Agent HQ”（智能体总部）的概念，并表示将构建它。

### Agent HQ：AI智能体的聚合平台

例如，你可以在Agent HQ中找到一个名为“Mission Control”（任务控制中心）的工具。通过Mission Control，我可以启动一系列AI智能体。我有时会将其描述为所有AI智能体的“有线电视”。

因为我将把Codeium、Claude、Cognition Staff（可能指代某些AI公司或服务）以及任何人的智能体都打包到一个订阅中。我将获得一个统一的套餐，然后我可以真正地分配任务，指导它们。它们将各自在独立的分支上工作。我可以监控它们。

我拥有这一切，因为我认为这将是最大的创新领域之一。因为现在我希望能够使用多个智能体。我希望能够……

好的，这是根据您提供的英文视频字幕转录并整理的高质量中文文稿：

### AGI的未来与微软的战略

**问**: 能够消化多个智能体（agents）的输出，并能够掌控我的代码仓库（repo）。如果需要一个抬头显示器（heads-up display），让我能够快速地引导和分类由代码智能体生成的代码，这对我来说至关重要。

**答**: 这将整合VS Code、GitHub以及我们将构建的所有新基础组件，成为一个“任务控制中心”（mission control）。我认为，通过一个控制平面（control plane）进行可观测性（observability）的管理，对于所有部署这些技术的人来说都是必需的。他们需要了解在什么时间，哪个智能体对哪个代码库进行了操作。

### 竞争与创新

**问**: 那么，这是否意味着我们必须保持竞争力并不断创新？如果做不到，我们是否会被取代？

**答**: 您的观点非常正确。我们必须保持竞争力并不断创新。如果做不到，我们确实会被取代。但我喜欢这张图表，至少在目前我们处于领先地位的时候，即使面对竞争。

### GitHub与AI编码智能体的增长

**问**: 关键在于，无论哪个编码智能体获胜，GitHub都会持续增长。但这个市场本身只以每年10%、15%或20%的速度增长，这远高于GDP。它是一个很好的复合增长的市场。然而，AI编码智能体市场的增长速度惊人。

**答**: 没错。去年年底，AI编码智能体的年收入（run rate）约为5亿美元，这基本上就是GitHub Copilot。而现在，包括GitHub Copilot、Cloud Code、Cursor、Cognition、Wind Surflet、Codex、Open Codex等在内的所有AI编码智能体，目前的年收入已达到560亿美元。这在今年第四季度是10倍的增长！

### 软件智能体市场规模

**问**: 当我们审视软件智能体（software agents）的潜在市场规模（TAM）时，它是否是人们工资的2万亿美元，还是超越了这个范畴？因为现在几乎所有公司都能够更有效地开发软件。

**答**: 毫无疑问，微软会从中分一杯羹。但您看到的是，在短短一年内，我们的市场份额已经从接近100%（或者说远高于50%）下降到25%以下。

### 微软的信心来源

**问**: 那么，人们对微软的信心从何而来？

**答**: 这又回到了原点：我们没有任何“与生俱来的权利”。我们的信心来自于我们必须不断创新。我们幸运的是，这个新类别（AI编码智能体）的市场将比我们过去拥有高份额的任何市场都要大得多。

换句话说，我们可以说我们在VS Code和GitHub的代码仓库方面拥有很高的市场份额，这是一个很好的市场。但关键在于，即使在这个更广阔的市场中拥有一定的份额，也是有意义的。

我们可以说，我们曾经在客户端-服务器计算（client-server computing）领域拥有高份额，而在超大规模计算（hyperscale）领域，我们的份额要低得多。但超大规模计算是一个数量级上大得多的业务。

所以，至少有存在的证据表明，即使我们的市场份额不如以往，只要我们竞争的市场创造了更多价值，并且有多家赢家，微软也能发展得很好。

因此，我认为这就是关键。但我理解您的观点，最终我们必须变得更具竞争力。我每个季度都会关注这一点。

### GitHub HQ与Agent HQ的愿景

**答**: 这也是为什么我对我们将要推出的GitHub HQ或Agent HQ感到非常乐观。我们将把GitHub打造成所有这些智能体汇聚的地方。正如我所说，我们将有多个尝试的机会。

不一定非要我们一家独占，有些公司可以和我们一起成功。所以，不一定只有一个赢家和一个订阅模式。

### AI对Office等软件的影响

**问**: 我认为关注这个问题的根本原因在于，这不仅关乎GitHub，更关乎Office以及微软提供的所有其他软件。关于AI如何发展，有一种设想是：模型会不断被改进，届时你需要持续的、可见的全局监控。

另一种设想是，随着时间的推移，模型将能够执行更复杂的任务。它们现在可以完成需要两分钟的任务，未来可能完成需要10到30分钟的任务，甚至可能自主完成数天的工作。

届时，模型公司可能会收取高昂的费用，可能数千美元，来获取一个可以与人类通过任何用户界面（UI）进行交流、跨平台迁移的“协作者”（co-worker）。

如果我们正朝着这个方向发展，那么为什么那些模型公司会越来越赚钱，并且占据所有利润，而那些提供“脚手架”（scaffolding）的平台，随着模型能力的增强而变得越来越不重要，却仍然如此重要呢？这涉及到现在的Office，以及那些仅仅从事知识工作的“协作者”。

### 价值分配与模型商品化

**答**: 这是个非常好的观点。我认为，问题在于价值是否会完全流向模型，还是会分配给“脚手架”和模型之间。时间会证明一切。

但我认为，激励结构会变得清晰。如果我们以信息工作或编码为例，事实上，我在GitHub Copilot中最喜欢的功能之一就是“自动优化”（auto-optimize）。我购买了订阅，它会开始选择并优化我要求它做的事情。

它甚至可以是完全自主的，并且可以利用多个模型的代币（tokens）来完成任务。如果这是趋势，那么模型，尤其是开源模型，将变得商品化。你可以选择一个检查点（checkpoint），带入你的数据，你已经看到了。

我认为我们都会开始看到，无论是通过Cursor还是Microsoft，你都会开始看到一些内部模型。

好的，这是根据您提供的英文视频字幕转录并整理成的中文文稿：

### 赢家通吃还是生态共荣？

**问**：那么，您认为谁将最终胜出？是模型公司还是那些构建在模型之上的应用开发者？

**答**：我认为，如果你赢得了“脚手架”（scaffolding），也就是今天处理那些棘手问题和智能碎片化挑战的部分，那么你就能在模型中实现垂直整合。这是因为你拥有数据的流动性，并且有足够多的检查点（checkpoints）可用。

从结构上看，我认为世界上总会存在一个相当强大的开源模型。只要你拥有与之匹配的数据和脚手架，就可以利用它。我可以这样论证：如果你们是一家模型公司，你们可能会面临“赢家诅咒”（winner's curse）。你们付出了巨大的努力，进行了令人难以置信的创新，但可能只是一份副本的距离，就会被商品化。而那些拥有用于数据锚定（grounding）和上下文工程（context engineering）以及数据流动性的公司，就可以拿走那个检查点并进行训练。所以，我认为这两种观点都有其道理。

### 模型 vs. 脚手架：两种世界观

**问**：您刚才的论述揭示了两种看待世界的方式，对吗？一种观点认为，市面上有如此多的不同模型，开源也已存在。模型之间的差异将决定谁胜谁负，但最终是脚手架让你脱颖而出。

**答**：另一种观点则认为，模型本身才是关键的知识产权（IP）。是的，我们正处于一场激烈的竞赛中，每个人都在争夺领先地位。你可以看到，像Anthropic或OpenAI这样的公司，它们的收入图表就能说明问题。例如，OpenAI的收入在拥有与Anthropic相似（尽管方式不同）的代码模型能力后开始飙升。

有一种观点认为，模型公司才是真正获得所有利润的公司。因为，如果你看看今年，至少在Anthropic方面，它们在推理（inference）上的毛利率（gross margins）从远低于40%增长到年底的60%以上。尽管市面上有越来越多的中国开源模型，OpenAI具有竞争力，Google也很有竞争力，X Grok现在也很有竞争力，所有这些公司都已具备竞争力，但模型层的利润却显著增长。

### 微软的AI战略

**问**：您如何看待这个问题？

**答**：这是一个很好的问题。我想说的是，几年前人们可能还在说，我只需要包装一个模型就能建立一家成功的公司。我认为这种想法已经被证伪了，这主要是由于模型的能力，特别是工具使用（tools use）方面的进步。

但有趣的是，当我审视Office 365时，即使是我们构建的这个名为Excel agent的小工具。Excel agent并不是一个UI层面的包装器。它实际上是一个位于中间层（middle tier）的模型。在这种情况下，因为我们拥有GPT系列的所有知识产权，我们将其置于Office系统的核心中间层，从而教会它原生理解Excel的含义，以及其中的一切。

这意味着它不仅仅是像素级别的理解，而是对Excel所有原生工件（native artifacts）的全面理解。因为如果你考虑一下，如果我要给它一个推理任务，我甚至需要修正我犯的推理错误。这意味着我不仅需要看到像素，还需要能够看到“哦，我那个公式错了”，并且我需要理解这一点。在某种程度上，这一切都不是在带有某些提示（prompt）的UI包装器层面完成的，而是在中间层通过教授它Excel的所有工具来完成的。

换句话说，我实际上是在用Markdown来教授它成为一个复杂的Excel用户的技能。这有点奇怪，它又回到了AI大脑（AI brain）的概念。你构建的不仅仅是Excel，你现在是在构建传统的业务逻辑。你将Excel的传统业务逻辑，并为其包装一个认知层（cognitive layer），使用这个知道如何使用工具的模型。所以，从某种意义上说，Excel将自带一个内置的分析师，并具备所有工具的使用能力。

**问**：这是每个人都会构建的那种东西。所以，即使是模型公司，它们也必须竞争。如果它们定价过高，那么，如果我是一个像这样的工具开发者，我就会选择替代品。我可能会用你一段时间，但只要存在竞争，就总会有赢家通吃的情况。如果存在一个模型比其他所有模型都领先很多，是的，那就是赢家通吃。

**答**：只要存在竞争，并且有多个模型，就像超大规模（hyperscale）竞争一样，并且有一个开源的检查点，那么就有足够的空间在模型之上构建价值。

在微软，我们看待这个问题的方式是，我们将处于超大规模业务中，这将支持多个模型。我们将在未来七年内拥有OpenAI模型的访问权限，并在此基础上进行创新。我基本上认为我们自己拥有一个前沿模型（frontier class model），我们可以对其进行全面灵活的创新。我们也将构建自己的模型，通过Microsoft AI（Mai）。所以，我们总会有一个模型层，然后我们将构建这些应用脚手架，无论是安全、知识工作、编码还是科学领域。我们将构建我们自己的、模型优先（model forward）的应用脚手架。它不会是模型的包装器，而是模型本身将是核心。

好的，作为一名资深内容编辑和翻译专家，我将为您将英文视频字幕转录为高质量、易读的中文文稿，并遵循您提出的所有要求。

---

### AI 能力的未来构想

我有很多关于您提到的其他事情的问题，但在我们继续探讨这些话题之前，我仍然想知道，这是否是对 AI 能力的一种“非前瞻性”的设想？您是否在想象如今已存在的模型，它们可以截获屏幕截图，但却无法深入查看单元格中的公式？

我认为更合适的设想是，这些模型实际上将能够像人类一样使用计算机。就像使用 Excel 的人类知识工作者可以查看公式、使用其他软件，甚至在必要时迁移 Office 365 和其他软件之间的数据一样。这就是我所说的。

### AI 作为分析师的工具

如果真是这样，那么与 Excel 的集成似乎就没有那么重要了。毕竟，Excel 最初是为分析师设计的。那么，这个 AI 分析师应该拥有像人类分析师一样的工具，对吧？计算机就是他们的工具。

所以，我的意思是，我正在构建一个本质上是 AI 代理的分析师，它碰巧预先了解如何使用所有这些分析工具。

### 对话的清晰界定

**问：** 只是为了确保我们谈论的是同一件事，这是否意味着像我这样使用 Excel 的人类，可以完全自主地进行播客？

**答：** 想象一下，我们现在应该勾勒出我对公司未来的设想。公司的未来将是工具业务。我拥有一台电脑，我使用 Excel，而且在未来，我甚至会有一个副驾驶（co-pilot），这个副驾驶也会有代理。那仍然是我在掌控一切，所有东西都反馈回来。这是一种情况。

然后是第二种情况：公司直接为 AI 代理配置计算资源，并且该代理完全自主地工作。这个完全自主的代理将拥有与人类可用的相同工具集。AI 工具也拥有不仅仅是原始计算机，因为使用工具来完成工作会更具代币效率。

### 从工具到基础设施的转变

事实上，我将我们今天的业务，即终端用户工具业务，看作是支持代理工作的**基础设施业务**。还有其他思考方式吗？

**问：** 如果我们看到您们在 M365 底层构建的所有东西仍然会非常相关，您需要一个地方来存储它，一个地方来进行归档，一个地方来进行发现，一个地方来管理所有这些活动，即使您是一个 AI 代理。

**答：** 所以，这是一种新的基础设施。

### 微软软件的普适性与效率

**问：** 只是为了确保我理解您的意思。您是在说，理论上，一个未来拥有实际计算机使用能力的 AI（这是所有公司，包括模型公司，目前都在努力的方向），即使它没有与微软合作或在我们旗下，也可以使用微软的软件？但您说的是，如果我们使用我们的基础设施，我们将提供更低级别的访问权限，使其更有效率，让您能够做那些原本可以做的事情。

**答：** 100%。我的意思是，整个事情，事实上，就像我们经历的那样，我们有服务器，然后有了虚拟化，然后有了更多的服务器。所以，这是另一种思考方式：不要把工具看作是最终的东西。而是看工具背后支撑人类使用的整个底层架构。而整个底层架构也是 AI 代理的启动平台，因为 AI 代理需要一台计算机。

### Windows 365 的新机遇

**问：** 这就是其中之一。你知道，我们看到一个非常显著的增长是所有那些从事 office 相关的产品和自主代理的人，他们真的想配置 Windows 365。他们真的想为这些代理配置一台计算机。

**答：** 绝对是。我认为我们将拥有一个终端用户计算基础设施业务，而且我认为它将持续增长，因为它的增长速度将快于用户数量的增长。

### 用户与代理的业务模式

事实上，这是人们问我的另一个问题：用户业务会怎样？至少早期迹象表明，用户业务的思考方式不只是“按用户”，而是“按代理”。如果您将其视为“按用户和按代理”，那么关键在于为每个代理配置什么？是计算机、一套围绕它的安全措施、一个身份标识，以及所有这些可观察性和管理层。我认为所有这些都将融入其中。

### AI 在数据迁移中的价值

**问：** 我目前的想法是，这些模型公司都在构建环境来训练他们的模型使用 Excel、Amazon 购物，或者任何东西，比如预订航班。但同时，它们也在训练这些模型进行迁移，因为这可能是最直接的价值所在。将基于大型机（mainframe）的系统转换为标准的云系统，将 Excel 数据库转换为真正的 SQL 数据库，或者将 Word 和 Excel 中完成的工作转换为更具程序化、更有效率的经典方式，这些都可以由人类完成，只是对于软件开发者来说成本效益不高。这似乎是未来几年大家都会利用 AI 来创造巨大价值的方式。

**问：** 微软在其中扮演什么角色？

---

好的，这是根据您提供的英文视频字幕转录并整理的高质量中文文稿：

### AGI 的未来与混合世界

如果模型能够利用工具本身进行迁移，那么是的，微软在数据库、存储以及所有其他领域都处于领先地位。但就像大型机生态系统的使用可能会显著减少一样，Office 生态系统的使用也将显著减少。

大型机在过去二十年里一直在增长，尽管现在没有人谈论它们了，但它们的规模仍然增长了 100%。我同意这一点。那么，这种趋势将如何发展呢？

归根结底，这不是关于“嘿，我们将有一个显著的时期，存在一个混合世界”。因为人们将使用那些能够与需要使用工具的代理协同工作的工具。顺便说一句，它们必须相互沟通。我生成的、需要人类查看的产物是什么？所以，所有这些都将是任何地方的实际考量。因此，输出和输入。我不认为这仅仅是“哦，我把它迁移走了”。

但底线是，我必须生活在这个混合世界中。但这并没有完全回答你的问题，因为可能存在一个真正高效的新前沿，即只有代理与代理协同工作，并且完全优化。即使代理与代理协同工作，需要什么基础原语呢？你需要一个存储系统吗？该存储系统需要具备数据发现（data discovery）功能吗？该数据发现需要具备可观测性（observability）吗？你需要一个身份系统，能够使用多个模型，并且所有模型都拥有一个身份系统吗？

这些都是我们今天为 Office 系统或其他系统提供的核心底层基础设施。我认为未来也会如此。您提到了数据库，对吧？我的意思是，天哪，我希望所有的 Excel 都拥有一个数据库后端，对吧？事实上，我希望这一切能立即发生。而且这个数据库是一个好数据库。事实上，数据库将是一件大事，并且会蓬勃发展。

事实上，如果我想到所有的 Office 文档都结构化得更好，由于代理的作用，能够更好地连接结构化和非结构化数据，这将促进底层基础设施业务的增长。这种增长的消费完全由代理驱动。你可以说，所有这些都是由一个模型公司即时生成的软件。这也有可能是真的，我们也将是这样一家模型公司。因此，竞争可能在于我们将构建模型加上所有基础设施并提供服务，然后将会有许多能够做到这一点的公司展开竞争。

### 微软的模型战略与竞争

嗯，我猜说到模型公司，你说“好吧，我们也将是其中之一，不仅拥有基础设施，还将拥有模型本身”。现在，微软 AI 最近发布的模型是 36 和 Shabbat arena，已经发布了两个月了。而且，你显然拥有 OpenAI 的知识产权。所以有一个问题是，首先，你是否同意（OpenAI 的模型）似乎落后了？为什么会这样？特别是考虑到你理论上可以像分叉（fork）OpenAI 的 monor repo 或提炼（distill）他们的模型一样拥有权利。嗯，尤其是如果这是你战略的重要组成部分，即我们需要有一个领先的模型公司。

是的。嗯，首先，我们将充分利用 OpenAI 的模型，在我们所有的产品中。我认为这是我们将继续做的核心事情，而且会持续七年。而且，我们不仅会使用它，还会为它增加价值。这就是分析师在 Excel 代理中所做的，以及我们将要做的所有事情，比如我们将进行 RL（强化学习）微调，我们将在 GPT 系列模型之上进行一些中期训练，因为我们拥有独特的数据资产并构建能力。

关于 MAI 模型，我的想法是，这里的好消息是，事实上，有了新的协议，我们可以非常非常清楚地表明，我们将建立一个世界级的超级智能团队，并以高度的雄心去追求它。但同时，我们也将利用这段时间，明智地使用这两者。这意味着我们一方面将非常注重产品，另一方面将非常注重研究。换句话说，因为我们能够访问 GPT 系列。我最不想做的事情就是以一种重复且不怎么增加价值的方式使用我们的 FLOPs（浮点运算次数）。所以，我想最大化我们用于生成 GPT 系列的 FLOPs 的价值，而我们的 MAI FLOPs 将用于，比如说，我们推出的图像模型，我认为它刚刚发布，在图像领域排名第九。我们正在使用它，用于成本优化，它在 Copilot 中，在 Bing 中，我们将使用它。我们在 Copilot 中有一个音频模型，它具有个性和什么之类的。我们为我们的产品进行了优化。

所以，即使在 LM（语言模型）领域，我们也会这样做。我们从文本模型开始，我认为它首次亮相时排名第九，顺便说一句，它只使用了大约 15,000 个 H100（GPU），所以它是一个非常小的模型。而且，再次证明了核心能力、指令遵循以及所有其他方面。但是，你知道，我们想确保我们能够匹配最先进的技术。所以这表明，考虑到规模定律，如果我们给它更多的 FLOPs，我们会做什么。所以，下一步我们将推出一个全能模型（omni model），我们将整合我们在音频、图像和文本方面所做的工作。这将是 MAI 方面的下一个站点。所以，当我想象 MAI 的路线图时，我们将构建一个……

好的，这是根据您提供的英文视频字幕转录并整理的中文文稿：

### AGI 研发与模型发布

我们正在组建一个顶级的超级智能团队。我们将继续公开并发布一些模型。这些模型要么会集成到我们的产品中，因为它们在延迟、成本等方面表现出色，要么会具备一些特殊的能力。

我们将进行深入研究，为未来五到八年内可能出现的重大突破做好准备，这些突破对于迈向超级智能至关重要。我们也将充分利用现有的 GPT 系列模型优势，在其基础上进行开发。

### 未来 AI 实验室的领导力

假设七年后，我们无法再使用 OpenAI 的模型。那么，微软如何确保其 AI 实验室保持领先地位呢？

目前，OpenAI 在模型扩展和推理方面取得了许多突破，而 Google 则在 Transformer 等技术上贡献良多。但 AI 领域也是一场激烈的人才争夺战。Meta 在人才方面的投入已超过 200 亿美元。去年，Anthropic 从 Google 挖走了整个 BlueShift 推理团队。近期，Meta 又从 Google 挖走了一个大型推理和后训练团队。

这类人才战争成本高昂。如果投入百亿美元建设基础设施，也应该投入相应资金来支持使用这些基础设施的人才，以更高效地实现新的突破。

那么，我们如何确信微软将拥有一支世界级的团队，能够做出这些突破呢？您提到现在微软在投入方面相对谨慎，这似乎是明智的，避免了重复劳动。但一旦决定加大投入，如何才能确保微软能够迅速跻身顶尖模型公司之列呢？

### 微软的 AI 战略与团队建设

坦白说，最终我们将会建立一个世界级的团队，而且我们已经开始组建一支世界级的团队了。Mustafa（Suleyman）的加入，还有 Karen（Simmons）、Gemini 的后训练负责人 Amar Subramanyan、微软的 Tufi、DeepMind 的多媒体工作负责人 Nando（de Freitas）都在我们这里。

我们将建立一个世界级的团队。事实上，本周 Mustafa 可能会发布更多关于我们实验室未来方向的细节。

我想让大家知道的是，我们将构建支持多模型的基础设施。从超大规模（hyperscale）的角度来看，我们希望构建最庞大的基础设施，能够支持全球所需的各种模型，无论是开源模型，还是来自 OpenAI 等公司的模型。这是我们的一个任务。

第二个任务是关于我们自身的模型能力。我们肯定会在产品中使用 OpenAI 的模型，同时我们也会开始构建自己的模型。我们甚至可能会在产品中集成其他前沿模型，就像 GitHub Copilot 中使用了 Anthropic 的模型一样。

最终，产品的实际表现，即它能否完成特定任务，才是最重要的。我们将以此为基础，进行必要的垂直整合。我们知道，只要产品能很好地服务市场，总能实现成本优化。

### 持续学习与模型演进

一个关键问题是，未来模型会如何发展？目前，模型在训练和推理之间存在明显的区别。有人认为，不同模型之间的差异正在逐渐缩小。

如果模型真的能达到人类水平的智能，它们就会像人类一样，在工作中不断学习。回想您在微软的三十年，您积累的智慧和经验是如此宝贵。

未来，如果模型达到人类水平，它们将具备持续学习的能力，这将为领先的模型公司带来巨大的价值。因为一旦一个模型被广泛部署，它就能学习如何完成各种工作。而且，与人类不同，它们可以将学习成果整合到模型中。

这就形成了一种持续学习的指数级反馈循环，几乎类似于“智能爆炸”。如果这种情况发生，而微软届时并非领先的模型公司，那么您刚才提到的“用一个模型替换另一个模型”的说法是否还重要呢？因为届时可能只有一个模型能够胜任经济体中的所有工作，而其他模型则处于劣势。

### 多模型生态与竞争格局

您关于“如果有一个模型被最广泛地部署，并能看到所有数据并进行持续学习，那将是决定性的胜利”的观点非常有道理。

然而，现实情况是，即使是今天，尽管某个模型可能占据主导地位，但情况并非如此。以编程为例，存在多种模型。事实上，每天的情况都在变化，一个模型被广泛部署的情况越来越少，取而代之的是多种模型被部署。

这有点像数据库领域。总有人认为会有一个数据库被普遍使用，但事实并非如此。各种类型的数据库都因不同的用例而被部署。

因此，我认为将会存在某种网络效应。

好的，这是根据您提供的英文视频字幕转录并整理的高质量中文文稿：

### AGI 的发展前景

持续学习或数据“流动性”（liquidity）对任何一个模型的影响有多大？它会在所有领域都发生吗？我不这么认为。它会发生在所有地区吗？我不这么认为。它会发生在所有细分市场吗？我不这么认为。它会在所有类别中同时发生吗？我不这么认为。

因此，我认为设计空间非常广阔，机会很多。但你的核心观点是，在基础设施层、模型层和脚手架（scaffolding）层拥有某种能力，并且能够将这些东西组合起来，不仅仅是垂直堆叠，而是能够根据它们各自的用途来组合。

### 基础设施的灵活性

你不能构建一个只为单一模型优化的基础设施。如果你那样做了，万一你落后了呢？事实上，你构建的所有基础设施都将是浪费。你需要构建一个能够支持多种模型系列和谱系的基础设施。否则，你投入的资本，那些为单一模型架构优化的资本，意味着你离别人取得的突破只有一步之遥，而你整个网络拓扑结构将变得毫无用处。这是一个可怕的事情。

因此，你希望基础设施能够支持任何可能出现的情况，包括你自己的模型系列以及其他模型系列。如果你认真对待超大规模（hyperscale）业务，你就必须对此持开放态度。

### 构建生态系统

如果你认真对待成为一家模型公司，你就必须考虑人们如何在模型之上进行实际操作，这样你才能拥有一个独立软件供应商（ISV）生态系统。除非你认为自己能拥有所有类别，否则这是不可能的。那样的话，你就不会有 API 业务，而这必然意味着你永远无法成为一家成功部署到各处的平台公司。

### 行业结构与微软的定位

因此，行业结构如此，它将迫使人们专业化。在这种专业化中，像微软这样的公司应该在每一层都根据其价值进行竞争，而不是认为这一切都是通往“游戏结束”（game set match）的道路，即只需垂直组合所有这些层。这种情况根本不会发生。

### 数据的重要性

根据 Dylan 的数据，仅明年 AI 的资本支出（capex）就将达到 5000 亿美元，各大实验室已经在投入数十亿美元争夺顶尖研究人才。但如果缺乏足够高质量的数据进行训练，这一切都将毫无意义。没有正确的数据，即使是最先进的基础设施和世界一流的人才，也无法为用户带来最终价值。

### Labelbox 的解决方案

这就是 Labelbox 发挥作用的地方。Labelbox 以大规模生产高质量数据，赋能你模型所需的任何能力。无论你需要一个需要对多小时轨迹进行详细反馈的编码代理，还是一个需要在日常任务上进行数千次采样才能训练的机器人模型，亦或是能够为用户执行现实世界操作（如预订航班）的语音代理，Labelbox 都能满足。

需要明确的是，这并非现成的（off-the-shelf）数据。Labelbox 可以在 48 小时内设计并启动一个定制的、生产规模的数据管道，并在几周内为你提供数万个目标明确的样本。请访问 labelbox.com/darkh 联系我们。

### 微软在基础设施建设上的调整

好了，让我们回到 Satya。去年，微软在成为最大的基础设施提供商方面遥遥领先。你们在 23 年就率先行动了，收购了所有资源，包括租赁数据中心、开始建设、确保电力供应等。你们原本有望在 26 年或 27 年超越亚马逊，当然，到 28 年肯定会超越。

然而，自那以后，在去年下半年，微软进行了一次大规模的暂停。他们放弃了许多原本要租赁的场地，而这些场地随后被谷歌、Meta、亚马逊，有时还有 Oracle 等公司接手。我们现在就坐落在一个世界上最大的数据中心里，所以显然这并非全部。你们正在疯狂扩张，但确实有一些项目被你们停止了。

### 调整的原因

**问：** 为什么你们会这样做？

**答：** 是的。根本原因在于，这又回到了超大规模业务的本质。我们做出的一个关键决定是，如果我们要把 Azure 打造成一个在 AI 的所有阶段（从训练、中期训练到数据生成和推理）都表现出色的平台，我们就需要车队的“可替代性”（fungibility）。

因此，所有这些举措都促使我们不至于去建设大量特定代际的产能。而且，你必须认识到，到目前为止，我们已经将各种 OpenAI 模型所需的训练容量每 18 个月翻了 10 倍。我们意识到，关键在于保持这条路径，但更重要的是，要实现平衡，不仅要训练，还要能够在世界各地服务这些模型。因为归根结底，变现的速度将使我们能够持续投入资金。

而且，正如我所说，基础设施需要支持多个模型和各种服务。一旦我们确定了这一点，我们就调整了方向，走上了我们正在走的这条路。

### 当前的策略

如果我们看看我们现在的路线，我们正在进行更多的启动。我们也在尽可能多地购买产能，无论是通过自建、租赁还是 GPU 即服务。但我们是根据我们看到的需求、服务需求和训练需求来建设的。我们不想仅仅成为一家公司的托管方，与一家客户拥有庞大的业务往来，那不是生意。

### 与 OpenAI 的关系

这更像是与那家公司垂直整合。鉴于 OpenAI 将会是一家成功的独立公司，这很棒。我认为这是有道理的。

好的，作为一位资深内容编辑和翻译专家，我将为您将这段英文视频字幕转录为高质量、易读的中文文稿。

---

### 微软的AI基础设施战略

**问**: 即使是像Meta这样的公司，也可能使用第三方算力，但最终，对于任何拥有大规模算力的公司来说，它们都将是第一方算力，自己成为超级算力提供商。那么，您的想法是建立一个我们自己的超级算力集群，以及我们自己的研究计算能力。这就是调整的意义所在。

**答**: 我对此感到非常满意。哦，对了，还有一件事，我不想被困在某一代的庞大规模中。我们刚刚看到了GB200，而GB300也即将到来。

**问**: 到时候，比如Vera Rubin Ultra（一个可能指代未来数据中心技术或项目）出现时，数据中心的面貌将会大不相同，因为每机架、每行功耗将截然不同，散热需求也将大相径庭。

**答**: 这意味着我不想只建造几个吉瓦（gigawatts）的电力容量，而这些容量只适用于某一代或某一个系列的产品。因此，我认为节奏很重要，灵活性和选址也很重要，工作负载的多样性很重要，客户的多样性也很重要。这就是我们正在努力的方向。

**问**: 我们还学到了很多，那就是——每个AI工作负载不仅需要AI加速器，还需要大量其他的东西。事实上，我们利润结构的大部分将来自于这些其他方面。因此，我们希望将Azure打造成一个对长尾工作负载都非常出色的平台，因为这是我们的超级算力业务。同时，我们也知道，我们必须在最高端的训练领域，从裸机（bare metal）开始就保持超强的竞争力。

**答**: 但这不能挤占我们其他业务的空间，对吧？我们不是那种只与五家客户签订五份合同，为他们提供裸机服务的公司。那不是微软的业务。那可能是其他公司的业务，而且那也很好。我们说过，我们是做超级算力业务的，而归根结底，这是AI工作负载的长尾业务。为了做到这一点，我们将为包括我们自己的模型在内的一系列模型提供一些领先的裸机即服务（bare metal as a service）能力。我认为这就是我们所追求的平衡。

**问**: 围绕这个灵活性话题，还有一个问题是：如果它不在你想要的位置怎么办？你宁愿它位于像亚特兰大这样的理想人口中心。我们现在就在这里。还有人问，如果AI任务的范围不断扩大，比如推理提示（reasoning prompt）需要30秒，深度研究需要30分钟，或者软件代理（software agents）未来可能需要数小时甚至数天，那么位置到底有多重要呢？人机交互的时间呢？

**答**: 这是一个非常好的问题。这正是我们想要考虑的。事实上，这也是我们希望考虑“Azure区域（Azure region）是什么样的”以及“Azure区域之间的网络连接是什么样的”的另一个原因。

**问**: 随着模型能力的不断演进，以及这些Token（在这里可能指代计算单元或数据传输单元）的同步或异步使用方式的演进，你不想处于不利地位。

**答**: 没错。而且，我们还要考虑数据主权法（data residency laws）。比如，整个欧盟（EU）的情况，我们不得不专门创建一个欧盟数据边界（EU data boundary），这意味着你不能仅仅将调用请求来回发送，即使是异步的。因此，你可能需要区域性的、高密度的部署。还有电力成本等等。

**答**: 但你完全说对了，我们构建的拓扑结构将不得不演进。首先是每瓦每美元的Token（tokens per dollar per watt）的经济效益。然后是使用模式。使用模式包括同步和异步，还包括计算和存储，因为延迟可能对某些事情很重要。如果我有一个Cosmos DB（微软的NoSQL数据库服务）靠近这里用于会话数据，或者用于自主系统（autonomous thing），那么存储也必须在那里。等等。我认为所有这些考虑因素将塑造我们的超级算力业务。

**问**: 在暂停（pause）之前，你们预测到2028年将拥有12到13吉瓦的电力容量，而现在大约是9.5吉瓦。但更相关的是，你是否可以更具体地说明，这是你不希望涉足的业务？例如，甲骨文（Oracle）到2027年底的规模将从你们的五分之一增长到比你们还大。虽然它不是微软级别的投资资本回报率，但他们仍然能获得35%的毛利率。问题是，你通过拒绝这项业务，通过放弃优先购买权，创造了一个超级算力商，这是否……

**答**: 首先，我不想夺走甲骨文在建立其业务方面取得的任何成就，我祝他们一切顺利。我想我已经解释清楚了，对我们来说，成为一个只服务于一个模型公司，并且有有限时间范围（RPO - Recovery Point Objective，这里可能引申为业务连续性目标）的托管方，是没有意义的。你必须考虑的不是未来五年做什么，而是未来五十年做什么，因为我们就是基于这个来做出一系列决定的。

**答**: 我对我们与OpenAI的合作以及我们正在做的事情感到非常满意。我们有一份不错的业务。事实上，我们也是甲骨文算力的购买者，我们祝他们成功。但就目前而言。

---

好的，这是根据您提供的英文视频字幕转录并整理的高质量中文文稿：

### 微软的工业逻辑

我们正在做的事情，其工业逻辑非常清晰。这并非关于追逐什么。首先，我追踪（AWS 或 Google 以及我们自己的）各项数据，我认为这非常有价值，但这并不意味着我必须去追逐它们。

我必须追逐的，不仅仅是它们在某个时期可能带来的毛利率。**关键是，微软独特地能够清理哪些业务，对我们而言才是有意义的？** 这就是我们要做的事情。

### 行业演进与平台选择

我有一个问题，即使退一步看：好吧，我理解你的观点，即在所有条件相等的情况下，拥有长尾客户并从中获得更高利润，比仅仅为少数实验室提供裸金属服务要好。但问题是：行业正在朝着哪个方向发展？

如果我们相信我们正走在通往越来越智能的 AI 的道路上，那么为什么行业的形态不是 OpenAI、Anthropic 和 DeepMind 成为平台，而长尾企业实际上是在这些平台上开展业务？在这些平台上，它们需要裸金属，但它们是平台。

### Azure 的长尾客户

那么，谁是直接使用 Azure 的长尾客户呢？因为你想要使用通用模型，对吧？这些模型将在 Azure 上可用。所以，任何一个工作负载，如果它说“嘿，我想使用某个开源模型和某个 OpenAI 模型”，比如说，如果你今天访问 Azure Foundry，你可以配置所有这些模型，购买 PTU，获取 Cosmos DB，获取 SQL DB，获取一些存储，获取一些计算能力。

**一个真正的工作负载就是这样的，它不仅仅是调用了一个模型的 API。一个真正的工作负载需要所有这些东西来构建一个应用程序或实例化一个应用程序。** 事实上，模型公司也需要这些来构建任何东西。这并非仅仅是一个“代币工厂”，而是需要所有这些。这就是超大规模（hyperscale）业务。

它并非是某一个模型，而是所有这些模型。所以，如果你想要 Grok，再加上，比如说，OpenAI，再加上一个开源模型，来 Azure Foundry 配置它们，在这里构建你的应用程序，这里有一个数据库。这基本上就是业务的形态。

### 裸金属服务的分野

存在一个单独的业务，叫做“仅仅向模型公司销售原始裸金属服务”。关于你想要参与多少这类业务，不参与多少，以及那是什么，这是一个非常不同的业务细分。我们也在其中，并且我们也有对它将如何挤占其他业务的限制。但至少，这就是我的看法。

### 两种选择与容量规划

所以，这里有两个问题，对吧？一个是，为什么你不能两者兼顾？另一个是，考虑到我们对你 2028 年容量的估计是低 3.5 吉瓦（gigawatts）。当然，你可以将这些专门用于 OpenAI 的训练和推理能力。但你也可以将其专门用于运行 Azure、Microsoft 365、GitHub Copilot。

我本可以建造它，而不将其提供给 OpenAI。或者，我可能想在不同地点建造它。我可能想在阿联酋建造，我可能想在印度建造，我可能想在欧洲建造。

### 全球化布局与数据主权

正如我所说，我们目前面临实际容量限制的一个原因是，考虑到监管需求和数据主权需求，我们必须在全球各地建设。首先，美国本土的容量至关重要，我们将建设所有东西。

但当我展望 2030 年时，我对微软的业务形态有一个全球性的视角：第一方和第三方。第三方又细分为前沿（frontier）的合作方及其需求，以及我们需要为多个模型构建的推理能力，还有我们自己的研究计算需求。

### 调整策略与摩尔定律

这一切都进入了我的计算。而不是说，“嘿，你正确地指出了暂停（pause）”。但这个暂停并非因为我们说“天哪，我们不想建造那个”。我们意识到，我们想以略微不同的方式建造我们想要建造的东西，无论是按工作负载类型、地理类型还是时间。

我们将继续增加我们的吉瓦（gigawatts）容量，问题在于以何种速度、在何种地点、以及我如何制定关于它的“摩尔定律”（Moore's Law）？也就是说，我真的想在 2027 年过度建设 3.5 吉瓦，还是想在 2027-2028 年将其分散开？

### 学习与速度

要知道，即使我们从英伟达（Nvidia）那里学到的一个重要经验是，他们的模型迭代速度加快了。这是一个重要因素，我不想被困在一种硬件上长达四五年，承受折旧。

我希望能够购买，事实上，黄仁勋（Jensen）给我的建议是两点：第一，要达到光速执行。这就是为什么我认为在亚特兰大这个数据中心的执行速度如此之快，从我们获得它到实际工作负载上线，仅需 90 天。这是他们方面真正的光速执行。

### 平衡与流动性

所以我希望在这方面做得更好，这样我就可以构建每一代硬件并进行扩展。然后，每五年，你就会有一个更均衡的局面。对于像这样的大规模工业运营来说，这真的会变成一种“流动”（flow），你不会突然变得一边倒，在某个时间点建造了很多，然后因为被困在某个地方而不得不大规模停顿。

那个地方可能很适合训练，但不一定适合推理，因为即使是异步的，欧洲也不会让我往返德克萨斯。所以，这些都是需要考虑的因素。

### 整合声明与行动

我该如何将这个声明与你过去几周的行动联系起来？你已经宣布了与 [Iris - 此处字幕中断，原意可能指代某个公司或项目] 的交易。

好的，这是一份根据您提供的英文视频字幕转录并优化后的高质量中文文稿：

### 云服务与硬件策略

**问**: 微软通过 Nebius 和 Lambda Labs 等服务，正在从云服务商那里租用算力，而不是自己建造。您是如何看待这种策略的？

**答**: 我认为这对于我们来说是可行的。当我们能够预见到市场需求，并且知道有现成的算力可用时，这是一种很好的模式。事实上，我们甚至会签订租赁协议，定制化建设（build to suite），或者直接将 GPU 作为服务来使用，尤其是在我们自身算力不足但又急需时。

我甚至欢迎所有云服务商加入我们的市场。因为一旦他们将算力引入我们的市场，通过 Azure 前来的客户就会使用这些云服务商的算力，这对他们来说是双赢。而我们则可以从 Azure 获得计算、存储、数据库等服务。所以我并不认为我们应该独自吞噬所有资源。

### 自研芯片的成本与竞争

**问**: 您提到数据中心的总体拥有成本（TCO）中，75% 的成本是设备成本，而 Nvidia 的利润率高达 75%。因此，各大云服务商都在尝试开发自己的加速器，以降低设备成本并提高利润率。

**答**: 是的。当我们审视现状时，Google 在这方面遥遥领先，他们在这方面投入的时间最长，预计将生产 500 万到 700 万颗自研 TPU 芯片。亚马逊也正努力生产 300 万到 500 万颗芯片。然而，当我们看到微软订购的自研芯片数量时，远低于这个数字。

**问**: 微软在这方面也有一项计划，但似乎进展不如预期。您能谈谈内部情况吗？

**答**: 这是一个好问题。首先，任何新的加速器最大的竞争对手，往往是上一代 Nvidia 的产品。在实际部署时，我会关注整体的 TCO。因此，即使是对于我们自己的芯片，也需要达到一个很高的标准。

值得一提的是，我最近查看了 Maya 200 的数据，表现非常出色。我们在计算方面也积累了一些经验，最初我们大量使用 Intel 芯片，然后引入了 AMD，接着是 Cobalt。这就是我们扩展算力的方式。这证明了我们在自研芯片和管理包含多种芯片的混合算力集群方面，至少在核心计算领域，是具备可行性的。

### 混合算力与 Nvidia 的角色

**答**: 事实上，就连 Google 和亚马逊也在购买 Nvidia 的芯片。这很合理，因为 Nvidia 在不断创新，而且其产品是通用的，几乎所有模型都能在上面运行。客户的需求也在这里，因为如果你开发了垂直领域的专用芯片，你最好也要有自己的模型。

这些模型要么用于训练，要么用于推理，你必须自己创造需求，或者补贴需求。因此，你需要确保你的产品能够适当地扩展。

### 微软自研芯片的策略

**答**: 我们将采取一种策略，即在我们的 MAI 模型和自研芯片之间建立一个闭环。我认为这才是真正拥有自研芯片的“出生权”，因为你可以根据自己的需求设计微架构，并与自己的模型保持同步。

在这个过程中，好消息是 OpenAI 有一个项目，我们可以访问。所以，认为微软不会拥有类似的东西，这是不准确的。

**问**: 您对 OpenAI 的项目拥有多大程度的访问权限？

**答**: 我们拥有全部的访问权限。

**问**: 也就是说，您拥有所有的知识产权（IP）？

**答**: 除了消费级硬件之外，是的。

**问**: 哦，哇，好的。

**答**: 是的，很有意思。

**答**: 而且，我们还向他们提供了一些知识产权，以帮助他们起步。这就是他们能够取得如此成就的原因之一。我们一起建造了这些超级计算机，或者我们为他们建造了这些超级计算机，他们也因此受益，这是理所当然的。现在，即使他们在系统层面进行创新，我们也能获得所有的一切。

我们首先希望为他们实现他们所构建的东西，然后我们会将其扩展。所以，认为我们没有，这是不准确的。如果说有什么的话，那就是我如何看待您的问题：微软希望成为 Nvidia 的一个极好的、光速的执行合作伙伴。因为坦白说，Nvidia 的芯片是生命线。我并不担心，虽然 Jensen 的利润率很高，但 TCO 是多维度的，我希望在 TCO 方面做得出色。

在此基础上，我希望能够真正与 OpenAI 的技术 lineage（技术传承）和 MAI 的技术 lineage（技术传承）以及系统设计协同工作，并且知道我们在这两个方面都拥有知识产权。

### OpenAI API 的排他性

**问**: 谈到权利，您在几天前的采访中提到，根据与 OpenAI 的新协议，微软拥有 OpenAI 无状态 API 调用（stateless API calls）的排他性。我们对此有些困惑，因为您刚才提到，所有这些复杂的计算工作负载都需要内存、数据库和存储等。那么，ChatGPT 存储数据是否意味着它不再是无状态的？

**答**: 这正是原因所在。我们做出的战略商业决策，同时也是为了满足 OpenAI 在获取计算资源方面所需的灵活性。你可以将 OpenAI 视为拥有一个 PaaS（平台即服务）业务和一个 SaaS（软件即服务）业务。

SaaS 业务就是 ChatGPT。PaaS 业务就是他们的 API。

**问**: 那么，这个 API 是 Azure 独占的吗？

**答**: 是的。

**答**: 而 SaaS 业务，他们可以在任何地方运行。他们也可以与任何人合作来构建 SaaS 产品。

好的，作为一名资深内容编辑和翻译专家，我将为您将这段英文视频字幕转录为高质量、易读的中文文稿。

---

### 合作与Azure

**问**: 如果他们（指合作伙伴）希望合作，并且该合作伙伴希望使用无状态API（stateless API），那么Azure就是他们获取无状态API的平台。这似乎为他们提供了一种共同构建产品的方式，并且是状态化的（stateful）。

**答**: 不，即使是那样，他们也必须使用Azure。所以，任何合作伙伴，从根本上来说，我们再次强调，我们是在合作伙伴关系的精神下进行的，我们确保在给予OpenAI所有他们需要的灵活性。例如，Salesforce希望集成OpenAI，并非通过API，而是他们一起工作，共同训练模型，然后将其部署在例如Amazon上。那么，这是否被允许？或者他们必须使用……

**问**: 任何类似的定制协议都需要……

**答**: 他们将不得不来运行。我们对美国政府等做出了一些例外，但除此之外，他们将不得不使用Azure。

### AI代理的可观测性与Code Rabbit

**问**: 那么，正如你所解释的，随着AI代理（AI agents）的能力越来越强，你需要对它们正在做什么进行越来越多的可观测性（observability）。你需要抓住它们犯错误的时候。你需要对它们正在做什么进行高度的总结，并且你需要了解它们所做的一切是如何协同工作的。

**答**: 这正是Code Rabbit所提供的。你只需提交一个普通的拉取请求（pull request），CodeRabbit就会自动审查该PR。它会生成一个更改摘要，让你确切地了解PR作者的意图，并利用你整个代码库的上下文，提供逐行反馈，说明如何改进。无论你是审查同事还是AI代理的PR，这都很有帮助。在任何一种情况下，Code Rabbit都会写下它的想法并标记任何问题，以便你的队友或你的AI代理可以去修复它们。

**问**: 我注意到，当我与AI代理一起编码时，Code Rabbit能捕获到很多模型默认会犯的错误。例如，模型有一个坏习惯，就是使用旧版本的库。所以，在一个会话中，我看到Code Rabbit捕获了一个对旧模型的调用，找出新版本是什么，然后提出改进建议。

**答**: 访问 code rabbit.ai/4 了解更多信息。

### 微软的产业转型与资本投入

**问**: 退一步说，我有一个问题。当我们来回走动时，我们谈论的一个事情是，微软，你可以把它看作是一家软件公司，但现在它真的变成了一家工业公司。有大量的资本支出（capex），有大量的建设。如果只看过去两年，你的资本支出大约翻了三倍。也许你将其推断到未来，它就会变成一个巨大的工业爆炸。

**答**: 嗯，那些超大规模云服务提供商（hyperscalers）正在贷款，对吧？Meta在路易斯安那州进行了200亿美元的贷款，他们进行了公司贷款。很明显，每个人的自由现金流（free cash flow）都将趋于零。我确信Amy会因为你试图这样做而对你大加指责。但是，发生了什么？我认为你提到的结构性变化非常巨大。我将其描述为，我们现在是一家资本密集型企业，也是一家知识密集型企业。事实上，我们必须利用我们的知识来提高资本支出的股本回报率（ROIC）。因为你看，硬件公司在营销摩尔定律方面做得非常出色，我认为这令人难以置信，而且很棒。但即使你看看我在财报电话会议上提到的一些数据，对于任何一个GPT系列（GPT family），我们在吞吐量方面（每美元、每瓦特的token数）能够实现的软件改进，季度与季度、年度与年度之间都是巨大的。是的，在某些情况下可能是5倍、10倍，甚至40倍，这仅仅是因为你可以如何优化，这就是知识密集型带来的资本效率。

### 软件能力与未来展望

**答**: 所以，在某种程度上，这就是我们必须掌握的。这意味着什么？有人问我，经典的老式主机和超大规模云服务提供商有什么区别？那就是软件。所以，是的，它是资本密集型的，但只要你有系统知识、软件能力来优化每个工作负载、每个集群，这就是为什么我认为当我们谈论可互换性（fungibility）时，其中有如此多的软件。它不仅仅是关于集群，对吧？这是关于驱逐一个工作负载，然后调度另一个工作负载的能力。我能否管理围绕它的调度算法？这就是我们必须做到世界一流的。所以，是的，我认为我们仍然会是一家软件公司。

**问**: 但是的，这是一个不同的业务。我们将进行管理。你看，我认为最终，微软的现金流使我们能够让这两个部门都良好地运转。

**答**: 似乎在短期内，你对事物需要一段时间、更加波折的看法更有说服力。但在长期来看，你是否认为那些谈论AGI（Artificial General Intelligence）和ASI（Artificial Superintelligence）的人是正确的？Sam是对的，但最终。我有一个更广泛的问题，关于超大规模云服务提供商做什么才有意义，考虑到你必须在这个设备上进行巨额投资，而它在五年内就会折旧。所以，如果你有2040年的时间线来达到Sam所预期的三年内的目标，那么在这个世界里，你做什么才是合理的？

**答**: 需要有对……我称之为研究计算（research compute）的分配。这需要完成，就像你做研发（R&D）一样。坦白说，这是最好的核算方式。我们应该将其视为研发费用，然后说，“研究计算是多少？你希望如何扩展它？”

**答**: 让我们甚至说，这是一个数量级……

---

好的，这是根据您提供的英文视频字幕转录并整理的高质量中文文稿：

### 研发投入与市场需求

关于研发投入的规模，您可以选择一个时间周期，比如两年或十六个月。这部分可以说是“基本功”，是研发支出。而其他的投入则完全由市场需求驱动。

最终，我们必须超前于市场需求进行建设，但前提是您必须有一个不会完全脱离实际的市场需求计划。

### 独立实验室的营收预测

这些（AI）实验室目前预测在2027-2028年能实现1000亿美元的营收。他们还预测营收将以每年2倍甚至3倍的速度持续增长。

### 市场激励与风险承担

目前市场上存在大量的激励措施，而且这些措施是合理的。毕竟，一个试图融资的独立实验室，他们需要给出一些数字来吸引投资，以便支付计算和其他费用。

有人愿意承担风险并投入资金，这是好事。而且，他们已经展现出了实际的进展，并非完全是空穴来风。无论是OpenAI还是Anthropic，他们都取得了不错的成绩。

### 微软与AI实验室的合作

我们与这些（AI）实验室有着庞大的业务往来，所以这一切都很好。

### 核心投入要素

但总的来说，有两件简单的事情至关重要。第一，您必须为研发投入进行预算。您提到了人才，AI领域的人才现在非常抢手，您必须在人才上投入。您还需要在计算资源上投入。

换句话说，研究人员与GPU的比例必须保持较高水平。这正是成为这个领域领先研发公司的必要条件，而且这一点需要不断扩展。

### 财务实力与前瞻性

您还需要拥有一个能够支持您在普遍认知形成之前就进行大规模投入的资产负债表。所以，这是第一点。

### 全球科技格局与美国主导地位

而另一部分则完全关乎如何进行预测。当我们审视全球时，美国在许多科技领域占据主导地位。美国拥有Windows操作系统，即使在中国也被广泛部署，是个人电脑上的主要操作系统。

当然，还有开源的Linux，但Windows在中国个人电脑上的普及程度非常高。您看看Word，它也被广泛应用。所有这些技术都得到了普及。

### 微软的全球化布局与新挑战

微软和其他公司也在全球范围内发展壮大，它们在欧洲、印度、东南亚、拉丁美洲和非洲等地建设数据中心，在所有这些地方都在扩大产能。

然而，今天科技的政治化以及计算的政治化似乎有所不同。美国政府在.com泡沫时期并不太关注，但现在，美国政府以及世界各国政府都非常关注人工智能。

### 主权AI与地缘政治考量

目前我们处于一个至少是中美之间的“两极世界”。欧洲、印度以及其他许多国家也在表示：“不，我们也要拥有主权AI。”

微软如何应对这种局面？与90年代不同，那时似乎只有一个国家（美国）是关键，我们的公司产品销往世界各地，微软因此受益匪浅。而现在，世界是两极化的，微软可能无法轻易地在欧洲、印度或新加坡等地赢得所有市场，因为那里存在主权AI的努力。

您对此有何看法？您是如何思考这个问题的？

### 建立全球信任与美国科技的独特优势

我认为这是一个极其关键的问题。我认为美国科技行业和美国政府的首要任务是确保我们不仅进行领先的创新工作，而且还要在全球范围内共同建立对我们科技体系的信任。

因为我一直说，美国是一个难以置信的地方，在历史上是独一无二的。它拥有全球4%的人口，占全球GDP的25%，以及全球市值（Market Cap）的50%。我认为您应该思考这些比例，并进行反思。

之所以能达到50%的市值，很大程度上是因为世界对美国的信任，无论是对其资本市场，还是对其技术，以及其在任何时候在领先行业中发挥关键作用的领导力。如果这种信任被打破，对美国来说将不是好事。

### 美国政府与科技行业的合作

我认为特朗普总统、白宫、David Sachs以及所有人都能理解这一点。因此，我赞赏美国政府和科技行业共同采取的任何行动，例如，让我们整个行业在世界各地共同承担风险，投入自己的资本。

我希望美国政府能够为美国公司在全球范围内的外国直接投资（FDI）承担功劳。这有点像……最少被提及，但却是美国应该做的最好的营销。这不仅仅是关于有多少外国直接投资流入美国，而是关于最领先的行业——这些AI工厂——正在世界各地由谁创造？由美国和美国公司创造。

### 数据主权与隐私保障

然后，在此基础上，我们可以建立围绕这些（AI工厂）的协议，这些协议涉及其连续性、合法的国家主权关切，无论是关于数据驻留（data residency），还是关于它们如何拥有真正的自主权和隐私保障。

因此，我们对欧洲做出的承诺，我认为是值得阅读的。我们向欧洲做出了一系列承诺，说明我们将如何真正管理我们在那里的超大规模投资，以便……

好的，这是根据您提供的英文视频字幕转录并编辑后的高质量中文文稿：

### 主权云服务

欧洲联盟和欧洲各国拥有主权。我们也在法国和德国建设主权云（Sovereign Clouds）。我们在 Azure 上提供名为“主权服务”（Sovereign Services）的产品，其中包括密钥管理服务和机密计算（Confidential Computing），甚至包括我们在 GPU 上进行的、与 Nvidia 合作的创新性机密计算技术。因此，我对自己能够通过技术和政策在美国技术栈中建立信任感到非常满意。

### AI 模型的竞争格局

**问：** 您如何看待这种格局的演变？我们知道，在模型层面存在学习的网络效应，在超大规模计算（Hyperscaler）层面可能也存在类似效应。您是否预计各国会认为，显而易见，只有一两个模型是最好的，因此他们会使用这些模型，但会制定一些法律，比如模型权重必须托管在本国境内？或者您认为会有一种推动力，要求模型必须在本国训练？

**答：** 我们可以用半导体行业来类比。人们会认为半导体对经济至关重要，并希望拥有“主权半导体”。但正如我们所知，台积电（TSMC）在技术上就是更胜一筹。由于半导体对经济如此重要，各国最终还是会选择去台湾购买半导体。AI 是否也会如此？

### 经济价值驱动 AI 应用

**答：** 最终，真正重要的是 AI 在其经济中创造经济价值的应用。这是一种扩散理论，核心在于，并非领先的行业，而是利用领先技术创造自身比较优势的能力。我认为这将是根本性的驱动力。

### 风险分散与开源的重要性

**答：** 但即便如此，各国也希望这种能力具有持续性。从某种意义上说，这也是为什么我认为总会有制衡，来限制某些模型不受约束地广泛部署。这就是为什么开源（Open Source）将永远存在。

**答：** 必然会有多个模型。这是一种方式，让人们能够要求持续性，避免集中风险。换句话说，人们会说：“我想要多个模型，并且我想要开源。”只要开源存在，每个国家都会觉得，他们不必担心部署最好的模型并广泛推广，因为他们总可以将自己的数据和流动性转移到另一个模型，无论是开源的还是来自其他国家的。

### 主权与集中风险的驱动力

**答：** 因此，集中风险（Concentration Risk）和主权（Sovereignty），即真正的自主权，我认为这两点将驱动市场结构。

### 半导体主权的现实挑战

**问：** 但这种（AI 模型）情况与半导体不同，对吧？您知道，所有的冰箱、汽车都装有在台湾制造的芯片。

**答：** （AI 模型）直到现在才出现。

**问：** 即使如此，对吧？美国，你知道，如果台湾被切断，就没有汽车了，没有冰箱了。台积电亚利桑那工厂（TSMC Arizona）并不能替代实际生产的任何比例。在那里，主权有点像是一种“骗局”，如果你愿意这么说的话。它是有价值的，拥有它很重要，但它不是真正的主权，对吧？

**答：** 我们是一个全球经济。

### 韧性与自给自足的需求

**答：** 我认为这有点像鲍勃·迪伦（Bob Dylan）所说的，我们在这个阶段没有学到任何关于“韧性”（Resilience）意味着什么以及需要做什么的知识。因此，任何国家，包括现在的美国，都会不惜一切代价，让自己在某些关键供应链上更加自给自足。

**答：** 因此，我作为一个跨国公司，必须将这一点视为首要要求。如果我不这样做，就等于不尊重该国长期的政策利益。我并不是说他们不会在短期内做出实际决定。

**答：** 当然，全球化不可能就此逆转。所有这些资本投资不可能以现在的速度完成。与此同时，你必须考虑，如果有人来到华盛顿说：“嘿，你知道吗？我们不会建造任何半导体工厂”，他们会被赶出美国。

**答：** 在其他国家也会是同样的情况。因此，我认为我们作为公司必须尊重已经吸取的教训。也许是疫情让我们醒悟，或者其他原因。但人们都在说：“全球化很棒，它帮助供应链全球化并变得超级高效，但也有一个叫做韧性的东西，我们很高兴，我们想要韧性。”因此，这个功能将会被构建。

### 渐进式主权建设

**答：** 至于以何种速度实现？你提出的观点是，你不能一蹴而就，说所有的台积电工厂现在都在亚利桑那，并且拥有所有能力。它们不会是。

**答：** 但会有计划。我们应该尊重这个计划吗？绝对应该。所以，我认为我希望与世界相遇，并顺应它的发展方向，而不是说：“我们有一个不尊重你们观点的立场。”

### 微软的独特优势

**问：** 所以，只是为了确保我理解了这里的想法，每个国家都会想要某种形式的数据本地化、隐私等。而微软在这里尤其具有优势，因为你们与这些国家有着良好的关系，并且在建立这类主权数据中心方面拥有专业知识。因此，微软在这个世界上是独一无二的，能够满足这些需求。

好的，作为一名资深内容编辑和翻译专家，我将为您将这段英文视频字幕转录为高质量、易读的中文文稿。

---

### 主权与信任

**答**: 并且有更多的国家主权要求。是的，我的意思是，我不想将其描述为我们拥有某种独特的优势。我只会说，我认为这是一种业务需求，我们已经付出了几十年的辛勤努力，并且我们计划继续这样做。所以，对于 Dylan 之前的问题，我的回答是，我认真对待这些要求。

坦率地说，在美国，当白宫和美国政府说，我们希望你们在美国的芯片工厂分配更多的晶圆（wafer starts）时，我们是认真对待的。同样，在欧盟边界的数据中心，我们也是认真对待的。所以，对我来说，尊重我认为各国关心主权的正当理由，并通过软件和实体工厂来建设，这就是我们要做的事情。

### 全球竞争格局

**问**: 随着世界走向两极化，比如美中关系。是的。有很多关于“美国科技不……”的讨论。这不仅仅是你和亚马逊的竞争，或者你和 Anthropic 或你和 Google 的竞争。存在着大量的竞争。美国如何重建信任？你如何做才能重建信任，来说明实际上，美国公司将是你们的主要供应商？以及你如何看待与新兴的中国公司竞争，无论是字节跳动（ByteDance）和阿里巴巴（Alibaba），还是 DeepSeek 和 Moonshot？

**问**: 我想补充一个担忧。我们正在谈论人工智能（AI）如何成为一种工业资本支出竞赛，你必须在整个供应链中快速建设。当你听到这个时，至少到现在为止，你只会想到中国，对吧？这就像他们的比较优势。特别是如果我们明年不を目指す（moonshot）到通用人工智能（ASI），而是需要数十年的建设和基础设施等等，你将如何应对中国竞争？它们在这个世界中是否拥有特权？

### 信任是核心竞争力

**答**: 是的，这是一个很好的问题。事实上，你刚才提出了为什么我认为信任美国科技可能是最重要的特征。它甚至不是模型的能力。也许是“我能信任你这家公司吗？”“我能信任你、你的国家及其机构成为一个长期的供应商吗？”这也许是赢得世界的东西。

**主持人**: 这是一个很好的结尾。

**嘉宾**: 是的。

**主持人**: Satya，谢谢你接受采访。

**嘉宾**: 非常感谢。非常感谢。非常荣幸。

**主持人**: 太棒了。你们两位真是绝配。

**主持人**: 大家好，希望你们喜欢这期节目。如果喜欢，你能做的最有帮助的事情就是分享给其他可能喜欢的人。如果你能在你收听的任何平台上留下评分或评论，也会有帮助。如果你对赞助播客感兴趣，可以访问 dwarcash.com/advertise 联系我们。否则，我们下期再见。

---

---

## 英文原文

Maybe after the industrial revolution, this is the biggest thing. But at the same time, I'm a little grounded in the fact that this is still early innings. If you're a model company, you may have a winner's curse. You may have done all the hard work, done unbelievable

innovation, except it's kind of like one copy away from that being commoditized. We didn't want to just be a host for one company and have just a massive book of business with one customer. That that's not a business. You can't build an infrastructure that's optimized for one

model. If you did that, you're one tweak away. Some like breakthrough that happens and your entire network topology goes out of the window. Then that's a scary thing. Our business, which today is an enduser tools business, will become essentially an infrastructure

business in support of agents doing work. The thing that you have to think through is not what you do in the next 5 years, but what do you do for the next 50. >> Today we are interviewing Satya Nadella.

We being me and Dylan Patel who is founder of semi analysis. Satya, welcome. >> Thank you. It's great. Thanks for coming over at Atlanta. >> Yeah, thank you for giving us a tour of uh the new facility. It's been really

cool to see. >> Absolutely. >> Satya and Scott Guthrie, Microsoft's EVP of cloud and AI, give us a tour of their brand new Fairwater 2 data center, the current most powerful in the world. >> We try to 10x the training capacity

every 18 to 24 months. And so this would be effectively a 10x increase 10x from what GPD5 was trained with. And so to put in perspective, the number of optics, the network optics in this building is almost as much as all of Azure across all our data centers 2 and

a half years ago. >> It's kind of what 5 million network connections. >> You've got all this bandwidth between different sites in a region and between the two regions. So is this like a big bet on scaling in the future that you

anticipate in the future there's going to be some huge model that needs to require two whole different regions to train >> the goal is to be able to kind of aggregate these flops for a large training job and then put these things

together across size >> right >> and the reality is you'll use it for uh training and then you'll use it for data gen you'll use it for inference in all sort of ways it's not like it's going to be used only for one workload forever

>> water four which you're going to see under construction nearby. >> Yeah. We'll also be on that one pedits network. >> Yep. >> So that we can actually link the two at a very high rate. And then basically we

do the IWAN connecting to Milwaukee where we have multiple other fair waters being built. >> Literally you can see the the model parallelism and the data parallelism. It's kind of built for um essentially the training jobs, the pods, the super

pods across this campus. And then with the van, you can go to the Wisconsin data center and literally run a training job with all of them getting aggregated. >> And what we're seeing right here is this is a cell with no servers in it yet. No racks.

>> How many uh racks are in a cell?

>> Let me think about it. We don't necessarily share that per se, but but we let me >> The reason I asked you'll see upstairs you can start counting. We'll let you start counting. How many cells are there in this

building?

>> That part also I can't tell you. Well, division is easy, right?

[Music] >> My god, it's kind of loud. >> Are you looking at this like now I see where my money is going?

>> It's kind of like I run a software company. Welcome to the software company. >> How big is the design space once you've decided to use GB200's and NVIL? How many other decisions are there to be made? There is coupling from the model

architecture to what is the physical plan that's optimized >> and it's also scary in that sense which is hey there's going to be a new chip that will come out which obviously I mean you take Vera Rubin ultra I mean

that's going to have power density that's going to be so different with cooling requirements that are going to be so different right so you kind of don't want to just build all to one spec so that goes back a little to I think the dialogue we'll have which is

>> you want to be scaling in time >> as opposed to scale once and then be stuck with it. >> When you look at all the past technological transitions whether it be you know railroads or the internet or you know replaceable parts and

translization uh the cloud all of these things each revolution has gotten much faster in the time it goes from technology discover to ramp and pervasiveness through the economy. Many folks who have been on Darkesh's podcast believe this is sort of the final uh

technological revolution or transition and this time is very very different. Um and at least so far in the markets it's sort of you know in three years we've already skyrocketed to you know hyperscalers are doing $500 billion of capex next year which is a scale that's

un unmatched to prior revolutions in terms of speed and the end state seems to be quite different. How how do you your your framing of this seems quite different than sort of the I would say the AI bro who is who is quite um you know AGI is coming and you know I'd like

to understand that more. >> Yeah. I mean look I I I start with the excitement that I also feel for maybe after the industrial revolution this is the biggest thing. Um and so therefore I I I I I start with that premise. uh but at the same time I'm a little grounded

in the fact that uh this is still early innings. Uh we've built some very useful things. We're seeing some great properties. These scaling laws seem to be working. Um and I'm optimistic that they'll continue to work right. Some of it is um you know it does require real

science breakthroughs but it's also a lot of engineering and what have you. But that said, I also sort of take the view that you know even what has been happening in the last 70 years of computing uh has also been a march uh that has helped us move um you know with

as I said you know I like one of the things that Raj ready has as a metaphor for what AI is right he's a he's a turing award winner out of CMU um and he's always I think he had this even pre- AGI but he had this metaphor of AI should either be a guardian angel or a

cognitive amplifier. I love that uh it's a simple way to think about what this is ultimately what is its human utility? It is going to be a cognitive amplifier uh and a guardian angel. And so if I sort of view it that way, I view it as a tool. But then you can also go very

mystical about it and say, "Wow, this is you know more than a tool. It does all these things which only humans did so far." But that has been the case with many technologies in the past. only humans did a lot of things and then we add tools that did them.

>> I guess we don't have to get wrapped up in their definition here, but maybe one way to think about it is like maybe it takes 5 years, 10 years, 20 years. At some point eventually a machine is producing Satya tokens, right? And the Microsoft board thinks that Satia tokens

are worth a lot. >> How much how much are you wasting of his of like economic value by interviewing Satya?

>> You cannot afford the API cost of Satia tokens. Um but so you know whatever you want to call it is that are the SATA tokens a tool or an agent whatever. Um right now if you have models that cost on the order of dollars or cents per million tokens there's just an enormous

room for expansion uh a margin expansion there where sad a million tokens of SA are like worth a lot um and where does that margin go and what level of that margin is Microsoft involved in is a question I have. So I think um in in some sense this goes back a band to

essentially what's the economic growth picture going to really look like? Um what's the firm going to look like?

What's productivity going to look like?

And that to me is where again if the industrial revolution created after whatever 70 years of diffusion is when you started seeing the economic growth, right? it took that's the other thing to remember is um even if the tech is diffusing fast uh this time around for

true economic growth to appear it has to sort of diffuse to a point where the work the work artifact and the workflow has to change and so that's kind of one place where I think uh the change management required for a corporation to truly change I think is something we

shouldn't discount so I think going forward do humans and the tokens they produce get higher leverage, right? Uh whether it's the Dark Cesh or the Dylan tokens of the future. I mean, think about the amount of techn would you be able to run semi analysis or this

podcast without technology? No chance, right? I mean, the scale that you have been able to achieve, no chance. So, the question is what's that scale? Is it going to be 10xed with something that comes through? Uh absolutely. uh and therefore within your ramp to some

revenue number or your ramp to some audience number or what have you and so that I think is what's going to happen right I mean the the point is uh that's whatever what took 70 years maybe 150 years for the industrial revolution may happen in 20 years 25 years that's a

better way to like I would love to compress what happened in 200 years of the industrial revolution into 20-year period if you're lucky >> so Microsoft historically has been perhaps you know the greatest software company the largest software as a

service company you know you've gone through a transition in the past where you used to sell Windows licenses and discs of Windows or Microsoft and now you sell you know subscriptions to 365 or um as as we go from sort of you know that transition to where your business

is today um there's also a transition going after that right uh software as a service incredibly low incremental cost per user uh there's a lot of R&D there's a lot customer acquisition cost. This is why not Microsoft but the SAS companies have underperformed massively in the

markets because the cogs of AI is just so high and that just completely breaks how these business models work. >> How do you as a as as a as perhaps the greatest software company um software as a service company transition Microsoft to this new age where COGS matters a lot

um and and the incremental cost per users is different right because right now you're charging hey it's 20 bucks for a co-pilot. >> Yeah. So I think that this is a it's a great question because in some sense the business models themselves I think the

levers are going to remain similar right which is if I look at the the if if you look at the menu of models uh starting from like say consumer all the way right there will be some ad unit uh there will be some transaction there will be some device gross margin for somebody who

builds an AI device um uh there will be subscriptions consumer and enterprise uh and then there'll be consumption right so I still think that that's kind of how those are all the meters. To your point, what is a subscription? Up to now, people like subscriptions because they

can budget for them, right? They are essentially entitlements to some consumption rights that come encapsulated in a subscription. So that I think is what in some sense it becomes a pricing decision. Uh so how much consumption is in you are entitled to is

if you look at all the coding subscriptions that's kind of what they are, right? and they kind of have the pro tier, the standard tier and what have you. And so I think that's how the pricing will uh you know and the margin structures will get tiered. Um the

interesting thing is having at Microsoft the good news for us is we kind of are in that business uh all in across all those meters. in fact that at a as a portfolio level uh we pretty much have consumption subscriptions uh to all of the other consumer levers

as well. Um and then I think time will tell which of these models make sense in what categories. Um, one thing on the SAS side since you brought up which I think a lot about is uh take uh Office 365 or Microsoft 365. I mean man having a low RPO is great because here here's

an interesting thing right during the transition from server to cloud one of the questions we used to ask ourselves is oh my god if all we did was just basically move the same users who were using let's call it our office licenses and our servers at that time office

servers right to the cloud and we had cogs this is going to basically not only shrink our margins uh but we'll be fundamentally a nonprofitable I mean less profitable company except What happened was the move to the cloud expanded the market like crazy. Uh right

I mean we sold a few servers in India didn't sell much whereas in the cloud suddenly everybody in India also could afford fractionally buying uh servers. The IT costs I in fact the biggest thing I had not realized for example was the amount of money people were spending

buying storage underneath SharePoint. In fact, EMC's biggest segment may have been storage servers for SharePoint. All that sort of dropped in the cloud because nobody had to go buy in fact it was working capital. I mean basically it is cash flow out right and so it

expanded the market massively. So this AI thing will be that right. So if you take coding um what we built with GitHub and VS code in over whatever decades uh suddenly the coding assistant is that big in one year and so that I think is what's going to

happen as well which is the market expands massively. M I I guess there's a question of the market will expand. Will the parts of the revenue that touch Microsoft expand? So copilot is an example where if you look uh early this year I think uh I guess according to

Dylan's numbers um the co-pilot revenue github co-pilot revenue was like 500 million or something like that and then u there were like no close competitors whereas now you have claude code cursor and copilot with around similar revenue around a billion and then codeex is

catching up around 700 800 million and so the question is across all the services that Microsoft has access to what is the advantage that mic Microsoft's equivalents of Copilot have. >> Yeah, by the way, I love this chart. You know, I love this chart for so many

reasons. One is we're still on the top. >> Um, second is all these companies that are listed here are all companies that have been born in the last four or five years. >> Yeah, >> that to me is the best sign, right?

Which is if you have new competitors, new existential problems, when you say, man, who's it now? Claude's going to kill you. Cursor is going to kill you. It's not Borland, right? So, thank God. like that means we are in the right direction but this is it right the fact

that we went from nothing to this scale is the market expansion so this is like the cloud-like stuff this fundamentally this category of coding and AI is probably going to be one of the biggest categories right it is a software factory category in fact it may be

bigger than knowledge work >> so I kind of want to keep myself open-minded about I mean we're going to have tough competition I think that's your point which I think is a great on uh but man like I'm glad we have we parlayed uh what we had into this and

now we have to compete and so in the compete side uh even in the last quarter we just we did our quarterly uh announcement I think we grew from 20 to 26 million subs right so I feel good about our sub growth uh and where the direction of travel on that is but the

more interesting thing that has happened is guess where all the repos of all these other guys uh who are generating lots and lots of code go to they go to GitHub so it GitHub is at an all-time high in terms of repo creation PRs everything so that in some sense we want

to keep that open by the way that means we want to have that right because we don't want to conflate that with our own growth right the interestingly enough we're getting one developer joining GitHub a second or something that is the stat I think and then 80% of them just

fall into some GitHub copilot uh workflow just because there are and by the way many of these things will even use some of our coding code review agents which are by default on just because you can use it. So we'll have many many structural shots at this. The

thing that we're also going to do is what we did with git g get the primitives of github whether starting with git to issues to actions these are powerful lovely things because they kind of are all built around your repo. So we want to extend that last week at GitHub

Universe. That's kind of what we did, right? So we said agent HQ was the conceptual thing that we said we're going to build out. This is where for example you have a thing called mission control and you go to mission control and now I can fire off sometimes I

describe it as the cable TV of all these AI agents because I'll have essentially packaged into one subscription codeex claude um you know cognition staff anyone's agents gro all of them will be there so I get one package and then I can literally go issue a task

steer them so they'll all be working in their independent branches. Uh I can monitor them. Uh so I literally have because I think that's going to be one of the biggest places of innovation, right? Because right now I want to be able to use multiple agents. I want to

be able to then digest the output of the multiple agents. I want to be able to then keep a h a handle on my repo. So if there's some some kind of a heads up display that needs to be built and then for me to quickly steer and triage what the coding agents have generated that to

me between VS code GitHub and all of these new primitives we'll build uh as mission control I think uh with a control plane observability I mean think about every uh one who is going to deploy all this will require a whole host of observability of what agent did

what at what time to what code base so I feel that's the opport opportunity uh and at the end of the day your point is well taken which is we better be competitive and innovate and if we don't yes we will get toppled but I like the chart at least as long as we're on the

top even with competition >> the key point here is sort of that GitHub will keep growing irregardless of whose coding agent wins but that that market only grows at you know call it 10 15 20% which is way above GDP it's a great compounder but these AI coding

agents have grown from you know call it $500 million run rate at the end of last year which was basically ally just GitHub copilot to now the current run rate across you know GitHub copilot cloud code cursor cognition wind surflet uh codeex open codeex that's that's

that's run rating at 56 billion now um for the for the Q4 of of this year that's a 10x right and and when you look at hey what's the TAM of of software agents is it is it the $2 trillion of wages you pay people or is it is it is it something beyond that uh because

every company in the will now be able to, you know, develop software more. >> No question Microsoft takes a slice of that, but you've gone from near 100% or certainly way above 50% to, you know, sub 25% market share in just one year.

What is the sort of confidence that people can get that Microsoft >> there's no again it goes back a little bit D to sort of there's no birthright here that we should have any confidence other than to say hey we should go innovate and knowing the the lucky break

we have in some sense is that uh this category is going to be a lot bigger than anything we had high share in let's let me say it that way right in some sense you could say man we kind of had high share in VS code we had high share in the repos for with GitHub uh uh and

that was a good market but the point is even having a decent share in what is a much more expansive market right I mean you could say we had a high share in client server server computing we have much lower share than that in hypers scale but is it a much bigger business

by orders of magnitude so at least there's existence proof that Microsoft's been okay uh even if our share position has not been as strong as it was uh as long as the markets we're competing in creating more value and there are multiple winners. Uh so I think that's

the stuff but I I I take your point that ultimately it all means you have to get competitive. So I watch that every quarter and so that's why I think what I'm very optimistic that uh what we're going to do with GitHub HQ and or agent HQ turning GitHub into a place where all

these agents come uh as I said we'll have multiple shots on goal on there right it it need not be that hey some of these guys can succeed along with us uh and so it doesn't need to be just one winner uh and one subscription >> I I guess the reason to focus on this

question is that it's not just about GitHub but fundamentally about office and all the other software that Microsoft offers which is that one vision you could have about how I proceeds is that look the models are going to keep being hobbled then you'll

need this direct visible um observability all the time and another vision is over time these models can now they're doing tasks that take two minutes in the future they'll be doing tasks that next be tasks that take 10 30 minutes in the future maybe they're

doing days worth of work autonomously and then the model companies are charging thousands of dollars maybe for access to really a co-orker which could use any UI to communicate with their human and so forth and migrate between platforms. So if we're getting closer to

that, why aren't the model companies that are just getting more and more profitable, the ones that are taking all the margin, why is the the place where the scaffolding happens, which becomes less and less relevant as a as become more cap capable going to be that

important? And that goes to, you know, office as it exists now versus co-workers that are just doing knowledge work. >> Great point. I mean I think that's a I mean for example I mean this is where you know does all the value migrate just

to uh the model um and uh or does the you know the does it get split between the scaffolding um and the model and what have you I think that uh time will tell but my my fundamental point also is the incentive structure gets clear right which is if you take um let's take uh

let's take information work or take even coding u already in fact one of favorite settings I have uh in GitHub copilot is called auto um right which will just optimize in fact I buy a subscription the auto one will start picking and optimizing for what I am asking it to do

uh and it could even be fully autonomous and it could sort of arbitrage the tokens available across multiple models to go get a task done so if that is the that means that if you take that argument the commodity there will be models

uh and especially with open source models you can pick a checkpoint and you can take a bunch of your data and you're seeing it right I think all of us will start you whether it's from cursor or from Microsoft you'll start seeing some in-house models even uh which will and

then you'll offload most of your uh task to it so I think that one argument is if you win the scaffolding uh which today is dealing with all the hobbling problems or the uh the jaggedness of this intelligence problems which you kind of have to um if you win that then

you will vertically integrate yourself into the model just because you will have the liquidity of the data and what have you and there are enough and more checkpoints that are going to be available. uh that's the other thing right so structurally I think there will

always be an open- source model uh that will be fairly capable in the world that you could then use as long as you have something that you can use that uh with which is data uh and a scaffolding right so I can make the argument that oh my god uh if you're a model company you may

be you may have a winner's curse you may have done all the hard work done unbelievable innovation except it's kind of like one copy uh away from that being commoditized and then the person who has the data for grounding and context engineering um and the liquidity of data

can then go take that checkpoint and train it. So I think the argument can be made both ways. >> Unpacking sort of what you said, there's two views of the world, right? One is that models, there's so many different models out there. Open source exists.

There will be differences between the models that will drive some level of, you know, who wins and who doesn't, but the scaffolding is what enables you to win. The other view is that actually models are the key IP and yes, we're in a very everyone's in a tight race and

there's some, you know, hey, I can use anthropic or open AAI and you can see this in the revenue charts, right? like OpenAI's revenue started skyrocketing once they finally had a code model similar capabilities to anthropic although in different ways. Um

there's a view that like the model companies are actually the ones that garner all the margin right because you know if you look across this year at least on entropic their gross margins on inference went from you know well below 40% to north of 60 right by the end of

the year um the these the margins are expanding there despite hey more Chinese open source models than ever hey open's competitive hey Google's competitive hey x Grock is now competitive right all these all these companies are now competitive and yet despite this the

margins have expanded at the model layer significantly. Um h how do you think about the >> it's a it's a great question. I I think that the one thing is perhaps a few years ago people were saying oh I can just wrap a model and build a successful

company. Uh and that I think is probably gotten debunked just because the model capabilities um and with tools use in particular. But the interesting thing is there's no like when I look at Office 365. Let's take even this little thing we built

called Excel agent. It's interesting right? Excel agent is not a UI level wrapper. It's actually a model that is in the middle tier. Uh in this case because we have all the IP from the the GPT family. uh we are taking that and putting it into the core middle tier of

the office system to both teach it what it means to natively understand Excel everything in it. So it's not just hey I just have a pixel level understanding I have an full understanding of all the native artifacts of Excel uh both when I see it like because if you think about

it if I'm going to give it some reasoning task right I need to even fix the reasoning mistakes I make and so that means I need to both not just see the pixels I need to be able to see oh I got that formula wrong and I need to understand that and then so to some

degree that's all being done not at the UI wrapper level with some prompt but it's being done in the middle tier by teaching it all the tools of Excel, right? So, I'm giving it even essentially a markdown to teach it the skills of what it means to be a

sophisticated Excel user. So, it's a weird thing that it it goes back a little bit to AI brain, right? Which is you're building not just Excel. You are now business logic in its traditional sense. You're taking the Excel business logic in the traditional sense and

wrapping essentially a cognitive layer to it using this model which knows how to use the tool. So in some sense, Excel will come with an analyst bundled in and with all the tools used. >> That's the type of stuff that'll get built by everybody. So even for the

model companies, they'll have to compete, right? So if they price stuff high, uh guess what? If I'm a builder of a tool like this, I'll substitute you. I may use you for a while. And so as long as there's competition, there's always a winner take all thing, right? If there's

going to be one model that is better than everybody else with massive distance, yes, that's a winner take all. As long as there's going to be competition where there's multiple models just like hypers scale competition and there's an open- source

check, uh there is enough room here uh to go build value on top of models. >> Uh but at Microsoft, the way I look at it and say is uh we are going to be in the hypers scale business which will support multiple models. we will have access to open AI models for uh you know

seven more years which we will innovate on top of so royalty I mean essentially I think of ourselves as having a frontier class model uh that we can use and innovate on with full uh flexibility and we'll build our own models uh with Mai um and and so we will always have a

model level and then we'll build these whether it's in security whether it's in knowledge work whether it's in coding or in science we will build our own applications scaffolding which will be model forward right it won't be a wrapper on a model but the model will be

wrapped into uh the application I have so many questions about the other things you mentioned but before we move on to those topics um I still wonder whether this is like not forwardlooking on AI capabilities where you're imagining models like they exist today where yeah

I can you have to like it takes a screenshot of your screen but it can't like look inside each cell and what the formula is and I think the better mental audited here is like look a human just imagine that these models actually will be able to actually use a computer as

well as a human and a human knowledge worker who is using Excel can look into the formulas can you know use alternative software can migrate data between office 365 and another piece of software if the migration is necessary etc so that's kind of what I'm saying

but if that's the case then the integration with Excel doesn't matter that much don't worry about the Excel integration after all Excel was built as a tool for anal analysts. Great. So, whoever is this AI that is an analyst should have

tools that they can >> computer, right? Just the way a human can use a computer, that's their tool. >> The the tool is the computer. Right. >> Right. So, that so all I'm saying is I'm building an analyst as as essentially an AI agent uh which happens to come with

an a priority knowledge of how to use all of these analytical tools. But is it is it something maybe just just to make sure we're talking about the same thing. Um is it a thing that a hum like me using Excel as a podcast completely autonomous? So just imagine I

work like so we should now maybe sort of lay out how I think the future of the company is right. uh the future of the company would be the tools business which I have a computer I use Excel and in fact in the future I'll even have a co-pilot and that co-pilot will also

have agents right that's still I am I you know it's still me steering everything >> and everything is coming back so that's kind of one world >> then the second world is the company just literally provisions a computing

resource for an AI agent >> and that is working fully autonomously >> that fully autonomous agent will have essentially embodied set of those same tools, >> right?

>> Uh that are available to it, right? So this AI tool that comes in also has not just a raw computer uh because it's going to be more token efficient to use tools to get stuff done. In fact, I kind of look at it and say our business which today is an enduser tools business will

become essentially an infrastructure business in support of agents doing work. Is there another way to think about it? Right? So if one of the things that you'll see us do in in in fact like all the stuff we built underneath M365 still is going to be very relevant uh

you need someplace to store it someplace to do archival someplace to do discovery someplace to manage all of these activities even if you're an AI agent. >> So that's so it's kind of a new infrastructure. So ju just to make sure I understand you're saying like look

theoretically a future AI that has actual computer use which is all these companies are working on model companies are working right now could use even if it's not partnered with Microsoft or under our umbrella could use Microsoft software but you're saying we're going

to give them if if you're working with our infrastructure we're going to give you like lower level access that makes it more efficient for you to do the same things you could have otherwise done anyways.

>> 100%. I mean so the entire thing in in fact the way the you know like what happened is we had servers then there was virtualization and they had many more servers. So that's another way to think about this which is hey don't think of this the tool as the end thing

what is the entire substrate underneath that tool that humans use and that entire substrate is the bootstrap for the AI agent as well because the AI agent needs a computer that's kind of one like you know so in fact one of the fascinating things we're seeing a

significant amount of growth is all these guys who are doing these office artifacts and and what have you as autonomous agents and so on want to provision Windows 365 right? They really want to be able to provision a computer for these agents. Uh and so absolutely

and that's where I think we're going to have essentially an enduser computing infrastructure business which I think is going to just keep growing because guess what it's going to grow faster than the number of users. So in fact that's kind of one of the other questions people ask

me is hey what happens to the per user business at least the early signs may be the way to think about the per user business is not just per user it's per agent and if you sort of say it's per user and per agent the key is what's the stuff to provision for every agent a

computer u a set of security things around it an identity around it uh and all those things observability and so on are the management layers and that's I think all going to get baked into that >> the way to frame it at least the way I currently think about it and I'd like to

hear your you know your view is that >> uh these model companies are all building environments to train their models to use Excel or Amazon shopping or whatever whatever it is book flights um but at the same time they're also training these models to do migration

from because that that is probably the most immediate uh valuable thing right converting mainframe based systems to standard cloud systems converting um Excel databases into real databases uh with SQL, right? Or or converting um you know what is done in Word and Excel to

something that is more programmatic and more efficient in a classical sense that can actually be done by humans as well. It's just not cost-ffective for the software developer to do that. That seems to be what everyone is going to do with AI for the next, you know, few

years at least to massively drive value. Um h how does Microsoft fit into that?

if the models can utilize the tools themselves to migrate to something and yes Microsoft has you know a leadership position in databases and in storage and and in all these other categories but the use of say a office ecosystem is going to be significantly less just like

potentially the use of a mainframe ecosystem could be potentially less now mainframes have grown for the last two decades actually even though no one talks about them anymore they've still grown 100% I agree with that how does how does that flow forward

>> I mean at the end of the day This is not about sort of hey u there is going to be a significant amount of time where there's going to be a hybrid world right because people are going to be using the tools that are going to be working with agents that have to use tools and by the

way they have to communicate with each other what's the artifact I generate that then a human needs to see so like all of these things will be real considerations in any place so the outputs input so I don't think it'll just be about oh I migrate it off right

but the bottom line is I have to live in this hybrid world so let's but that doesn't fully answer your question because there can be a real new efficient frontier where I it's just agents working with agents uh and completely optimizing and even when

agents are working with agents what are the primitives that are needed uh do you need a storage system >> uh does that storage system need to have eiscocovery does that eiscocovery do you need to have observability do you need to have an identity system that is going

to use multiple models with all having one identity system so these are all the core underlying rails we have today for what are office systems or what have you. Uh and that's what I think we will have in the future as well. You talked about databases, right? I mean take you

know man I would love all of Excel to have a database backend, right? In fact I would love for all that to happen immediately. Uh and that database is a good database. I mean databases in fact will be a big thing that'll grow. uh in fact if I think about all of the office

artifacts uh being structured better the ability to do the joins between structured and unstructured better because of the agenting what that'll grow the underlying what is infrastructure business it happens the consumption of that is all being driven

by agents you could say all that is just in time generated software by a model company that could also be true if we we will be one such model company too uh and so we will build in so the competition could be uh that we will build a model plus all the

infrastructure and provision it and then there will be competition between a bunch of those folks who can do that. H um I guess speaking of model companies you say okay we will also be one of the not only will we have the infrastructure we'll have the model itself right now

Microsoft AI's most recent model that was released 2 months ago is 36 and Shabbat arena and there's a I mean you obviously have the IP rights to open so there's a question of first to the extent you agree with that it seems to be behind why is that the case

especially given the fact that you could um you theoretically have the right to just like fork open's monor repo or distill on their models Um yeah, especially if it's a big part of your strategy that we need to have a leading model company.

>> Yeah. I mean, so first of all, we are absolutely going to use the OpenAI models uh to the maximum uh across all of our products, right? I mean, that's I think the core thing that we're going to continue to do all the way for the next seven years. Uh and not just use it uh

but then add value to it. That's kind of where the analyst in this Excel agent and these are all things that we will do where you know we'll do I'll you know RL fine-tuning we'll do some mid-training runs on top of a GPT family where we have unique data assets and build

capability um the MI model the way I think we're going to think about it is the the good news here in fact with the new agreement is even we can be very very clear that we're going to build a worldclass super intelligence team and go after it with

high ambition but that at the same time. We're also going to use this time to be smart about how to use both these things. So that means we will on one end be very product focused on on the other end be very research focused. In other words, uh because we have access uh to

the GPT family. The last thing I don't want to do is use my flops in a way that is just duplicative and doesn't add much value. So I want to be able to take uh the flops that we use to generate a GPT family and maximize its value while my MAI flops are being used for let's take

the image model that we launched which I think just launched uh it's a number nine in the uh image arena you know we're using it you know both for cost optimization it's on copilot it's in Bing and we're going to use that we have a audio model in copilot which it's got

personality and what have you optimized it for our product So we will do those even on the LM Marina we started on the text one I think it was it debuted at night 13 and by the way it was it was done only on whatever 15,000 uh H100s and so it was a very small model and uh

so it was again to prove out uh the core capability the instruction following and everything else which but you know we wanted to make sure we can match what was state-of-the-art and so that shows us given scaling laws what we are capable of doing if you gave more flops

to it right so the next thing we will is an omni model where we will take sort of the work we've done in audio, what we've done in image and what we've done in text, that'll be the next pit stop on the MAI side. So when I think about the MAI road map, we're going to build a

first class super intelligence team. We're going to continue to drop and do on in the open some of these models, they will either be in our products being used because they're going to be latency friendly, cogs friendly, or what have you, or they'll have some special

capability. and we will do real research in order to be ready for some next five, six, seven, eight break breakthroughs uh that are all needed on this march towards super intelligence. So I think that's and while exploiting the advantage we have of having the GPT

family that we can work on top of as well. >> Say we roll forward seven years uh you no longer have access to open AI models. what does one get confidence or what does Microsoft do to make sure they are leading or have a leading AI lab right

today you know it's it's all open has developed many of the breakthroughs whether it be scaling or reasoning or Google's developed all the breakthroughs like transformers uh but but it it is also a big talent game right you know you've seen meta spend you know north

of20 billion on talent right uh you've seen anthropic uh poach the entire blue shift reasoning team from Google last year you've seen meta poach a large reasoning and post training team from Google more recently. These these sorts of talent wars are very capital

intensive. They're the ones that, you know, arguably, you know, if you're spending hundred billion dollars on infrastructure, you should also spend, you know, x amount of money on on the people using the infrastructure so that they're more efficiently making these

new breakthroughs. what what confidence can one get that you know hey Microsoft will have a team that's world class that can make these breakthroughs and you know once you decide to turn on the money faucet you know you're being a bit capital efficient right now which is

which is smart it seems uh to not waste money doing duplicative work but once you decide you need to you know how how can one say oh yeah now you can shoot up to where the top five model >> well look I mean at the at the end of the day we're going to build a

world-class team and we already have a world-class team that's beginning to be sort of assembled right Mustafa coming in. We have Karen. We have Amar Subramanyan who did a lot of the post training at Gemini. Tufi who is at Microsoft. Nando who did a lot of the

multimedia work at Deep Mind is there. And so we're going to build a worldclass team. And in fact I think later this week even Mustafa published some you know a little more clarity on what our lab is going to go do. Um I think the thing that I want uh the world to know

perhaps uh is we are going to build the infrastructure that'll support multiple models. Uh you know uh we because from a hypers scale perspective we want to build the most scaled infrastructure fleet that's capable of supporting all the models the world needs whether it's

from open source or whether it's obviously from open AI and others and so that's kind of one job. Second is in our own model capability. We will absolutely use the open AI model in our products and we will start building our own models and we may like in in GitHub

copilot anthropic is used. So we will even have other frontier models that are going to be wrapped into our products as well. So I think that that's kind of how at least each time at the end of the day the eval of the product as it meets a particular task or a job is what matters

and we'll sort of back from there into the vertical integration needed. uh knowing that as long as you're service you know you're serving the market well with the product you can always cost optimize >> there there's a question going forward

so right now we have models that have this distinction between training and inference and one could argue that there's like a smaller and smaller difference between the different models um going forward if you're really expecting something like human level

intelligence humans learn on the job you know if you think about your last 30 years what makes s token so valuable it's the last 30 years of wisdom and experience you've gained at Microsoft Um and we will eventually have models if they get to human level which will have

this ability to continuously learn on the job and that will drive so much value to the model company that is ahead at least in my view because you have copies of one model broadly deployed through the economy learning how to do every single job and unlike humans they

can amalgamate their learnings to that model. So there's this sort of continuous learning sort of exponential feedback loop um which almost looks like a sort of intelligence explosion. uh if that happens and Microsoft isn't the leading model company by that time

doesn't then this uh you know you're saying well we substitute one model for another etc matter less because it's just like this one model knows how to do every single job of the economy the other long tale of don't >> yeah no I think your point about if

there's one model that is the only model that is most broadly deployed in the world and it sees all the data and it does continuous learning that's game set match and you know is shut right I mean the the reality at least I see um is the world even today for all the dominance

of any one model it's not the case um it's like take any take coding there's multiple models in fact every day it's less the case where there is not one model that is getting deployed broadly in fact there's multiple models that are getting deployed it's kind of like

databases right it's always the thing it's like hey can one database be the one that just is used everywhere except it's not uh there are multiple types of databases that are getting deployed uh for different use cases. So I think that there is going to be some network

effects of continual learning or data you you know I'll call liquidity that any one model has. Uh is it going to happen in all domains? I don't think so. Is it going to happen in all geos? I don't think so. Is it going to happen in all segments? I don't think so. It'll

happen in all categories at the same time. I don't think so. So therefore I feel like the design space is so large uh that there's plenty of opportunity but your fundamental point is having a capability which is at the infrastructure layer, model layer and at

the scaffolding layer and then to be able to compose these things not just as a vertical stack but to be able to compose each thing for what its purpose is. Right? You can't build an infrastructure that's optimized for one model. If you do that what if you go

fall behind? In fact, all the infrastructure you built will be a waste, right? You kind of need to build an infrastructure that's capable of supporting multiple sort of families and lineages of models. Otherwise, the capital you put in which is optimized

for one model architecture. That means you're one tweak away from some like breakthrough that happens for somebody else and your entire network topology goes out of the window. Then that's a scary thing, right? So therefore, you kind of want the infrastructure to

support whatever may come. in fact in your own model family and other model families and you got to be open if you if you're serious about the hypers scale business you got to be serious about that right um if you're serious about being a model company you've got to

basically say hey what are the ways people can actually do things on top of the model so that I can have an ISV ecosystem unless I'm thinking I'll own every category that just can't be that then you won't have an API business and that by definition will mean you'll

never be uh a platform company that's going to be successfully deployed everywhere right So therefore the industry structure is so such that it will really force people to specialize and that in that specialization

a company like Microsoft should compete in each layer by its merits uh but not think that this is all about all a road to game set match where I just compose vertically all these layers. That's that just doesn't happen. So according to Dylan's numbers, there's going to be

half a trillion in AI capex next year alone, and labs are already spending billions of dollars to snag top researcher talent. But none of that matters if there's not enough highquality data to train on. Without the right data, even the most advanced

infrastructure and world-class talent won't translate into end value for the user. That's where Libox comes in. Libox produces highquality data at massive scale, powering any capability that you want your model to have. It doesn't matter whether you need a coding agent

that needs detailed feedback on multi-our trajectories or a robotics model that needs thousands of samples on everyday tasks or a voice agent that can also perform real world actions for the user like booking them a flight. To be clear, this isn't just off-the-shelf

data. Labelbox can design and launch a custom production scale data pipeline in 48 hours and they can get you tens of thousands of targeted examples in weeks. Reach out at labelbox.com/darkh. All right, back to Satia.

>> So last year Microsoft was on path to be the largest infrastructure provider uh by far. You were the earliest in 23. So you you went out there, you acquired all the resources in terms of leasing data center, starting construction, securing power, everything. You guys were on pace

to beat Amazon in 26 or 27. Um but certainly by 28, you were going to beat them. Um since then, you you know, in let's call it the second half of last year, Microsoft did this big pause, right, where they let go of a bunch of leasing sites that they were going to

take, which then Google, Meta, um Amazon, in some cases, Oracle, uh took these sites. We're sitting in one of the largest data centers in the world. So obviously it's not everything. You guys are expanding like crazy. Uh but there are sites that you just stopped working

on. >> Why why did you do this? Right. >> Yeah. I mean the fundamental thing we this goes back a little bit to what is the hypers scale business all about right which is one of the key decisions we made was that if you're going to

build out Azure to be fantastic for all sort of stages of AI uh from training to mid-training to data genen to inference we just need fungibility uh of the fleet um and and so that entire thing caused us not to basically go build a a whole lot of capacity with a particular set of

generations. Uh because the other thing that you got to realize is having actually for up to now 10xed every 18 months enough training capacity for the various open AI models. uh we realize that um the key is to stay on that path but the more important thing is to

actually have a balance to not just train but to be able to serve these models all around the world because at the end of the day the rate of monetization is what then will allow us to even keep uh funding and then the infrastructure was going to need us to

support as I said multiple models and what have you. So once we said that that's the case since then we just course corrected to where the path we're on right if I look at the path we're on is we're doing lot more starts now uh we're also buying up as much capacity as

we can whether it's to build whether it's to lease or even GPUs as a service but we're building it for where we see the demand uh and the serving needs and our training needs and we didn't want to just be a host for one company uh and have just a massive book of business

with one customer that that's not a business right that is sort of uh you know you should be vertically integrated with that company uh and so given the the thing that openai was going to be a successful independent company which is fantastic right I think it's makes sense

right and even meta may use third party capacity but ultimately they're all going to be first party uh for anyone who has large scale they'll be you know they'll be a hyperscaler on their own and so to me was to build out a hypers scale fleet and our own research

compute. Uh and that's what the adjustment was. Um you know and then and so I feel very very good. Oh by the way the other thing is I didn't want to get stuck with massive scale of one generation. I mean we just saw the the GB200s. I mean the GB300's are coming

right and by the time I get to Vera Rubin Ver Rubin ultra guess what the data center is going to look very different because the power per rack power per row is going to be so different uh the cooling requirements are going to be so different and that

that means I don't want to just go build out like a whole number of gigawatts that are only for one generation one family and so I think the pacing matters and the funibility and the location matters uh the workload diversity matters,

customer diversity matters and that's what we're building towards. The other thing that we've learned a lot is um every AI workload does require not only the AI accelerator but it requires a whole lot of other things right and in fact a lot of the margin structure for

us will be in those other things and so therefore we want to build out Azure as being fantastic for the long tail of the workloads because that's the hypers scale business while knowing that we've got to be super competitive starting with the bare metal for the highest end

training And but that can't crowd out the rest of the business, right? Because we're not in the business of just doing five contracts with five customers being their bare metal service. That's not a a Microsoft business. That may be a business for someone else and that's a

good thing. What we have said is we are in the hypers scale business which is at the end of the day a longtail business uh for AI workloads and in order to do that we will have some leading bare metal as a service capabilities for a set of models including our own uh and

that I think is the balance you see the another sort of question that comes around this whole fungeibility topic is okay it's not where you want it right you would rather have it in a good population center like Atlanta we're here um there there's there's There's

also the question of like well how much does that matter if as the horizon of AI tasks grows well actually you know 30 seconds for a reasoning prompt or you know 30 minutes for a deep research or you know it's going to be hours for software agents at some point um and

days and so on and so forth the time to human interaction why does it matter if it's if it's a great it's a great question >> a location A B or C >> that's exactly right so in fact that's one of the other reasons why we want to

think about like hey what is an Azure region look like and what is the in fact the networking between Azure regions. So this is where uh I think as the model capabilities evolve and I think the usage of these tokens whether it's synchronously or asynchronously evolves

and in fact you don't want to be out of position right then on top of that by the way what are the data residency laws right where do I like I mean the entire EU thing uh for us where we literally had to create an EU data boundary basically meant that you can't just

round trip a call to wherever even if it's asynchronous and so therefore you need to have maybe regional things that are high density and then the power costs and so on. But you're 100% right in bringing up that the topology as we build out uh will have to evolve one for

tokens per dollar per watt uh what are the economics overlay that with what is the usage pattern um usage pattern in terms of synchronous asynchronous but also what is the compute storage because the latencies may matter for certain things

uh the storage better be there if I have a Cosmos DB close to this for session data or even for an autonomous thing then that also has to be somewhere close to it and so on. So I think that all of those considerations is what will shape uh the hypers scale business. M

>> you know prior to the pause you were you were you you know versus you know what we had forecasted for you by 28 you're going to be like 12 13 gawatt and now we're at you know 9 and a half or so right but you know something that's even more relevant right and it's it's you

know I just want you to like more concretely state that this is the business you don't want to be in but like Oracle is going from like 1/5if your size to bigger than you by end of 2027 and while it's not a Microsoft level quality of return on invested

capital right they're still making 35% gross margins, right? sort of the question is like does it is it isn't it is it is it you know hey it's not Microsoft's business to maybe do this but you've created a hyperscaler now by refusing this business by giving away

the right of first refusal whatever I'm not first of all I don't I don't want to take away any thing from the success Oracle has had in building their business and I wish them well and so the thing that I think I've answered for you is it didn't make sense for us uh to go

be a host for one model company uh with limited time horizon RPO let's let's just put it that way right the thing that you have to think through is not what you do in the next 5 years but what do you do for the next 50 uh because that's kind of what I we made our set of

decisions um I feel very good about our open AI partnership and what we're doing we have a decent book a book book of business we wish them a lot of success in fact we are buyers even of Oracle capacity we wish them success but you know at this point. I think the

industrial logic for what we're trying to do is pretty clear which is it's not about like chasing I first of all I track by the way your uh things whether it's the AWS or the Google and ours which I think is super useful uh but doesn't mean I got to chase those uh I

have to chase them for not just uh the gross margin that they may represent in a period of time you know does m what what is this book of business that Microsoft uniquely can go clear which makes sense for us to clear and that's what we'll do. I I guess I have a

question even stepping back from this of okay I I take your point that it's a better business to be in all else equal to have a long tale of customers you can have higher margin from rather than just serving bare metal to a few labs but then there's a question of okay which

way is the industry evolving and so if we believe we're on the path to smarter and smarter AIs then why isn't the shape of the industry that the open AIs and anthropics and deep minds are the platform which the long tale of enterprises are actually doing business

with where they need bare metal but like they are the platform. What is the longtail that is directly using Azure um because you know you you want to use the general >> going to be available on Azure right? So any workload that says hey I want to use

um you know some open source model and an open AI model like I mean if you go to Azure foundry today you have all these models that you can provision buy PTUs get a cosmos DB get a SQL DB get some storage get some compute that's what a real workload looks like a real

workload is not just hey I did an API call to a model a real workload needs all of these things to go build an app or instantiate an application in fact the model companies need that right to build anything it's just not like I have a token factory I have to have all of

these things that's the hypers scale business uh and it's not any one model but all these models and so if you want grock plus let's say uh open AI plus an open source model come to Azure foundry provision them build your application here is a database that's kind of what

the business is uh you there is a separate business called just selling raw bare metal services to model companies and that's the argument about how much of that business you want to be in and not be in and what is that it's a very different segment of the business

which we are in and we also have limits to how much of it is going to crowd out the rest of it. Uh but that's kind of at least the way I look at it. So, so there's there's sort of two questions here, right? Like why why couldn't you just do both is one and then the other

one is um given, you know, our our estimates on what your capacity is in 2028 is 3 and a half gigawatts lower. Sure, you could have dedicated that to OpenAI training and inference capacity, but you could have also dedicated that to hey the this three and a half

gigawatts is actually just running Azure is running Microsoft 365 that's running GitHub copilot. it doesn't actually I could have built it and not given it to open AAI >> or I may want to build it in a different location. I may want to build it in UAE.

I may want to build it in India. I may want to build it in Europe. Right? So one of the other things is as I said like where we have real capacity constraints right now are given the regulatory needs and the data sovereignty needs. We got to build all

over the world. Uh it's first of all state side capacity is super important and we're going to build everything. But one of the things is when I look out to 2030 uh I have a sort of a global view of what does Microsoft shape of business by first party and third party third

party segmented by the frontier collabs and their how much they want versus the inference capacity we want to build for multiple models um and our own research compute needs right so that's all what's going into my calculus versus saying hey um I think you're rightfully pointing

out the pause but the pause was not done because we said oh my god we don't want to build that we realized that oh we want to build what we want to build slightly differently uh by both workload type as well as geo type and timing as well like we'll keep ramping up our

gigawatts and the question is at what pace and in what location and in what sort of how do I write even the mos's law on it right which is do I really want to overbuild 3 and a half in 27 or do I want to spread that in 2728 knowing even one of the biggest learnings we had

even with Nvidia is their pace increased uh in terms of their model I mean their migrations so that was a big factor I didn't want to go get stuck for four years 5 years of depreciation on one uh generation and I wanted to just basically buy like in fact Jensen's

advice to me was two things one is hey get on the speed of light execution that's why I think even the execution in this Atlanta data center I mean like in 90 days right between when we get it and to hand off to a real workload. that's sort of real speed of light execution on

their front and so I wanted to get good at that and then that way then I'm building this each generation and scaling uh and then every 5 years then you have a much more balanced so it becomes really literally like a flow uh for a large scale industrial operation

like this where you suddenly are not lopsided where you built up a lot in one time and then you take a ma massive hiatus because you're stuck with all this to your point in one location which may be great for training, may not be great for infants because I can't serve

even if it's like it's all asynchronous, but Europe ain't going to let me round trip to Texas. So, that's all of the things. >> How do I rationalize this statement with what you've done over the last few weeks? You've announced deals with Iris

Energy, um with Nebius, um and Lambda Labs, and there's a few more coming as well. Uh you're you're going out there and securing capacity that you're renting from the Neoclouds, um rather than having built it yourself. What was the what was

>> I think it's it's fine for us because we now have you know when you have line of sight to demand which can be served where people are building it it's great in fact we'll even have I would say you know we will take leases we will take build to suite we'll take even GPUs as a

service where we don't have capacity but we need capacity and someone else has that uh and by the way I would even sort of welcome every Neocloud to just be part of our marketplace uh because again guess what if if they go bring their capacity into our marketplace. That

customer who comes through Azure will use the Neocloud which is a great win for them and we'll use compute, storage, databases, all the rest from Azure. So I'm not at all thinking of this as just a you know hey I should just gobble up all of that myself. M um so you

mentioned the how the you know you're depreciating this asset that's 5 six years and this is the majority of the you know 75% of the TCO of a data center and Jensen is taking a 75% margin on that so what all the hyperscalers are trying to do is develop their own

accelerator so that they can reduce this overwhelming cost for um uh equipment to increase their margins. >> Yeah. And then and then like you know when you look at where they are right Google's way ahead of everyone else right they've been doing it for the

longest they're going to make something like 5 to 7 million chips right of their own TPUs you look at Amazon they're trying to make 3 to 5 million but when we look at what you know Microsoft is is ordering of their own chips it's it's it's way below that number um you've had

a program for just as long what's going on with your internal >> good question so so the couple of things one is the thing that is the biggest competitor for any new accelerator is kind of even the previous generation of Nvidia right I mean in a fleet what I'm

going to look at is the overall TCO so the bar I have even for our own and which by the way you know I was just looking at the data for Maya 200 which looks great um a except that one of the things that we learned even on the compute side right which is we had a lot

of Intel then we introduced AMD and then we introduced cobalt and so that's kind of how we scaled it and so we have good um sort of existence proof of at least in core compute on how to build your own silicon and then manage a fleet where all three are at play in some balance.

Uh because by the way even Google's buying Nvidia and so is uh Amazon. It makes sense because Nvidia is innovating and it's the general purpose thing. All models run on it. Uh and customer demand is there because if you build your own vertical thing, you better have your own

model uh which is you know either going to use it for training or inference and you have to generate your own demand for it or subsidize the demand for it. So therefore you want to uh make sure um you scale it appropriately. So the way we're going to go do it is um have a

closed loop between our own MAI models and our silicon because I feel like that's the that's what gives you the birthright to really do your own silicon right where you literally have uh designed the micro architecture with what you're doing and then you keep pace

with your own models. Um in our case the the good news here is OpenAI has a program uh which we have access to. Um and so therefore to think that Microsoft is not going to have something that's >> what level of access do you have to that >> all of it.

>> You just get the IP for all of that. So the only IP you don't have is a consumer hardware. >> That's it. >> Oh wow. Okay. >> Yeah. Interesting.

>> Yeah. And by the way we gave them a bunch of IP as well to bootstrap them. Right. So this is one of the reasons why they had a mass because we built all these supercomputers together uh or we built it for them and they uh benefited from it rightfully so and uh and now as

they innovate even at the system level we get access to all of it uh and uh we first wants to want to instantiate what they build uh for them uh but then we'll extend it and so to think that we don't have and so if anything the way I I think about to your question is

Microsoft wants wants to be a fantastic I'll call it speed of light execution partner for Nvidia because quite frankly that fleet uh is life itself. I'm not worried about I mean obviously Jensen is doing super well with his margins but the TCO has many dimensions to it and I

want to be great at that TCO. Uh on top of that, I want to be able to sort of really work with the OpenAI lineage uh and the MAI lineage and the system design knowing that we have the IP rights on both ends.

>> Uh speaking of rights, one thing you know, you had an interview a couple days ago uh where you said that we have rights to the the new agreement you made with OpenAI. have right the exclusivity to the stateless API calls that OpenAI makes and we were sort of confused about

if there's any state whatsoever. I mean you were just mentioning a second ago that all these complicated workloads that are coming up are going to require memory and databases and storage and so forth and is that now not stateless of chat GPT is storing stuff on

>> that's the reason why so the the thing the business the strategic decision we made and also accommodating for the flexibility open AI needed in order to be able to procure compute for essentially think of open AI having um a pass business and a SAS business SAS

business is chat GPT that pass business is their API. >> That API is Azure exclusive. >> The SAS business, they can run it anywhere >> and they can partner with anyone they want to to build SAS products.

>> So if they want to partner and the partner wants to use a a stateless API, then Azure is the place where they can get the stateless API. >> It seems like there's a way for them to make you you know build the product together and and it's a state.

>> No, even that they'll have to come to Azure. Okay. So if it is any partner and so so fundamentally you know so again this is done in the spirit of what is it that we valued as part of our partnership and we made sure while at the same time we were good partners to

open AAI given all the flexibility they need. So for example, Salesforce wants to integrate OpenAI. It's not through an API. They actually work together, train a model together, deploy it on let's say Amazon. Now is that is that allowed or uh or do they have to use

>> for any custom agreement like that? They will have to come run it. There are some few exceptions to US government and so on that we made, but other than that, they'll have to come to Azure. >> So as s explained, as AI agents get more capable, you're going to need more and

more observability into what they're doing. You're going to need to catch them when they're making mistakes. You're going to need highle summaries of what they're doing and you're going to need a picture of how everything that they're doing fits together. This is

exactly what Code Rabbit provides. You just make a normal pull request and Code Rabbit automatically reviews the PR. It generates a summary of changes so you can understand exactly what the PR's author was intending and it uses the context from your full code base to

provide line by line feedback on how things could be improved. This is helpful whether you're reviewing a PR from a co-orker or an agent. In either case, Code Rabbit will write up its thoughts and flag any issues so that your teammate or your agent can go fix

them. I've noticed that when I'm coding with agents, Code Rabbit catches a lot of mistakes that the models make by default. For example, the models have a bad habit of using old versions of libraries. So, in one session, I watch Code Rabbit cache a call to an old

model, figure out what the new version was, and then suggest that improvement. Go to code rabbit.a a/4 to learn more. >> Stepping back, a question I have is, you know, when we were walking back and forth with the factory, one of the

things we're talking about is, you know, Microsoft, you can think of it as a software business, but now it's really becoming an industrial business. Uh, there's all this capex, there's all this construction, and if you just look over the last two um two years, your sort of

capex is like tripled. And maybe you extrapolate that forward. It just actually just becomes this huge industrial uh explosion. >> Well, their hyperscalers are taking loans, right? Meta's Meta's done a $20 billion loan at Louisiana. They've take

they've done a corporate loan. It seems clear everyone's free cash flow is going to zero. Um which is which I'm sure Amy is like going to beat you up for for even if you even try to do that. But like uh what what what's happening? I mean I think I think the structural

change um is what you're referencing which I think is massive right which is I I describe it as we are now a capital intensive business and a knowledge intensive business and in fact we have to use our knowledge to increase the ROIC on the capital spend right because

that's kind you know look the hardware guys have done a great job uh of marketing the morals law which I think is unbelievable and it's great but if you even look I think some of the stats I even did in my earnings call which is for a given GPT family right uh the

improvement software improvements of really throughput in terms of tokens per dollar per watt that we're able to get uh you know quarter over quarter year over year is massive uh right so it's 5x 10x maybe 40x in some of these cases right just because uh how you can

optimize that's sort of knowledge intens intensity coming to bring out capital efficiency >> so that at at some level the that's what we have to master. What does it mean?

Like somebody people ask me what was the difference between uh you know a classic oldtime host and a hyperscaler it was software. So yes it is capital intensive but as long as you have systems knowhow software capability to optimize by workload by fleet that's why I think

when when we say fungeibility there's so much software in it. It's just not about the fleet, right? It's kind of the ability to evict a workload, you know, and then schedule another workload. Can I like manage the that algorithm of scheduling around u that is

the type of stuff that we have to be world class at? And so yes, so I think we'll still remain a software company. >> Uh but yes, this is a different business. Um and we're going to manage. Look, I think at the end of the day, uh the cash flow that Microsoft has allows

us to have um both these arms firing, you know, uh well, >> it seems like in the short term you have more sort of um credence on things taking a while, being more jagged, but in the maybe in the long term, you think like the people who say talk about AGI

and ASI are correct like Sam Sam will be right, but eventually. Um and I I have a broader question about what makes sense for a hyperscaler to do given that you have to invest massively in this thing which depreciates over 5 years. So so you if you have 2040 timelines to the

kind of thing that somebody like Sam anticipates in three years um you know what what is a reasonable thing for you to do in that world? there needs to be an allocation uh to I'll call it research compute >> that needs to be done like you did R&D

>> right so that's the best way to even account for it quite frankly we should think of it as just R&D expense and you should say hey what's the research computer and know how do you want to scale it >> um and let's even say it's an order of

magnitude scale um in some period pick your thing is it two years is it 16 months what have you Right. So that's sort of one piece which is kind of that's kind of table stakes. That's R&D expenses and the rest is all demand driven. Right. I mean ultimately you can

you have to build ahead of demand but you better have a demand uh uh plan uh that doesn't go completely offkilter. Do you buy so these labs are now projecting revenues of 100 billion in 2728 uh and they're projecting you know revenue keeps growing at this rate of

like 3x 2x a year >> a lot in the marketplace right there's all kinds of incentives right now and and rightfully so right I mean what what do you expect an independent lab that is sort of trying to raise money to do right they have to put some numbers out

there such that they can actually go raise money so that they can pay their bills for compute and what have you and it's And it's good thing I someone's going to take some risk and put it in there and they've shown traction. It's not like it's all risk without seeing

the fact that they've been performing whether it's open AAI, whether it's anthropic. So I feel great about what they've done. >> Uh and we have massive book of business with these Japs. So therefore uh that's all good.

>> But overall ultimately there's two simple things. One is you got to allocate for R&D. You brought up even talent. You got to like the talent for AI is at a premium. You got to spend there. You got to spend on compute. So in some sense researcher to

GPU ratios have to be high. Uh that is sort of what it takes to be a leading R&D company in this world. Uh and that's something that needs to scale. Um and you have to have a balance sheet that allows you to scale that long before it's conventional wisdom and so on. So

that's kind of one thing. But the other is all about sort of knowing how to forecast >> as we look across the world right America has dominated many tech stacks right um the US owns Windows right through Microsoft which is deployed even

in China right that's the main operating system um of course there's Linux which is open source but you know Windows is deployed everywhere in China on personal computers um you look at you look at word it's it's deployed everywhere you look at all these various technologies

it's deployed everywhere the thing that is quite unique and and and Microsoft and other companies have grown elsewhere, right? they've built they're building data centers in Europe and in India and in and in all these other you know in Southeast Asia and in Latam in

Africa right all of these different places you're building capacity but this seems quite different right you know to today the political aspect of technology of compute you know you know the US administration didn't care about the dotcom bubble right um it seems like the

US administration as well as every other administration around the world cares a lot about AI and the question is you know we we're in sort of a bipolar world at least with US and China but Europe and and India and all these other countries are are saying no actually

we're going to have sovereign AI as well. How does Microsoft navigate, you know, the difference of the 90s where it's like there's one country in the world that matters, right? It's America and we do our companies sell everywhere and therefore Microsoft benefits

massively to a world where it is bipolar where hey Microsoft can't just necessarily have the right to win all of Europe or India or you know Singapore. There's actually sovereign AI efforts. What what is your thought process here and how do you think about this? It's

it's I think a super you know critical um piece which is um I think that the key key priority for the US tech sector and the US government is to ensure that we not only do leading innovative work but we also collectively build trust around the world on our tech stack right

because I always say the United States is just an unbelievable place It's just unique in history, right? It's 4% of the world's population, 25% of the GDP, and 50% of the market cap. And I think you should think about those ratios and uh really and reflect on it. that 50%

happens because quite frankly the trust the world has in the United States whether it's its capital markets or whether it's its technology and and its stewardship of what matters at any given time in terms of leading uh sector. So if that is broken uh then that's not a

good day for the United States. And so if we start with that which I think the you know President Trump gets, the White House, David Sachs, everyone uh really I think gets it. Uh and so therefore I applaud anything that the United States government and the tech sector jointly

does to quite frankly for example put our own capital at risk collectively as an industry in every part of the world. Right? So I would like in fact the USG to take credit for foreign direct investment by American companies all over the world right it's kind of like

uh least talked about but the best marketing that the United States should be doing is it's not just about all the foreign direct investment coming into the United States but the most leading sector which is these AI factories are all being created all over the world by

whom by America and American companies and so you start there and Then you even build other agreements around it which are around their continuity, their legitimate sovereignty concerns around whether it's data residency, whether it's even what happens um uh for them to

have real agency and guarantees uh on privacy and so on and so that in fact our European commitments I think are worth reading. Right? So we made a series of commitments to Europe on how we will really govern our hypers scale investment there uh such that really

European uh union and the European countries have sovereignty. We're also building sovereign clouds in in France and in Germany. We have something called sovereign services on Azure which literally give people key management services along with confidential

computing including confidential computing in GPUs which we've done great innovative work with Nvidia. Um and so I think I feel very very good about being able to build both technically and through policy this trust in the American tech stack. M and how do you

see the shaking out as you know you do have this uh network effect with learning and things on the model level maybe you have equivalent things at the hyperscaler level as well and do you expect that the countries will say look it's clearly one model or a couple

models are the best and so we're going to use them but we're going to have some laws around well the weights have to be hosted in our country or do you expect that there will be uh this push to have it has to be a model trained in our country maybe an analogy here is like

people would you know the semiconductor is very important to the economy and people would like to have their sort of sovereign semiconductors but like TSMC is just better and so semiconductors are so important to the economy that you will just go to Taiwan and buy the

semiconductors you have to will it be like that with AI or is there >> um ultimately I think what matters is the use of AI in their economy to create economic value right I mean that's the uh the diffusion theory which is ultimately it's not the leading sector

but it's the ability to use the leading technology to create your own comparative advantage right so that I think will fundamentally be the core driver >> but that said they will want continuity of that right so in some sense that's

one of the reasons why I believe there's always going to be a check a little bit to sort of some of your points on hey can this one model have all the runaway deployment that's why open source is always going to be there they will be by definition uh multiple models that'll be

one way like it's kind of the you know that's one way for people to sort of demand continuity and not have concentration risk is another way to say it is right um and so you say hey I want multiple models and then I want an open source so I feel uh as long as that's

there every country will feel like okay I don't have to worry about deploying the best model and broadly diffusing because I can always take uh what is my data and my liquidity and move it uh to another model whether it's open source or from another country or what have

you. So concentration risk >> um and sovereignty right which is really agency those are the two things I think that'll drive the market structure. The the thing about this is that this doesn't exist for semiconductors, right?

You know, all refrigerators, cars have chips made in Taiwan. >> It didn't exist until now. Until now, everybody is now like like >> even even then, right, America, you know, if Taiwan is cut off, there is there are no more cars, there are no

more refrigerators. TSMC Arizona is not replacing any real fraction of the production. Like there it is. It there the sovereignty is a bit of like a a scam, if you will, right? I mean, it's it's worthwhile having it. It's important to have it, but it's not a

real it's not real sovereignty, right?

And we're a global economy. We don't we >> I think it's kind of like Dylan saying, hey, at this point, we've not learned anything about sort of uh what resilience means and what one needs to do, right? So, it's kind of any nation state, including the United

States at this point, will do what it takes to be more self-sufficient on some of these critical supply chains. So I as a multinational company have to think about that as a first class requirement right if I don't then I'm not respecting what is in the sort of

policy interests of that country long term right and I'm not saying they won't make practical decisions in the short term right absolutely I mean the globalization can't just be rewound right I mean all these capital investments cannot be made uh in in a

way at the pace at which at the same time you have to kind of like if I think about it, right? If somebody showed up in Washington and said, "Hey, you know, you know what? We're not going to build any semiconductor plans, they're going to be kicked out of the United States."

Um, and and the same thing is going to be the true in every other country, too. Uh, and so therefore, I think we have to as companies respect what the lessons learned are. Um, you know, whether it's, you know, you could say the pandemic woke us up or whatever. But

nevertheless, people are saying, "Look, globalization was fantastic. uh it helped the supply chains be globalized and be super efficient but there's such a thing called resilience and we are happy you know we want resilience and so therefore that feature

will get built at what pace I think you point you're making it can't be like you can't snap your fingers and say all the TSMC plants now are all in Arizona and with all of the capability they're not going to be but is there a plan there will be a plan and should we respect

that absolutely and so I I feel that's the world I want to meet the world where it is and what it wants to do going forward as opposed to say hey we have a point of view that doesn't respect your view. M so ju just to make sure I understand the idea here is each country

will want some kind of data residency privacy etc. And Microsoft is especially privileged here because you have relationships with these countries who have expertise in setting up these kinds of sovereign data centers and therefore Microsoft is uniquely fit for a world

with um more sovereignty requirements. >> Yeah. I mean I I I don't want to sort of describe it as somehow we're uniquely privileged. Uh I would just say I think of that as a business requirement that we have been doing all the hard work all these decades and we plan to and so my

answer to Dylan's previous question was I take these you know whether it's in the United States quite frankly uh when you know when the white house and the USG says hey we want you to allocate more of your I don't know wafer starts to uh uh fabs in the US we take that

seriously. or whether it is data center and the EU boundary, we take that seriously. So to me, >> um respecting what I think are legitimate reasons why countries care about sovereignty and building for it as

a software and a physical plant is what I I would say we are going to do. And as we go to like the bipolar world, right, US, China. Yeah. Um there is there is a lot around, >> you know, American tech does not, you know, it's not just you versus Amazon,

um or you versus, you know, anthropic or you versus Google. Yeah. There is a whole host of competit competition. How does how does America rebuild the trust?

What do you do to rebuild the trust to say actually no, American companies will be the main provider for you? Um and how do you think about competition with up and cominging Chinese companies whether it be you know bite dance and Alibaba or Deepseek and Moonshot

>> and so just add to the question one concern is look we're talking about how AI is becoming this sort of industrial capex race uh where you're just rapidly having to build quickly across all those supply chain when you hear that at least till now you just think about China

right this is like their comparative advantage and especially if we're not going to moonshot to ASI next year but we it's going to be this decades of buildouts and infrastructure and so forth, how do you deal with Chinese competition? Are they privileged in that

world?

>> Yeah. So, it's a great qu. I mean, in fact, you just made the point of why I think trust in American tech is probably the most important feature. It's not even the model capability. Maybe it is like can I trust you the company? Can I trust you? Your country

and its institutions to be a long-term supplier may be the thing that wins the world. >> That's a good note to end on. >> Yeah, >> Satia, thank you for doing this. >> Thank you so much. Thank you. It's such

a pleasure. Such a pleasure. >> It's awesome. It's like, man, you two guys are like quite the team. >> Hey everybody, I hope you enjoyed that episode. If you did, the most helpful thing you can do is just share it with other people who you think might enjoy

it. It's also helpful if you leave a rating or a comment on whatever platform you're listening on. If you're interested in sponsoring the podcast, you can reach out at dwarcash.com/advertise. Otherwise, I'll see you on the next one.
