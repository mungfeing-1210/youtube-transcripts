---
layout: post
title: "Intelligence Isn’t Enough: Why Energy & Compute Decide the AGI Race – Eiso Kant"
channel: "The MAD Podcast with Matt Turck"
date: 2024-11-06
---

# Intelligence Isn’t Enough: Why Energy & Compute Decide the AGI Race – Eiso Kant

**频道**: The MAD Podcast with Matt Turck
**发布日期**: 2025-11-06
**视频链接**: https://www.youtube.com/watch?v=6lmuo8RD-Rs

---

## Key Takeaways

## 一句话总结
本视频深度剖析了 AGI 竞赛的关键驱动力不再是单纯的“智能”，而是**能源、算力和基础设施的规模化与成本控制**，并揭示了 Poolside 在此背景下构建“AI 工厂”的战略布局和对未来 AI 发展路径的独特见解。

## 内容概览
本视频采访了 Poolside 的联合创始人 Eiso Kant，深入探讨了 AGI（Artificial General Intelligence）竞赛的本质。Kant 认为，随着智能本身逐渐商品化，拥有强大的**物理基础设施**（数据中心、能源、算力）成为领先的关键。Poolside 的“Project Horizon”计划旨在建设美国最大的数据中心之一，以确保其在能源、算力和智能三层技术栈中的主导地位。此外，视频还介绍了 Poolside 在**强化学习（Reinforcement Learning, RL）**领域的创新方法，特别是“Reinforcement Learning to Learn (RL to L)”的理念，以及对未来 AI 训练范式的预测。

## 核心要点

### 1. 智能的商品化与基础设施的战略必要性
**背景/问题**: 许多 AI 公司专注于模型能力的提升，但忽视了支撑其运行的物理基础。Eiso Kant 认为，未来的智能将成为一种商品，其价值将更多体现在**规模化**和**成本效益**上。
**核心观点**: 如果一家基础模型公司不构建自己的物理基础设施（数据中心、能源），它就是在“cosplaying”（扮演）自己的业务。Project Horizon 是 Poolside 构建超大规模数据中心的战略体现，旨在拥有能源、算力，从而控制智能的交付成本和规模。
**实践启示**: 任何致力于 AGI 竞争的公司，都需要认真考虑并投资于物理基础设施建设，以获得规模经济和成本优势，避免被基础设施瓶颈所限制。

### 2. Project Horizon：万兆瓦级 AI 工厂的构建
**背景/问题**: 现有的数据中心无法满足 AGI 训练和推理所需的指数级算力增长需求，且传统的数据中心租赁模式存在严重的**提前期（lead time）**和**资本投入**问题。
**核心观点**: Project Horizon 是 Poolside 在美国建设的一个超大规模数据中心园区，其规模以**百兆瓦（megawatts）**计，并计划扩展至**吉瓦（gigawatts）**级。这不仅仅是为了获取算力，更是为了实现**规模化**和**成本控制**，是实现 Poolside 使命的**必要条件**。
**实践启示**: 构建类似 AI 工厂的物理基础设施，是实现大规模 AI 部署和商业化的关键一步，需要巨额资本投入和长期的战略规划。

### 3. 能源、算力和智能：AGI 竞赛的“三层栈”
**背景/问题**: AGI 的发展需要三个核心要素协同作用：能源（提供动力）、算力（执行计算）和构建在算力之上的智能模型。
**核心观点**: Poolside 认为，随着智能本身逐渐变得差异化不大，**能源和算力**的**可扩展性**和**成本**将成为决胜的关键。因此，拥有对这三层栈的控制权至关重要。
**实践启示**: 关注 AI 发展的公司需要将目光从单纯的算法和模型，扩展到能源供应、芯片制造、数据中心建设等更广泛的物理基础设施领域。

### 4. 基础设施建设的经济学：成本与利润空间
**背景/问题**: 建设和运营超大规模数据中心涉及巨额的资本支出（Capex）和运营支出（Opex），理解其经济模型是战略规划的关键。
**核心观点**: 一个 250 兆瓦的数据中心项目，其总成本（包括土地、能源基础设施、建筑、芯片和网络设备）可能高达 80 亿美元。虽然能源成本是 Opex 的重要组成部分，但与总算力成本相比，其占比正在下降。通过**端到端拥有**整个基础设施，可以**消除中间环节的利润**，从而在 token 成本上实现 20-40% 的降低。
**实践启示**: 垂直整合和自建基础设施可以显著降低 AI 服务的成本，从而在市场竞争中获得价格优势，特别是在智能逐渐商品化的未来。

### 5. 混合模块化数据中心：平衡速度与效率
**背景/问题**: 传统数据中心建设方式（stick-built 或全模块化）各有局限，难以快速、高效地满足 AI 算力需求。
**核心观点**: Poolside 采用了**增量式交付的混合模块化数据中心**（incrementally delivered hybrid modular data centers）方法。即，主体建筑采用传统方式建造，但内部的计算、电力、冷却等核心系统采用模块化设计，可实现 2.5 兆瓦的增量部署，从而兼顾了建设速度、灵活性和成本效益。
**实践启示**: 创新的基础设施设计和建设模式，能够有效应对 AI 算力需求的快速变化，降低项目风险，并加速算力的上线。

### 6. 强化学习（RL）的演进：从代码执行到“RL to L”
**背景/问题**: 早期 AI 发展过度依赖“next token prediction”的预训练范式，而强化学习（RL）被认为是更重要的能力扩展途径，但传统的 RL 方法也面临挑战。
**核心观点**: Poolside 最初专注于**RL from code execution feedback**，通过模拟环境（目前已扩展至百万级）进行模型训练。现在，他们提出了“**Reinforcement Learning to Learn (RL to L)**”的新范式，旨在通过**反向工程（reverse engineering）**互联网数据，找到一种通用的 RL 目标，使其能够像语言模型一样，从数据中学习思考和行动的内在机制，而非依赖外部的奖励模型或人类标注。
**实践启示**: 探索通用的、自监督的 RL 学习目标，是实现 AGI 的关键突破点，有望比依赖人工标注和环境模拟更具可扩展性和效率。

### 7. AI 训练的四个阶段：通往 AGI 的路径
**背景/问题**: 如何构建一个能够持续学习、适应并最终达到 AGI 水平的 AI 系统？
**核心观点**: Eiso Kant 提出了 AI 训练的四个阶段：
1.  **预训练（Pre-training）**: 基于网络数据的 next token prediction。
2.  **RL to L**: 通过反向工程数据，学习内在的思考和行动机制。
3.  **RL from verified environments**: 在模拟环境中进行技能打磨（如代码执行反馈）。
4.  **Continuous learning from real-world experiences**: 从真实世界的代理（agent）反馈中持续学习。
**实践启示**: AGI 的实现可能需要一个多阶段、多范式的训练过程，结合不同方法的优势，才能构建出真正通用和强大的智能。

### 8. “热炉问题”与持续学习
**背景/问题**: 当前的深度学习模型（基于梯度下降）需要大量数据才能有效学习，难以实现像人类一样从单次经验中快速学习（“热炉效应”）。
**核心观点**: 解决“热炉问题”（hot stove problem），即让模型能够从**单次经验**中有效学习，是实现 AGI 的一个关键挑战，也是提升模型**计算效率**和**价值**的重要途径。虽然不确定是否是 AGI 的必要条件，但它无疑会加速这一进程。
**实践启示**: 未来 AI 研究需要关注更高效的学习算法，使其能够模拟人类的快速学习能力，从而大幅降低训练成本并提高泛化能力。

### 9. 智能的维度与代理（Agent）的定义
**背景/问题**: AGI 的概念往往被过于简化，而智能本身是多维度的；同时，对“Agent”的定义和实现时间表存在争议。
**核心观点**: 智能是多维度的，例如创造力、逻辑推理、工程设计等。AI 可能在某些维度上达到人类水平，但未必在所有维度上都如此，并且不一定需要完全模拟人类的全部能力才能实现经济价值。**模型（Model）和代理（Agent）**在本质上是相似的，都是在环境中运行并具备工具访问能力的计算实体。
**实践启示**: 在讨论 AGI 和 Agent 时，需要更细致地考虑智能的**多维度性**和 Agent 的**具体能力范畴**，避免过于笼统的预测。

### 10. 基础设施建设中的人才与声誉
**背景/问题**: 构建大规模物理基础设施需要跨领域的专业人才，并且在这样一个相对封闭的行业中，声誉至关重要。
**核心观点**: Poolside 通过**招聘顶尖人才**（例如 VP of Data Centers）并赋权来应对挑战。在基础设施领域，**人际关系和声誉**是关键驱动力，能够快速识别出行业内的优秀参与者，并与他们建立合作关系。
**实践启示**: 在进入新领域时，识别、吸引和赋权顶尖专家是成功的关键。同时，建立和维护良好的行业声誉是长期合作和发展的基石。

## 关键概念与资源
**核心概念**:
*   Artificial General Intelligence (AGI)
*   Foundation Model
*   Reinforcement Learning (RL)
*   Pre-training
*   Next Token Prediction
*   Reinforcement Learning from Code Execution Feedback
*   Reinforcement Learning to Learn (RL to L)
*   Agentic RL
*   The Hot Stove Problem
*   Continuous Learning
*   Data Center Infrastructure
*   Compute Scale (Megawatts, Gigawatts)
*   Capex (Capital Expenditure)
*   Opex (Operational Expenditure)
*   Lead Time
*   Modular Data Centers
*   Hybrid Modular Data Centers

**工具/技术**:
*   Foundation Models
*   Nvidia GPUs (GB200, GB300)
*   Direct Liquid to Chip Cooling
*   Natural Gas Turbines
*   Battery Storage Systems
*   Selective Catalytic Reduction (SCR) for emissions control

**推荐资源**:
*   Poolside (公司名称)
*   OpenAI, Anthropic, AWS, GCP, Azure (提及的公司和平台)
*   Coreweave (提及的合作伙伴)
*   The Mitchells (德州土地合作伙伴)
*   Andrej Karpathy's blog post "The Unreasonable Effectiveness of Recurrent Neural Networks" (Eiso Kant 提及的早期启蒙文章)

## 目标受众
**最适合**:
*   AI 领域的创业者、投资者和研究人员，特别是那些关注 AGI 发展路径、基础设施建设和前沿 AI 技术的人。
*   对 AI 产业的未来格局、算力瓶颈、能源供应以及大规模 AI 部署的经济学感兴趣的商业领袖和技术决策者。
*   对强化学习、模型训练新范式有深入研究兴趣的技术专家。

**价值场景**:
*   在评估 AI 公司投资价值时，理解其对基础设施的布局和战略。
*   在制定 AI 产品路线图时，考虑算力、能源和成本的制约因素。
*   在研究 AGI 的可行路径时，了解当前 AI 领域的前沿思想和技术挑战。
*   在思考 AI 行业未来的颠覆性机会时，洞察基础设施建设可能带来的新价值。

## 延伸思考
1.  **AGI 的“商品化”与“价值创造”的平衡**: 如果智能本身将成为商品，那么基础模型公司未来的核心竞争力将更多地体现在如何通过**增值服务**（value-added services）来创造利润？这是否意味着 AI 公司将更倾向于成为平台型企业，提供多样化的解决方案，而非仅仅提供原始算力？
2.  **能源与算力的“绿色化”与“规模化”的博弈**: Poolside 在能源方面采取了天然气与可再生能源结合的策略，并强调了排放控制。在未来 AGI 算力需求爆炸式增长的背景下，如何在全球范围内实现能源供应的**绿色化**和**规模化**的平衡，将是 AI 产业面临的长期挑战。是否会有新的能源技术或存储方案出现，以满足 AGI 对能源的无止境渴求？
3.  **“RL to L”范式对当前 AI 生态系统的颠覆性影响**: 如果 Poolside 的“RL to L”范式能够成功实现，其对当前依赖大量标注数据和人工设计的 RL 方法的生态系统将产生怎样的影响？这将如何加速通用 AI 的发展，并可能对现有 AI 公司的技术栈和商业模式带来哪些重塑？

---

## 中文文稿

好的，作为一名资深内容编辑和翻译专家，我将为您将这段英文视频字幕转录为高质量、易读的中文文稿。

---

### AGI 竞赛的真正驱动力

**主持人**: 欢迎来到 Mad Podcast，我是 First Mark 的 Matt Turk。今天我的嘉宾是 Poolside 的联合创始人 Eiso Kant。Poolside 是一家专注于软件工程的基础模型实验室公司，据报道，该公司目前正在进行一轮 20 亿美元的融资，估值达到 140 亿美元，其中包括英伟达（Nvidia）据称 10 亿美元的投资。我们将深入探讨 Poolside 雄心勃勃的 Project Horizon，这是一个多吉瓦（multi-gigawatt）级别的人工智能工厂数据中心，以及为什么 AI 实验室必须拥有能源、算力和智能。Eiso 还将揭示“强化学习到学习”（Reinforcement Learning to Learn, RL to L），一条超越预训练和经典强化学习的新路径。请享受这次与 Eiso 的深度对话。

**Eiso Kant**: 很高兴来到这里，Matt。

**主持人**: 我应该说，欢迎回来。我们差不多在两年前就聊过，那时你们公司才刚刚起步。

**Eiso Kant**: 我当时还在听我们之前的播客，我都没意识到那是我们作为 Poolside 做的第一个播客。是的，两年时光飞逝。快进到今天，2025 年底，前沿 AGI 实验室之间的竞争变得更加疯狂和泡沫化。那么，你们公司现在处于什么位置？在其他实验室的超高竞争环境中，Poolside 存在的理由是什么？

### Poolside 的起源与理念

**Eiso Kant**: 这回到了我们创立公司的初衷。回想 2023 年初，当我们创立 Poolside 时，我们对研究有自己的看法。如果你还记得，在那年早些时候，GPT-4 刚刚发布，当时全球的叙事是，要达到 AGI，我们只需要扩展语言模型，进行更多的下一个词预测，增加参数数量和数据量。我们同意规模的重要性，至今仍然如此。但我们的观点是，强化学习将成为模型能力最重要的扩展途径。

当时这是一个极其有争议的观点，在我们做播客的时候是这样，可能在接下来的 12 个月里也是如此。今天情况就大不一样了。我认为这已经成为共识，我认为世界已经认识到，我们现在有能力继续扩展模型，使其朝着越来越强大的方向发展，并真正缩小人类智能与人工智能之间的差距。这就是 Poolside 最初的使命，至今仍然是 Poolside 的核心使命。

### 规模化与 RL 的崛起

**Eiso Kant**: 但在此过程中，我认为我们预测的事情发生了。我们将继续扩展，需要更多的算力。但与此同时，预测下一个词的网络预训练范式正变得呈 S 型曲线，其收益正在放缓。因此，在过去的两年半里，我们在预训练方面取得了进展，尤其是在我们称之为基础模型构建和语言建模的“入场券”方面。同时，我们在强化学习方面建立了一些非常显著的优势。

现在，这两方面已经结合在一起了。我们这些前沿公司都在进行大规模的模型训练，规模与以往相同。我们在研究中建立了一系列优势，在工程方面也建立了一些优势，这些可能是最重要且可持续的优势。但现在我们正处于一个需要将模型扩展到前沿规模的时刻。因此，我们最近发布了一些关于算力规模即将上线的重要公告。

### 软件开发作为 AGI 的代理

**Eiso Kant**: 我们正在竞争，我们身处这场竞赛之中。我记得听了我们之前的播客，我说，我们正在追赶 OpenAI 和 Anthropic。这仍然是事实。但我们的起点有些不同，对吧？我们认为，要实现高度智能、能够推理、规划和理解世界，最好是戴上“眼罩”，专注于一个代理。对我们来说，这个代理就是软件开发。

随着我们在执行更长周期的软件开发任务方面变得越来越强大，我们也变得越来越擅长处理几乎所有类型的知识工作任务。从模型的角度来看，我认为我们都在朝着一个相似的方向发展。我甚至可以说，智能将变成一种商品。

### Poolside 的未来定位

**Eiso Kant**: 在那个世界里，我们希望成为什么？我们希望被企业信任，被商家信任，为知识工作者提供动力。这始于编码代理，现在已经超越了这一点。我认为在这个领域，很多东西将会转型。我们正处于一个“前电时代”和“后电时代”的时刻。我不认为这是一个“赢家通吃”的市场，但确实似乎只有少数几家公司能够实现这一目标。

### Project Horizon 的重要性

**主持人**: 作为迈向 AGI 的竞赛的一部分，你们在过去几周内有一些非常重大的消息。首先，据传有一笔高达 20 亿美元的大规模融资，英伟达可能会投资高达 10 亿美元，估值在 120 亿（未融资前）到 140 亿（融资后）美元之间。这是一轮非常大的融资，似乎与你们正式宣布的另一个重大消息——Project Horizon——紧密相关。Project Horizon 是什么？

---

好的，作为一名资深内容编辑和翻译专家，我将为您将这段英文视频字幕转录为高质量、易读的中文文稿。

---

### 项目背景与核心理念

**嘉宾**：是的。所以，Project Horizon 是我们在美国建设的最大的数据中心综合体之一。这回到了我之前提到的一个观点。我们曾讨论过“智能正在成为一种商品”。我们公司自成立两年多以来，一直秉持着一个核心观点：技术栈中有三个层面至关重要，那就是能源（energy）、算力（compute）以及建立在它们之上的智能（intelligence）。

### 智能商品化与规模成本

**嘉宾**：在这个世界里，如果你认为智能在不同公司之间的区分度会越来越低，最终成为一种商品，就像石油或面包店里的面包一样普遍，那么只有两件事是关键：你的规模化能力以及你向最终用户提供服务的成本。

### 算力规模的演变与挑战

**嘉宾**：坦率地说，规模化能力是 Project Horizon 的主要驱动力，成本也是。重要的是要思考，与 12 个月前相比，你今天能做什么。就我们行业谈论的算力规模而言，两年前，你可以联系一家数据中心托管公司，说“我需要这么多算力，六个月后交货”，那时会有可供你部署的物理空间，或者有现成的可用容量。而今天，我们谈论的算力规模是以兆瓦（megawatts）为单位，很快将达到吉瓦（gigawatts）。但你已经找不到可以随便打电话预订的地方了。

### 定制化数据中心与漫长的交付周期

**嘉宾**：没错，这些都是定制化（built to suit）的数据中心，规模之大以至于在没有确定租户之前，没有人会去建造。当你接近前沿（frontier）领域，你的模型变得越来越强大，你想将这种智能服务于全世界，并且想随着时间的推移扩展你的训练规模时，从决定到实现这一目标的时间周期，不再是联系一家超大规模云服务商（hyperscaler）或另一位合作伙伴，几个月就能搞定。现在，我们谈论的是 12 个月、14 个月、18 个月，并且伴随着巨额的资本投入。

### 基础设施的重要性：非建不可的现实

**嘉宾**：我的联合创始人经常这样说（可能不是为了上播客），如果一个基础模型公司（foundation model company）不自己建设物理基础设施，那它就是在“cosplay”自己的业务。这并非我们觉得建基础设施很酷，而是为了实现我们的使命，这是必需的。

### 独立实验室的自主选择

**嘉宾**：所以，你会觉得被排除在外，对吧？显然，OpenAI 是一个重要的目标，他们刚刚宣布了计划建造的第四个数据中心。Anthropic 与 AWS 有着特殊的合作关系，我猜 AWS 正在为他们建造。这大概就是你刚才说的，需要有租户（tenant）在场。AWS 愿意这样做，是因为他们有 Anthropic 这个租户。所以，作为一个独立实验室（indie lab），用一个不太恰当的说法，你必须掌握自己的命运，而这涉及到建设自己的数据中心。

### 两种发展路径：深度合作或自主建设

**嘉宾**：是的，我认为作为一个基础模型公司，你可以选择两条路。你可以选择与超大规模云服务商深度合作，让他们成为你的股东，一路走下去。我认为这是一条路。但同时，世界正朝着一个方向发展，我认为现在没有人怀疑我们正朝着达到人类水平的能力和智能迈进。在这种情况下，价值 29 万亿美元的知识工作将重塑自身，科学进步将超越我们前所未见的水平。而这一切都依赖于“智能+算力”。

### 物理基础设施的真实价值

**嘉宾**：所以，一个基础模型公司在很大程度上是在从事物理基础设施业务，这比大多数人意识到的要多。因为规模化算力并将其交付给终端用户，并且以成本效益的方式做到这一点，这至关重要。你的 token 成本（cost of your tokens）将越来越重要。随着我们的智能越来越接近，规模化能力以及将其交付给终端用户，其中每 token 的几美分（cents per token）将决定用户是否购买，这一点至关重要。

### Project Horizon 的启动与规模

**嘉宾**：因此，我们早在几年前就开始问自己，要实现这一点需要什么。随着时间的推移，我们学到了更多，观察到了更多，然后我们开始采取行动，而 Project Horizon 就是我们采取行动的具体体现。Project Horizon 简而言之，就是我们今天宣布的 2 吉瓦（gigawatt）园区。实际上，我们可以在现有地块上实现远超 2 吉瓦的规模。这是与德克萨斯州一个了不起的家族——Mitchell 家族——的合作。Mitchell 家族拥有美国最大的地块之一，占地 50 万英亩。

### 能源与基础设施的协同优势

**嘉宾**：这总是让我震惊，因为作为参考，洛杉矶（LA）才 30 万英亩。在那片土地上，我们正在建设可再生能源，有电网连接，有水源，同时还有一条 20 英寸的主天然气管道。这为我们提供了快速、大规模地为数据中心提供能源的可能性。

### 首期建设与未来规划

**嘉宾**：所以，你们看到的是，在今年年底之前，我们将开始建设一个 250 兆瓦（megawatt）的数据中心。它将由天然气提供动力，并有电网作为备用，能够容纳海量的算力。但这将是我们正在建设的众多项目中的第一期。

### 基础设施对经济性的影响

**主持人**：很好。我们来梳理一下您刚才提到的一些观点。您提到了拥有自己的物理基础设施对前沿 AI 实验室（Frontier AI labs）的经济性影响。为了帮助我们理解这一点，在 AI 领域，一个关键问题是毛利率（gross margin）。您认为拥有自己的物理基础设施会在整体结构上将我们引向何方？

**嘉宾**：我还不确定我们行业的毛利率会像 SaaS 公司那样达到 80% 以上。我认为当我们谈论一种商品时……

---

好的，这是根据您提供的英文视频字幕转录、整理和翻译的高质量中文文稿：

### 基础模型与云服务类比

我们构建的智能，在其之上添加增值服务，作为基础模型公司，一方面，你们出售代币，也就是原油、原材料。另一方面，你们构建增值服务，即面向终端用户和客户的产品，为他们的企业或消费者解锁价值。在这种模式下，我认为它会更像一家云公司。

### 规模与利润率

我认为其规模将是云公司的级别，并且后面会多一个零。从利润率来看，可能非常相似，因为如果你想想 AWS、GCP 或 Azure 是什么，它们本质上是将硬件虚拟化，并在其上提供服务。我不认为基础模型与此有太大不同。因此，我认为从利润率来看，它可能更接近云公司看到的 40% 的水平，而不是软件行业 80% 的利润率。

### 智能成本构成

现在，当你开始考虑智能成本栈中的构成时，你实际上拥有土地、能源、数据中心、数据中心内的芯片。举个例子，如果我们用大致在正确范围内的示意性数字来分解，为 250 兆瓦供电大约需要 5 亿美元的物理基础设施。

### 数据中心与计算成本

就燃气轮机而言，那是燃气轮机，但有不同的实现路径。建造一个数据中心，比如那个能容纳计算设备运行的机房，成本大概在 20 亿到 25 亿美元之间。而现在要安装的计算设备成本大约是 55 亿美元。所以总的来说，当我们把这些加起来，包括芯片、网络以及数据中心装修所需的一切，250 兆瓦的总成本大约是 80 亿美元。因此，当你在行业内听到“一个吉瓦（gigawatt）是 400 亿到 500 亿美元”时，人们指的就是这个，这是成本的细分。

### 资产寿命与维护

芯片的寿命是有限的。在我们这个行业，关于它们能用多久有很多争论，这取决于供需关系。数据中心传统上寿命大约是 15 年，但随着新一代芯片的出现，需要进行翻新。而电力基础设施可以持续很长时间，但需要维护。

### 运营成本与能源占比

在这个 80 亿美元、250 兆瓦的项目中，每年的运营支出（Opex）大约在 3 亿到 3.5 亿美元之间。这可以大致分成两部分：融资成本和能源及运营成本，其中能源成本是最大的。

### 能源成本的相对地位

这里有一个有趣的见解：能源部分今天并不是成本栈中最大的部分。你每年花费大约 1.6 亿美元在能源上。当然，我使用的是西德克萨斯州的能源价格，所以在其他地方能源成本可能翻倍，但与计算的总拥有成本（TCO）相比，它仍然相对较小。不过，这些数字仍然非常庞大。

### 利润压缩与成本优势

我之所以提到这一点，是因为随着时间的推移，资本主义的规律是利润会普遍压缩。能源的利润已经压缩了，我们一直在努力让能源尽可能便宜。数据中心在过去 20-30 年里已经越来越商品化。而今天的 GPU 计算仍然是高利润业务，但随着时间的推移，它也会面临类似的利润压缩。

### 整合优势与成本降低

随着芯片成本的压缩，成本栈中其他部分的成本也变得更加重要。当你端到端地拥有并构建所有这些设施时，之前存在于中间环节的利润就会消失。这包括土地成本、能源成本、数据中心建筑成本，以及成本栈中的所有层级。

### 物理基础设施的成本主导

基础模型本身在构建时有资本支出，那是你的研发成本，你会将其分摊到一段时间内。但实际上，物理基础设施才是成本的大头。我一开始就讲得太详细了，但我觉得让大家了解成本的构成很重要。

### 价格竞争与市场转变

当你剔除了所有这些利润后，你就能看到，你的代币价格可以比别人便宜 20%、30%、40%。这很重要，因为我们正在从以人力劳动为基础的经济产出，转向以智能计算与人力劳动相结合的模式。而这在世界范围内才刚刚开始。

### 智能计算的未来规模

我们将进入一个智能计算成为一种商品的世界，其规模将远超我们所见的云计算。坦白说，云计算往往是企业最大的开销。控制这些成本会产生巨大的影响。

### 规模化与瓶颈

然而，成本只是答案的一部分。你还必须能够规模化。你必须能够说，如果下个季度我需要更多的计算能力，我能否迅速部署？问题在于，并非芯片短缺，而是能源和数据中心是瓶颈。

### 能源与数据中心的挑战

正如 Sacha 在一次播客中提到的：“我的瓶颈是能源和数据中心，而不是芯片。”这是事实。Nvidia 在向市场供应芯片方面做得非常出色，TSMC 和其他环节也是如此，它们能够大规模扩展。但物理空间和电力供应是关键的挑战。

好的，这是根据您提供的英文视频字幕转录并整理的高质量中文文稿：

### 数据中心融资模式

在建设数据中心方面，科技界的我们都开始意识到现实世界的挑战。您提到了能源成本和融资成本，这涉及到项目融资的设置。

没错。传统的房地产租赁业务，数据中心业务也类似。您需要投入一部分股本，并有一个贷款与成本的比例。这个比例很大程度上取决于数据中心承租人的情况。传统上，您会签订一份为期 15 年的租赁合同，然后进行传统的项目融资，这种方式非常有效，并且已经沿用了很长时间。这种融资方式利率相对较低，对科技界来说也并不新鲜。

### 核心租户与营收模式

其中一个想法是，Poolside 将成为核心租户，但您也会将设施出租给其他人。那么，您的收入来源是什么？

我们还没有达到能够签订长达 15 年、涉及数十亿美元巨额建设项目的租赁合同的阶段。不过，我们正在迅速扩张，并看到了能够大规模扩展计算能力的途径。因此，我们与 Coreweave 建立了非常棒的合作伙伴关系，这已经对外公布了。坦白说，Coreweave 在运营 Nvidia 的计算能力方面是首屈一指的。他们率先大规模部署了 GB200，现在又推出了 GB300，并且一直是我们在这一领域非常出色的合作伙伴。因此，我们与他们找到了一个非常棒的混合解决方案。他们是我们数据中心的初始租户。我们可以共同扩展数据中心内的计算能力，而我们选择不使用或无法使用的任何容量，他们可以提供给其他客户。

### 灵活的合作模式

这一点非常重要，因为我们不是那种能够提前两三年做出数十亿美元巨额投资的超大规模企业。所以，我们需要找到一种方式，建立一种双赢的合作伙伴关系，让我们能够扩展计算能力，而无需提前数年做出决策。而这种合作模式非常棒，因为它为我们的场地带来了两全其美的优势。

您解释的这一切在雄心勃勃方面令人着迷，同样令人着迷的是，一家年轻的软件 AI 公司最终如何成为一家主要的物理基础设施公司。所以，核心的合作伙伴关系有帮助，但您肯定需要招聘各种各样的人才，这需要完全不同的技能。您是如何考虑这方面的，以及财务方面呢？

### 招聘与专家团队

是的，我可以说我职业生涯的大部分时间都在遵循一种算法。至少我记得，早在九年前，作为创始人，你必须尽一切可能避免“邓宁-克鲁格效应”（Dunning-Kruger effect）。因为当你进入一个像物理基础设施这样全新的领域时，你首先要做的就是学习。

你开始阅读，开始会见专家，不断学习。当你对一个话题了解得越多，你就会进入一个阶段，开始觉得自己真的懂了。但你其实并不懂，因为你还没有真正接触过现实世界，或者你以前从未做过。我发现，很久以前我得到的一个最好的建议是，当你发现自己处于这种情况时，就是时候开始寻找专家来加入你了。但你如何找到专家呢？因为你自己不是专家。这时，你就会开始为每一个职位面试一百多个人，建立一个分布。你开始了解谁处于这个分布的右侧极端。

事实证明，即使我们当时的知识相对来说还不算深入，因为我们已经花了数年时间去理解这个领域并与之亲近，但肯定还没有达到专家的水平。我们开始能够识别出那些出类拔萃的人，而这些人就是你要招聘并引入的。你会赋权于他们，给予他们正确的自主权。再加上我们重视的价值观。自零时起，Poolside 一直非常重视低姿态、善良、极其勤奋——在我们这个领域，你工作的时间太长了——以及极高的智力好奇心和能力，并且深深关心你交付的工作。

最近我们增加了一条，因为我们发现这一点非常真实：人们在沟通中非常明确和直接。或者说，沟通可以非常清晰。我们从这一点开始，并且从高层开始。我们找到了一个非常出色的招聘对象 Lance，他是我们的数据中心副总裁，然后我们从那里开始组建团队。

### 基础设施行业的特点

随着时间的推移，我学到的关于基础设施领域的一个特别有趣的事情是，这是一个极其小的圈子。每个人都认识彼此。这使得科技界看起来像一个大世界。建造大规模数据中心，实际上只有少数几个参与者真正做过。所以，在整个端到端的供应链中，每个人都认识彼此。因此，你也能非常迅速地了解谁被认为是好的参与者，谁被认为是坏的参与者。谁建立了声誉。小行业非常注重声誉。

所以，一切都始于人，赋权于他们，不断地向他们学习，知道在哪里信任，知道在哪里深入挖掘并提问，并试图挑战他们。这不仅让我们建立了一家拥有卓越团队的基础设施公司，还让我们找到了行业中那些能够开始重新思考数据中心建设方式的人。所以，我们还没有……

好的，这是根据您提供的英文视频字幕转录并整理的高质量中文文稿：

### 数据中心建设新模式

**嘉宾**：我们不仅仅是遵循大家都在做的传统模式。事实上，我们在数据中心建设方面采取了一种全新的方法。

**主持人**：听起来很棒，能详细说说吗？

**嘉宾**：我认为，行业内已经看到了 XAI 数据中心以创纪录的速度建成，这无疑为所有人设定了新的标杆。那么，你们是如何建造数据中心的呢？

### 传统与模块化之争

**嘉宾**：在数据中心建设方面，一直存在一种传统方式。我倒不认为我们现在采用的是“新”方式，因为它已经存在十多年了。

一方面，是所谓的“现场建造”（stick-built）模式。所有工作都在现场进行，所有材料都运到现场，成千上万的工人在现场组装数据中心。

另一方面，则是像西方的 Verdive 或中国的 Day One 及其母公司那样，采取了完全模块化的方式。你可以想象，数据中心单元就像工厂生产线上的产品一样直接运出。

### 两种模式的局限性

**嘉宾**：有趣的是，这两种方法都有其局限性。当成千上万的人员聚集在现场进行组装时，尤其是在西德克萨斯州这样实际上是沙漠的地区，会面临诸多挑战。

这包括劳动力调动、天气、扬尘、物流以及供应链等问题。

而工厂化生产模式在西方尚未达到一定的规模。我必须说，在中国情况有所不同，在那里，你可以在几年内实现吉瓦（gigawatts）级别的产能。

### 混合模式的创新

**嘉宾**：因此，我们决定采取折衷方案，即“兼顾双方”。我们选择现场建造主体建筑，就像一个非常长的走廊，两边附带房间。

同时，我们采用模块化方法，但这种模块化是能够装载到平板卡车上的。

### 模块化组件的实现

**嘉宾**：我们如何实现每台 2.5 兆瓦（megawatt）的计算单元滑橇（skids）呢？这涉及到电气、机械和冷却，以及计算本身。

这三个是数据中心内部的核心要素。我们选择与美国的制造伙伴合作来完成这些。

**嘉宾**：当你开始分解这个问题时，你会发现电气模块化组件可以由许多优秀的制造商完成，这并不算什么独特之处。

你确实有很棒的合作伙伴，包括一些非常大的公司，可以提供帮助。

当涉及到冷却和机械部分时，公司数量会少一些，但他们同样经验丰富，声誉卓著，可以合作。

### 计算与冷却的突破

**嘉宾**：至于计算部分，最近取得了一个突破。在计算与冷却的结合方面，我们现在有了直接芯片液冷（direct liquid to chip cooling）技术。

设计正在改变，你可以以一种更受控的方式完成更多工作。

### 增量交付的优势

**嘉宾**：因此，我们决定在每个独立的房间里增加更多的冗余，这样我们就可以逐步地、完全地将它们投入使用。

目前，许多为 AI 工厂建造的数据中心，通常以 40 兆瓦的批量投入使用。

我们则设想了一种设计，可以在建设过程中，就将 2.5 兆瓦的计算能力投入使用。

**嘉宾**：想象一下，在走廊里，你正在接入大约一千个 GPU（图形处理器），或者根据 GPU 类型，未来可能会更少。

你正在将这些投入使用。我们将这些结合起来，称之为“增量交付混合模块化数据中心”（incrementally delivered hybrid modular data centers），这可以说是兼顾了双方的优势。

### 降低风险与提升效率

**嘉宾**：这通常能降低交付的风险，也能降低计算能力的上线风险。

而且，这意味着如果下个月突然需要更多计算能力，你可以决定增加 2.5 兆瓦的容量，就像“我想要多一点”。

另一方面，你利用了这样一个事实：对于大型建设，比如建筑本身，你采用了传统方法。而在制造方面，你可以与多家制造商合作，而且可以在地理位置上获得优势。

我们运往现场的大部分组件实际上是在德克萨斯州或附近地区制造的。

**主持人**：这样一来，你们实际上也降低了供应链风险和现场交付的时间。

**嘉宾**：是的。而所有这些最大的优势不仅仅是速度，还包括你可以调动的劳动力。

### 偏远地区的优势与挑战

**嘉宾**：我们位于一个相当偏远的地区，这个地区提供了所有这些优势：几乎无限的土地，我们可以建造单层、水平延伸的建筑，以及巨大的能源供应。

但同时，偏远也意味着调动劳动力在那里更具挑战性。

所以，当我们在建造 250 兆瓦（megawatts）时，我们只需要不到 450 名现场工作人员，他们都是我们带来的、并安置在那里的技术精湛的工人和建筑工人。

这与传统上可能需要 2000 多人来建造相同规模的设施相比，有了巨大的不同。

### 速度与资本效率

**嘉宾**：这一切都是为了降低风险和提高速度。你可以看到一个共同的主题。

我们考虑的是交付周期（lead time），我们在模型建造现场也是这么做的。我们思考的是：从决策到投入使用，需要多快？

以及如何确保你投入的资本不必闲置 12 到 18 个月。因为这样，即使是小型公司也能迅速扩大规模。

**嘉宾**：大型项目之所以需要更长的准备时间，是因为这些决策需要提前数年做出。而我们可以提前数月做出决策。

**主持人**：那么，第一部分投入使用的大致目标日期是什么时候？

**嘉宾**：嗯，大概在第四季度初，或者第三季度末。

好的，这是根据您提供的英文视频字幕转录并整理的高质量中文文稿：

### 数据中心能源供应

**嘉宾**: 那么明年，我们就有首批计算能力上线。在 2027 年第一季度，我们将完成首个 250 兆瓦的建设。实际上，就在明年夏天，下一个 250 兆瓦的建设就将开始。这是一个分阶段的建设过程。这意味着我们每年都会有三次机会，每次新增 250 兆瓦的计算能力上线。

### 环境影响考量

**问**: 关于这个话题的最后一个问题，然后我们再转向更熟悉的软件和人工智能方面。数据中心的环境影响，这显然是整个数据中心行业都在努力解决的问题。你们是如何看待的？

**答**: 首先，我们得诚实地说。我们目前使用天然气来发电。在当前数据中心建设的规模下，太阳并不能全天照耀，风也不能持续吹拂。因此，仅靠可再生能源，如果不配备足够支撑数据中心运行的电池容量，目前在经济上是不可行的。

但是，天然气与可再生能源和电池相结合，我们正在建设一个大型电池系统。这能在中间找到一个最优的平衡点。

在谈到天然气发电时，你需要确保你的排放得到控制。为此，我们增加了选择性催化还原（SCRs）。SCRs 是一种催化还原技术，它能过滤掉天然气发电过程中产生的一些更具危害性的颗粒物。所有这些都符合联邦标准。我们会申请并获得联邦层面的空气许可证。作为企业，这是至关重要的一步。所以我们正在将这些方面结合起来。这是关于能源方面。

数据中心环境问题的第二个方面通常是水资源。水是一种稀缺资源，这取决于你所在的地理位置。在其他地方，它可能是一种丰富的资源，并可能影响当地社区。

我们非常幸运，这得益于我们的地理位置以及我们的合作伙伴拥有的大片土地。我们所需的水资源可以得到满足，而不会影响当地社区。在美国或其他地方，情况并非总是如此。因此，选择这个地点不仅是为了其巨大的扩展潜力，更是为了能够以一种负责任且对社区友好的方式实现规模化。

### 社区与长期发展

**嘉宾**: 超越环境话题，社区因素也极其重要。即使我们在一个相对偏远的地区建设，最终这一切都离不开当地社区的支持。我们所在的波科斯县（Pecos County）聚集着我遇到过的最棒的人们。说实话，如果你想去美国任何地方体验最棒的人们，就去波科斯县。我们对社区的欢迎程度感到非常惊讶。

这与我们如何投资于就业项目以惠及当地人民息息相关。我不想听起来像个政客，马特（Matt），因为当你谈论社区时，感觉就像是。但这些项目不是一次性的。当我们谈论这个大型多兆瓦项目时，我们计划在那里进行大约八年的建设。

而且，随着核能未来几年可能成为一种可行的选择，我们或许可以在那里建设更多的核能发电。这意味着那里的建设可能会持续到我们能想象到的最长久的时间，因为这里的空间几乎是无限的。因此，让所有人参与进来，一直是我们的一个关键点。我们努力成为一个负责任的企业，并就积极因素、负面外部效应以及如何缓解它们保持透明。

### 从代码执行到强化学习

**问**: 听起来非常迷人，而且与强化学习（RL）的世界截然不同。但既然我们几年前聊过，我们就来精确地切换到强化学习。你当时提到世界已经改变了，过去几年大家非常专注于预训练。现在感觉在 2025 年，预训练和强化学习的结合已经成为了许多人所追求的最新技术。能否介绍一下你们的方法？

**答**: 当我们两年前做这个播客时，我们谈到了我们从代码执行反馈中进行强化学习的方法。我记得当时我说，我们拥有数万个这样的环境，模型会在其中进行，它们会被赋予合成任务，探索解决方案，然后我们根据解决方案给予奖励。

**问**: 能否提醒一下听众，那是什么意思？所以你有一个完整的层级，里面有你构建的那些执行环境。

**答**: 完全正确。我们当时大约有 10,000 或 20,000 个代码库，也就是包含复杂软件的真实世界存储库。我们在其中定义任务，并且经常寻找一次性解决方案。也就是说，这里有一个任务，模型可以编写一些代码，然后进行测试，它可能会从奖励模型和其他方面获得额外的信号，并且能够提供反馈：代码是否正确，是否错误，风格是否得当等等。

随着时间的推移，这在两个方面都得到了极大的发展。一方面，现在我们拥有超过一百万个这样的环境，以及数千万个修订版本，总计达到了约 3000 万个。所以，在过去两年里，规模扩大了两个数量级。这是因为随着你增加环境类型的多样性……

好的，这是一份根据您提供的英文视频字幕转录并优化后的高质量中文文稿：

### 模型能力提升的两个维度

视频中提到，模型能力的提升主要体现在两个方面。首先，可以通过增加模型需要学习的问题数量来提升其能力。

其次，模型在推理能力和更长远的时间规划及执行能力方面取得了显著进步，例如多步骤的复杂规划和执行。这使得强化学习（Reinforcement Learning, RL）从单次性（singleshot）的强化学习，演进为**智能体强化学习（agentic RL）**。

### 智能体强化学习的应用

在智能体强化学习模式下，我们不再是给予模型一个简单的任务，而是赋予它一个更高层次的目标。然后，一个智能体（agent）会被派去，利用其工具（如编辑代码、执行命令、更新软件包等）来尝试解决问题，就像一名软件开发者所做的那样。

这种方式在今天的编码智能体（coding agents）中已经非常普遍，并且其工作方式看起来越来越接近我们人类的实际操作。智能体在执行任务的过程中也会获得相应的奖励。

### 从简单任务到复杂项目的转变

我们正进入一个新时代，模型的推理能力、长远规划和执行能力变得如此强大，以至于我们赋予它们的任务不再是教科书末尾的简单问题，而是类似于学校里要求完成的复杂项目。这些项目需要更多的步骤和更精细的执行。

我认为，目前大家都在沿着这些方向进行投入和发展。两年前，这还是一种不被广泛接受的观点，但如今已成为绝对的共识。然而，我们又一次发现自己处于一个新观点的萌芽期，这个观点可能与当前的主流趋势有所不同。

### 对未来趋势的独立思考

我们并非刻意追求“与众不同”，而是经过长时间的深入思考。我早在2016年就开始关注这个问题，当时我创立了Source公司，并首次尝试将长短期记忆网络（LSTMs）与语言模型进行强化学习的结合。在过去的两年半里，我们则专注于更大型的语言模型（LLMs）。

我们现在的看法是，世界正朝着一个我们可能并不完全认同的方向发展。

### 人工专家评估与模型能力提升

目前大家普遍关注的是，如何通过引入人类专家的评分标准（human expert rubrics）来扩展模型所处的环境。这不仅仅局限于编码领域，我们已经开始将其应用于更广泛的场景，例如在电子表格操作、化学家或营销专业人士的复杂任务中。

通过由专家制定的评分标准，模型可以用来评估其输出的答案，这被证明是非常有效的方式，能够显著提升模型在特定任务上的能力。我们自己也在实践这种方法。

### 商业价值与AGI的距离

在未来几年，我们可能会看到许多模型公司，包括我们自己，深入企业，挖掘高价值的应用场景。我们会利用智能体将任务完成度提升到60%或70%，然后结合领域专家的知识，通过额外的强化学习将其提升到80%、90%甚至100%。

然而，我们并不认为这种方法能够直接将我们带到通用人工智能（AGI）的阶段。我们认为，这种方式在AGI发展初期的“尴尬青春期”是很有价值的，因为它能为客户创造切实的经济效益。毕竟，我们是企业，需要为实现“奔向火星”的宏伟目标、达到甚至超越人类智能水平的使命提供经济支持。

但我们不认为，仅仅通过创建环境和引入专家来“孤立地收集技能”，就能够实现通用人工智能。

### 语言模型的启示与RL的未来

我们的观点是，强化学习（RL）将迎来一个类似语言模型发展的时刻。语言模型之所以如此迷人，是因为通过在网络上预测下一个词元（token），它拥有了一个通用的自监督目标。我们可以将整个互联网的数据喂给它，当然，还需要不断过滤和优化，例如使用合成数据等。这个过程迫使模型不断学习。

然而，这种方法之所以未能让我们达到AGI，是因为互联网本身并未包含创造它的“思考”和“行动”的数据集。它包含了最终的代码、最终的文章，但却没有产生这些内容时的思考过程和行动轨迹，没有那些试错的过程。

### RL的通用目标：反向工程

我们的观点是，存在一个通用的强化学习目标，它不需要人类的评判、外部环境或奖励模型，而是能够像“下一个词元预测”那样，在语言层面实现泛化。

可以这样理解：传统上，我们利用合成技术对网络数据进行重构和改进，相当于“正向生成”网络内容。那么，我们能否进行“反向生成”呢？换句话说，能否将网络反向工程（reverse engineer）或解压缩（decompress）成创造它时的思考和行动？是否存在这样一个通用的RL目标？

### RL²：新的研究方向

在过去一年半的时间里，我们一直在深入研究这个问题，并开始看到实现这一目标的巨大潜力。不过，目前我们还不能公开更多细节，因为研究仍处于需要大量验证的阶段。

**问**：这个方法有名字吗？
**答**：是的，我们称之为“**强化学习以学习（Reinforcement Learning to Learn, RL²）**”。这是我第一次公开提及这个名称。

**主持人**：为了让大家更好地理解，我来复述一下。目前有一种方法是强化学习，即创建环境，然后通过试错来获得奖励或惩罚。而强化学习的扩展路径是……

（字幕内容在此处中断）

好的，作为一名资深内容编辑和翻译专家，我将为您将这段英文视频字幕转录为高质量、易读的中文文稿。

---

### 训练模型的四个阶段

**主持人**: 我们可以将强化学习（reinforcement learning）扩展到各种不同的任务上。这是选项 A。您刚才说您正在研究选项 B，或者也许刚才那个是选项 A，而您正在研究的是另一种方法，即回归到互联网数据，逆向工程（reverse engineer）最初得出结论的思想过程，是这样吗？

**嘉宾**: 您对这个问题的描述非常到位。而且，我们认为这两种方法并非相互排斥。我们认为，模型应该尽早学会思考和推理，然后在环境中进行学习，以磨练其技能。这和我们人类很相似，对吧？我们大学毕业时学了很多东西，但真正进入职场后，我们通过实践来学习，从经验中获得成长。

因此，从环境中学习，也就是强化学习，从经验中学习，本质上就像一种可再生能源。这些标记（tokens）中的信息密度不如物理学书籍那样高，物理学书籍在极少的标记中蕴含着巨大的信息密度。经验中的信息密度较低，但价值极高。

所以，我们的观点是，通过在网络上进行预训练（pre-training）并预测下一个标记，可以极大地促进对语言的理解，并帮助我们达到一定的智能水平。我们内部将其称为“Bondi技术”，即强化学习（reinforcement learning）用于内部学习。我们认为这将使模型在训练的早期阶段就能达到比现在更高的推理和思考水平。

然后，还有通过代码执行反馈（code execution feedback）和其他经过验证的环境进行的强化学习，这有助于真正磨练技能，在模拟环境中进行学习。

最后，随着时间的推移，第四个训练阶段将越来越多地是这些智能体（agents）从真实世界经验中进行持续学习（continuous learning）。

### 持续学习的挑战与机遇

**主持人**: 您能否稍微详细地谈谈这个第四阶段？持续学习是人们可能听说过的一个概念，它一直被认为是人工智能系统进步的下一个重要方向。它是什么？它目前是如何运作的？在“Poolside”的背景下，它又是如何运作的？

**嘉宾**: 目前，基础模型（foundation models）有一个我们至今仍未充分优化的能力，那就是从单个数据样本中学习的能力。如今的基础模型可以做到这一点，但我们还没有找到一种真正能有效提升其能力的方法。我们内部称之为“烫伤的炉子”（hot stove problem）。如果一个孩子碰了一下烫伤的炉子，他以后就不会再碰了。一次样本学习就足够了，对吧？

基础模型由于其底层技术——梯度下降（gradient descent）——是一种非常需要大量数据的算法。它需要大量的样本才能在更高维度的空间中导航，找到更优化的解决方案。

因此，在通往通用人工智能（AGI）的道路上，我们面临一个巨大的挑战：如何让模型像我们人类一样，从少量经验中持续学习？

现在有一个问题是，达到AGI是否必须具备这种能力？我倾向于认为，如果我们对AGI的定义是基础模型能够胜任我们和你一样完成绝大多数经济上有价值的任务，首先是通过电脑，然后随着时间推移通过机器人实现。那么，我倾向于认为这可能不是必需的。

不过，它无疑会极大地增强智能，因为它将大大提高计算效率，并随着时间的推移增加其价值。

而我们已经有了一条实现这一目标的途径，那就是当有大量用户或真实世界对智能体或模型的行为提供反馈时。对我来说，模型和智能体基本上是相同的。我们可以将这些反馈整合到模型的改进中，这才是真正的从真实世界经验中学习。这发生在有成千上万甚至数百万个智能体轨迹（agent trajectories）——即完成的任务，并提供了一些反馈。这些反馈可以来自任务的发起者，也可以来自某种环境或系统。

这就像是从我们作为基础模型公司在集群中进行训练，转变为真正触及用户，然后这些反馈会回传。

那么，这对于实现AGI是必需的吗？我还不确定。这是一个非常坦诚的回答。但它在通往AGI的旅程中，能否让我们的模型更强大、更符合我们的期望，从而变得更有价值？绝对是。

### 智能的多维度性

**嘉宾**: 在这一切中，我开始有点在意使用“AGI”这个词，因为它更常用，但它似乎将智能（intelligence）视为一个单一的点，一旦我们到达那里，就包罗万象了。但智能是如此的多维度。有些人是杰出的作家，能创作出像陀思妥耶夫斯基那样令人惊叹的作品。另一方面，有些人是杰出的数学家，能够证明复杂的定理。还有些人非常擅长设计工厂。

而且，目前还不清楚所有这些是否都属于同一个单一的范畴。我们通过模态（modalities）非常清楚地看到了这一点，对吧？一个看不见的人仍然可以是一名出色的软件开发者，但不太可能成为一名出色的赛车手，对吧？或者说，不可能成为一名出色的赛车手。因此，智能……

---

好的，这是根据您提供的英文视频字幕转录并整理的高质量中文文稿：

### AGI 的多维度与技术挑战

智能（Intelligence）是一个多维度的概念，我们可能会达到一个点，即人工智能（AI）在处理我们通过笔记本电脑完成的所有知识性工作方面，能够达到与人类同等的水平。然而，即便如此，我们可能仍然无法就它是否是通用人工智能（AGI）达成一致，并且我们也尚未解决所有技术挑战。

但这也没关系。我认为，要真正实现人类所拥有的那些令人难以置信的、多功能的特质，我们还有很长的路要走。但是，在未来几年内，我们是否会达到一个点，即人工智能能够承担经济上具有价值的知识性工作？我认为答案是肯定的。

### 关于 Agent 与 AGI 的辩论

**问**: 您刚才提到模型（model）和智能体（agent）基本上是同一回事，这是一个很有趣的观点。我想将这个问题与当前的一些热门话题联系起来。几天前，有人说智能体（agents）还需要十年才能成熟。您对此有何看法？智能体目前在 AGI 的发展中处于什么位置？另外，有没有可能在不成为一家基础模型公司（foundation model company）的情况下构建智能体？

**答**: 我还没有听 karpathy 的那期访谈，但我非常想听，因为我进入这个领域很大程度上就是受到了 karpathy 的影响。早在 2015 年，Andrej Karpathy 就写了一篇题为《循环神经网络的非凡有效性》（The Unreasonable Effectiveness of Recurrent Neural Networks）的文章，这是一篇关于语言模型的博客。正是这篇博客促使我创办了我的公司 Poolside，并让我对语言模型及其能力产生了长达十年的痴迷。因此，我对他的观点怀有极大的敬意。

虽然我还没有听那期访谈，但我无法具体评论他说了什么。但我可以分享我的看法：目前，我们对智能体的定义是什么？它是一个在环境中运行的、能够访问一系列工具（tools）的模型，并且它在执行更长期的任务。我们是如何训练智能体的呢？作为基础模型公司，我们在训练智能体能力时，会将这个智能体——也就是那个在循环中运行并能访问某种容器（container）或一系列工具的二进制代码——与模型一起进行训练。

这就是为什么我们看到最强大的编码智能体（coding agents）往往出自那些与模型一起训练的团队。而且，这很可能适用于许多其他领域。但当你已经拥有一个在智能和能力上都能胜任智能体任务的模型时，它就不需要更多的智能，也不需要通过强化学习（reinforcement learning）进一步提升。那么，智能体的差异化体现在哪里呢？

智能体的差异化在于，构建它的人需要拥有某种形式的专有数据（proprietary data）、某种形式的专有环境（proprietary environment），或者其他能够让模型运行的循环比竞争对手更具优势的东西。这其中确实存在很多机会。

### 智能体与基础模型的竞争

**答**: 但我需要谨慎的地方在于，我们在编码领域已经看到了这一点，也许我们自己也体会到了。如果你决定创办一家公司来构建一个编码智能体，但你无法改进模型本身，也就是说，智能体无法与模型一起训练，而像我们这样的基础模型公司正在深度专注于此，那么你就没有优势。在这种情况下，你玩的猫鼠游戏会变得更加困难。

但这并非不可能。我见过一些为特定事物构建的、令人惊叹的智能体，因为没有人能够专注于所有事情，包括任何一家基础模型公司，包括我们自己。但在 Poolside，我们有一句格言：“随着时间的推移，一切都会内化到模型中。”（Over time, everything collapses into the models.）

我认为，这种情况越来越明显。两年前、三年前围绕的框架（frameworks）、智能体或产品，要么已经消失，要么它们的界面（UIs）已经极大地简化。过去，你会看到很多带有各种花哨功能、背后有复杂机制的产品，以及大量的用户界面选项。而今天，它们可能只是一个显示“Agent”的屏幕，你与之对话，它就去完成工作了。

### 你的核心优势是什么？

**答**: 因此，你必须问自己，你的定位是什么？你的“不公平优势”（unfair advantage）是什么？如果一家基础模型公司决定专注于这个领域，你的优势是否还能持续下去？还是会因为他们能够将模型训练得更进一步、更强大，并与他们的智能体相结合，而让你失去优势？

**问**: 有趣的是，我们从两三年前谈论的“薄包装”（thin wrappers）变成了去年的“厚包装”（thick wrappers），现在感觉在 2025 年到 2026 年，我们又开始重新思考智能体，它们似乎又变回了“薄包装”。这似乎是一个循环。

**答**: 我们可能都忽略了某些东西。也许我这样说不太恰当。我认为我们很难坚持一个观点，即模型将达到世界顶尖人才在各个领域所拥有的智能和能力水平。

当你退一步，不看未来 12 个月，而是看未来 36 个月（对于知识性工作而言），也许是未来 5 年（有些人可能会这么认为）。我认为今天很少有人会争辩说，这在未来十年内不会发生。而当你创业时，你不是为未来 5 年而建，你是为未来 20 年或 30 年而建，对吧？你是要建立一个能够持续存在并取得成功的公司，尤其是在风险投资支持的业务中。

在这种情况下，如果人工智能的能力与该领域最顶尖的人类一样，你的业务会是什么样子？这意味着某些东西仍然存在，比如我的 Uber Eats 应用仍然存在。

好的，这是根据您提供的英文视频字幕转录并整理的高质量中文文稿：

### 智能的局限性

存在的原因是我希望能够查看我的食物等等。但大量的垂直软件或垂直代理可能会消失。我认为，找出自己在那个世界中的位置至关重要。我认为我们所有人，包括我自己，都有一个倾向，就是过于关注眼前的事物，而没有退后一步，回归到那些宏大的议题上。我认为，宏大的议题在于，我们将达到那些能力水平，这将重塑世界，其影响远超我们今天所能想象的，甚至比电力出现之前和之后的影响还要深远。我认为，这一点甚至尚未被金融市场或我们运营业务的方式所完全消化。它正以一种我从未见过的指数级速度发展。

我们至今所创造的一切都源于智能。但现在，智能本身将是我们能够创造和扩展的东西，我认为之后的世界将截然不同。

### 对AI停滞论的回应

**问：** 那么，您如何看待那些认为AI进展实际上正在停滞的观点？

**答：** 坦白说，我的看法是一样的。自从创办Poolside以来，我两年半来一直面临这个问题。这不是我第一次在播客中听到“我们是否遇到了瓶颈？”的疑问。我认为，我们正处于一个持续的阶段，随着每一代新芯片的出现，我们都能构建更大的模型。这一点很重要，它与每一代新芯片联系在一起，因为这不是一个可以投入无限资金来扩展的世界。那是一个错误的叙事，你可以很容易地自己理解这一点。因为如果我考虑到模型的规模，它仍然是训练模型所需计算量的最大决定因素，这受到网络限制和芯片浮点运算能力的限制。

我并非可以通过线性增加GPU数量来训练越来越大的模型。如果我这样做，所需的时间将呈指数级增长。所以，如果明天有人说他们想训练一个拥有300万亿参数的模型，这完全超出了目前任何人在做的事情的范畴，无论你有多少钱，无论你投入多少芯片，这都是不可能的。训练它将变得指数级地昂贵且耗时。这是因为当前的硬件限制。但每两年（现在芯片周期甚至更短），我们就会迎来一款令人难以置信的新芯片和一套令人难以置信的新网络堆栈，这使得下一代模型尺寸成为可能。

我认为这不会无限期地扩展。我将智能视为一种压缩，你知道，在某个点上，你可以进一步压缩，或者至少压缩到不值得的程度。这是边际效益递减，但我们仍然有能力这样做。所以，一方面，我们享受着“免费午餐”——随着新一代芯片的问世，我们可以构建和训练更大的模型，而且这持续表明它提高了模型的性能。另一方面，我认为更有趣的是，我们现在正在进入扩展强化学习（Reinforcement Learning）的世界，我们可以用更多数据训练模型更长时间。因为我想要这个尺寸的轴，而另一件事是我们拥有持续时间轴——我们向模型展示了多少数据，而且每一年。

我们还通过利用现有数据使模型变得更强大，在更少的计算预算下实现了多倍的效率提升。看到这一点真是令人难以置信，即使我想到年初的预训练，或者我们现在正在进行的新的训练运行，与过去相比，你可以看到巨大的进步，因为你正在更好地优化数据，你正在创建更干净的数据，你正在创建更好的课程。我知道这是一个冗长的回答，但重要的是要理解，你不能仅仅无限地投入金钱来追求智能。它在某种程度上受到每一代芯片物理极限的约束。随着强化学习日益成为一个重要的扩展轴，我们能够在这些代际中进一步改进模型。但是，我们还没有找到一种通用的、无限可扩展的、可以投入到强化学习中的方法。因此，无论是增加模型规模还是通过强化学习增加数据，都仍然存在限制。如果这些限制不存在，Poolside就不可能追赶上来，对吧？我认为这是非常重要的一点，因为我们现在正处于一个阶段，我们应该花一些时间来研究模型。它们已经变得非常出色。我们现在正在扩展计算能力以达到前沿。如果仅仅是金钱的世界，这本不可能实现。但现在确实是一个地方，是的，我认为目前我没有看到任何限制。我看到的是机会，尤其是在我们进行的一些强化学习研究中，可以学习并可能完全打开新的扩展轴，使其能够走得更远。然后，随着每一代新芯片的问世，我们的行业将迎来一个阶梯式的增长。

### 软件在AGI竞赛中的作用

**问：** 您非常专注于软件开发。您如何看待“苦涩的教训”（Bitter Lesson）？这是你们会担心的事情吗？

**答：** 如果你回顾我们公司的第一篇播客或我们在零天发布的文章，我们一直说AGI（通用人工智能）的道路是通过软件实现的。它不是纯粹的软件。我们制定了一个三步走的宏伟计划：第一步是协助人们编码，这还处于非常早期的阶段。第二步是让任何人都能构建软件。我认为世界现在显然已经达到了这一步。然后是第三步，泛化到所有领域。我们现在正处于这个第三步的阶段。因此，我们今天的模型已经变得通用。

好的，这是一份根据您提供的英文视频字幕转录并编辑的高质量中文文稿：

### 聚焦软件开发，拓展知识工作

**嘉宾**：我们一直专注于软件开发能力，将其作为衡量智能的代理指标。在最初的几年里，这种专注让我们得以深入发展，并在我们认为重要的方面取得了显著进展。

**嘉宾**：当时我们认为真正缺失的是推理能力的提升，以及知识的改进。知识可以通过模型外部的途径来提升，例如为模型提供正确的信息来源。但复杂推理能力的提升才是当时所欠缺的。

**嘉宾**：因此，我们逐渐殊途同归。如今，我使用我们的模型来写作科幻小说，这很有趣。我的兄弟在他的增长营销工作中也使用我们的模型，并且比使用其他模型更喜欢它。

**嘉宾**：我们在这些领域已经取得了进展。而软件开发，尤其是针对企业和政府客户，仍然是知识工作中价值最高、成本驱动因素最大、影响也最深远的领域之一。虽然不是唯一的领域，但它是一个非常重要的领域。

**嘉宾**：因此，进入这些组织一直是一个令人惊叹的切入点。现在，我们正越来越多地拓展到其他领域。本周，我们与 Red Panda 公司达成了一项重要合作。我们非常欣赏他们，并已将他们超过 300 个企业级数据连接器集成到我们的产品中。

**嘉宾**：我们在纽约证券交易所展示的演示，是一个金融领域的反洗钱（AML）流程。我们的智能体（agents）与这些数据连接器协同工作，能够端到端地完成这项任务。因此，我们正逐步向软件开发之外的更广阔领域拓展。

**嘉宾**：正如我们一直所说的，我们的目标是赋能全球所有的知识工作。

### 产品与模型现状

**主持人**：在对话的最后部分，让我们深入探讨一下您刚才提到的内容。我相信大家可能对 Poolside 目前的实际情况很感兴趣，包括模型、产品和商业化方面。我们先从模型开始。你们的网站上有三个产品：Malibu、Point 和 Assistant。这些产品的当前状态是什么？它们各自的功能是什么？

**嘉宾**：好的。回溯到两年半前，Point 是我们最早推出的产品之一，它主要用于代码补全。如今，这已经成为标配。我们拥有这项技术，但智能的核心并不在于此。

**嘉宾**：我们的 Malibu 系列模型是我们的第一个大型模型家族。最初，Malibu 模型被设计为非常强大的编码助手。现在，它们已经演变成极其强大的编码智能体（agents）。而且，它们也成为了出色的知识工作智能体。

**嘉宾**：就其尺寸和性能而言，这些 Malibu 模型目前处于同类最佳水平。它们在编码方面已经变得非常强大，但尚未达到我所定义的“前沿”水平。

**嘉宾**：目前的前沿模型由 OpenAI、Anthropic、Google 和 XAI 等公司引领。这也是为什么我们正在部署大量的计算资源来扩展我们的模型规模。我们即将推出的模型家族是 Laguna，其中包括小型、中型和大型版本。

**嘉宾**：小型模型将在几周内完成训练。中型模型本周开始训练。而大型模型将在我们的 41,000 个 GB300 GPU 投入使用后开始训练。

**主持人**：你们有这些模型的基准测试数据吗？

**嘉宾**：我们确实有基准测试数据。我们主要与合作的商业客户私下分享这些数据。但以 Malibu 智能体为例，作为编码智能体，它目前的表现，例如在 Sweetbench 验证方面，达到了 Gemini 2.5 Pro 发布时的水平。

**嘉宾**：这是一个规模小得多的模型，但由于我们在强化学习方面的努力，它极大地提升了这些能力。

**主持人**：我们稍后应该会安排一个演示。事实上，我们将在几周后举行的 AI Engineer Summit 上在纽约公开演示，我们之前只对企业客户进行过私下演示。有兴趣的人可以前来观看，演示应该也会被录制。

### 市场策略与企业合作

**主持人**：从市场推广的角度来看，这是一个很有意思的思考。你们在过去几年里，一方面保持公开，另一方面又有些“隐身”状态。我们现在身处一个信息爆炸的时代，每个人都在争夺注意力。你们是如何平衡与企业合作的策略，以及让开发者能够使用你们产品的愿景的？

**嘉宾**：我们的观点其实非常简单：当我们在一个非常有价值的维度上明显处于最佳状态时，我们才会公开我们的模型。我认为这一点很重要，不仅仅是为了在 Twitter 上获得几周的热度然后就发布产品。你需要提供能够扩展且对世界有价值的东西。

**嘉宾**：在此之前，也就是在您提到的商业化部分，我们并未处于前沿。但现在我们正越来越接近前沿，这也为我们打开了市场。

**嘉宾**：在达到前沿之前，我们选择了一个别人无法涉足的市场：国防工业基地和政府部门。这是因为我们拥有企业级 DNA，愿意将我们完整的模型权重和整个技术栈，以及智能体，部署到客户需要的任何地方。

**嘉宾**：我们可以在安全级别（skiff）的工作站上运行，也可以在本地服务器上运行，甚至在隔离（airgapped）的网络环境中运行。我们也可以在 AWS 等商业云环境中，部署在私有 VPC 中。

好的，作为一位资深的内容编辑和翻译专家，我将为您将这段英文视频字幕转录为高质量、易读的中文文稿。

---

### 企业级AI的构建

我们也在更广泛的商业化和常规的云环境中运营，包括GVC云和绝密云等需要模型权重传输到客户那里的场景。我们这样做是因为，这不仅仅关乎模型本身，我们深知随着世界朝着代理（agents）在企业中承担日益繁重工作的方向发展，围绕模型构建一切将变得至关重要。

如今，我们可以列举出一长串我们长期以来构建的企业级功能，以确保这些代理和模型能够在复杂且受监管的环境中运行。这涵盖了从数据访问层，到代理的分配管理、基于角色的访问控制（role-based access control），再到与Active Directory等系统进行深度集成，以及所有与可观测性（observability）相关的监控和审计日志记录。

我们之所以在国防行业进行了这些实践，是因为该行业拥有庞大的组织、极高的复杂性和严格的监管要求，并且经常需要为不同的任务部署隔离的解决方案。因此，我们一直在该行业中进行规模化扩展。

现在，随着模型发展到我们认为“可以竞争”的阶段，我们正走向更广泛的企业市场。您会发现我们越来越多地出现在金融服务、工业和科技等大型企业的视野中。我们将我们的业务视为两种商业模式。一方面，我们希望公开我们的模型，让任何人都能使用。但我们这样做的前提是，我们认为我们拥有独特之处。

### 价值驱动的AI解决方案

而这正是计算规模化（scaling up of compute）的必要之处。Laguna 系列模型将是公开的。另一方面，我们提供极具附加值的业务。在此，我们从产品起步。

我们最初推出了一个可以集成在VS Code和IntelliJ中的编码助手，现在也支持独立使用。随后，我们推出了您期望的、能够与数据源连接并进行模型交互的Web界面。但我们反复发现一个现象：许多企业和组织拥有可以通过AI解决的、极具价值的问题。甚至在很多情况下，仅凭两年前或一年前的AI能力就能解决，但他们并未成功实现。

这很大程度上源于项目意图与实际落地之间的差距，包括整合正确的数据源、进行有效的上下文工程（context engineering），以及通常所需的额外强化学习（reinforcement learning）。

### 前沿研究工程师的价值

为此，我们开始建立一支强大的前沿部署（forward deployed）团队。在Poolside，我们有前Palantir的员工加入，他们真正将那种DNA融入其中——即识别高价值问题，并以高度主动性帮助客户解决问题的DNA。

您甚至有称之为FDR的团队，即“Forward Deployed Research Engineers”。我们发现，在传统的“前沿部署工程”（forward deployed engineering）领域，即专注于高价值问题、整合数据源并构建界面的工作，存在技能上的差距。这本身就是一项了不起的工作，我对此深表敬意。

而研究工程（research engineering）领域则更有趣，因为我们寻找的是那些同时具备与模型和代理（agents）高效协作的经验和天赋，并且在必要时能够进行额外强化学习的人才。因此，我们将这些高能力的研发工程师派驻到客户内部。

### 全球化布局与人才战略

这是您将看到我们不断加强的战略。这也是我们下周将在英国开设办事处的原因之一。我们还将在纽约开设办事处，以真正加强前沿部署研究工程（forward deployed research engineering）的模式。

关于地域问题，你们最初似乎主要在欧洲起步，但过去几年，公司似乎在很大程度上重新聚焦于美国。这种看法是否准确？如果是，是什么驱动了这种转变？

我们一直是一家美国公司。从第一天起，我们就已在美国注册并总部设在美国。而且，在任何一个时间点，我们的人员构成比例总是在40/60或60/40之间波动。因此，将公司打造成一家全球性公司对我们至关重要。

但我们早期就做出了一个决定——我想我们在两年前的播客中也谈过——那就是在湾区以外招聘研究人员，最初尤其集中在欧洲。即使今天，如果我们看Poolside的研究人员分布，他们绝大多数（95%）都不在硅谷。

这为我们提供了巨大的机遇。它让我们有机会找到那些高能力、高积极性的人才，他们不受湾区“回音室效应”的影响。湾区是世界上最有价值的回音室之一。我个人很喜欢那里，也花了很多时间在那里。但您已经看到，并且可能也意识到，我们所做的一些事情与那个“回音室”的普遍看法是相当背离的。我认为，我们身处其外并持续保持这种状态，在一定程度上促成了这一点。

因此，我们招聘的范围遍及全美，遍及欧洲。随着我们规模的扩大，我们也在亚洲招聘越来越多的人才。

---

好的，这是为您转录和编辑后的中文文稿：

### 避免回声室效应

我们一直并会继续努力，尽可能地避免陷入“回声室效应”。因为我认为，我们目前仍处于这个领域的非常早期阶段，通往通用人工智能（AGI）的道路，理应由各种不同观点和背景的人共同构建，而不是只有一种声音。

### 全球人才的发现

我们发现，世界上并不缺乏才华横溢的人才。只是他们的才能可能不像在硅谷那样，在简历或领英（LinkedIn）上那么显眼。

### Poolside 未来展望

**问**: 宏观来看，未来 12 到 18 个月，Poolside 将会有哪些动态？我们能期待什么？

**答**: 模型将达到技术前沿。我认为目前我们正朝着这个方向稳步前进。您会看到物理基础设施的规模化，这既能支持训练更大、更强大的模型，也能将它们服务于全世界。

您会看到我们将在全球范围内，在越来越多的企业中部署前沿研究工程师。您会看到我们将继续朝着我们的使命迈进。对我来说，我们绝不能忘记，我们正在构建这家公司及其相关的经济引擎，因为我们相信，在这一切之后的世界，将是一个可以创造大量富足的世界。

这种富足既来自于即将发生的科学进步，也来自于商品和服务的成本最终取决于其生产成本的事实。当我们把智能转向计算（Intelligence to Compute）时，我们可以进入一个能够降低成本的世界。

回顾过去 100 年，我认为没有哪个时刻比今天更值得我们去生活。这就是我们的核心使命，一直以来都是如此。您将看到更多这方面的进展。我很期待 12 个月后能再次来到这里，与您分享我们的成就。

**问**: 非常期待。Eiso，很高兴能与您交流。非常感谢您抽出宝贵时间。恭喜你们在过去几年取得的所有成就。非常期待看到数据中心和那些新模型。感谢您与我们共度时光。

**答**: 我也很感激。

**主持人**: 谢谢你，Matt。

**主持人**: 大家好，我是 Matt Turk。感谢收听本期 MAD Podcast。如果您喜欢这期节目，如果您还没有订阅，我们非常感谢您考虑订阅，或者在您收听或观看节目的平台上留下积极的评价或评论。这真的能帮助我们发展播客，并邀请到很棒的嘉宾。谢谢，我们下期节目再见。

---

## 英文原文

I would actually go as far as saying that intelligence is going to become a commodity. If you're a foundation model company and you're not building physical infrastructure, you're cosplaying your business. Project Horizon is us building out one of the largest data center

complexes in the United States. We're talking about a scale of compute. It gets counted in the hundreds of megawatts and soon gigawatts. So we call it reinforcement learning to learn RL to L. This is the first time I'm probably publicly saying this.

>> Hi, I'm Matt Turk from First Mark. Welcome to the Mad Podcast. Today my guest is Iso Clant, a co-founder of Pullside. Poolside is a foundation model lab company focused on software engineering that's currently reported to be raising a $2 billion round at a $14

billion valuation, including a reported $1 billion check from Nvidia. We dig into Poolside's ambitious project horizon, an AI factory data center at multi- gigawatt scale, and why AI labs must own energy, compute, and intelligence. ISO also unveils

reinforcement learning to learn, a new path that goes beyond pre-training and classic RL. Please enjoy this deeply insightful conversation with ISO. >> ISO, welcome. >> Good to be here, Matt. >> I should say welcome back. You and I

chatted almost 2 years ago now when you guys were just getting started. >> I was actually listening back to our podcast and I hadn't realized that it was the very first podcast we had done as poolside and yeah, two years have flown by. Fast forward to today at the

end of 2025 the race between Frontier AGI Labs has only gotten crazier frothier. So where do you guys stand?

What is the reason for Poolside to exist in a world of hyper competition with other labs? So it comes back to why we started the company. Right? So if if you go back to early 2023 when we started Poolside, we started it because we had our own point of view on the research.

So if you remember early in earlier in that year, you know, GPT4 had just come out and the narrative in the world was all we have to do to reach AGI is scale up language models, do more next token prediction, a larger number of parameters and more data. And we agree

with the importance of scale and till the date still do. But our point of view was that reinforcement learning was going to become the most important scaling access for model capabilities. Extremely contrarian opinion at that point still when we were doing our

podcast as well and probably for the following, you know, 12 months as well. Today that's of course very different. uh today I think that's become consensus and I think the world has has understood that we now have an ability to continue to scale models towards more and more

capable like direction and really close the gap between human intelligence and AI and and that mission was the original mission of poolside is still very much the mission of poolside but along the way I think what we had predicted would happen played out we were going to

continue to scale up more compute was going to be required but at the same time the first paradigm of kind of pre-training of predicting the next token on the web was becoming sigmoidal and was slowing down in terms of the gains that it had. So over the last 2

and 1/2 years we caught up on the pre-training side and really on like the what we refer to as the table stakes of foundation model building and language modeling while building some pretty serious advantages on the reinforcement learning side and and now we're at a

moment where those two things have come together. All of us like as Frontier companies are you know developing our big model runs are still at the same scale. So there's series of advantages that we've built in our research.

There's advantages that we've built on our engineering and those are probably some of the most important sustained ones. Uh but now we're getting to a point where scaling up our models to frontier sizes uh is also necessary for us. And hence we've recently had some

announcements about the sheer scale of compute that's coming online. And so we're competing like we're we're in this race. Uh I think I I listened back to to our podcast and I said, you know, we're going after open eye and entropic.

That's still very much true. But our starting point was a little bit different, right? We said, you know, to get to highly capable intelligence that can reason, that can do planning, that can understand the world, you're almost better off putting a set of blinders and

focusing on on one proxy for that. And for us, that proxy was software development. And as we've gotten more and more capable at doing longer horizon software development tasks, we've also gotten more and more capable at frankly all type of knowledge work tasks. the

world from a model perspective. I think we are all over time converging to a similar point. I would actually go as far as saying that intelligence is going to become a commodity. In that world, who we want to be is we want to be trusted by by enterprises. We want to be

trusted by businesses to to power the knowledge workforce that started with coding agents and is now going beyond that as well. Uh and I think in the space, you know, so much is going to transition, right? We're like a pre and post electricity moment. So I don't

think it's a winner takes all market but it does seem to be that it's a small number of companies who are who are able to get there. >> As part of that race towards AGI you've had uh some very big news in the last couple of weeks. First of all there is a

rumored large fund raise up to $2 billion where Nvidia reportedly would be investing up to a billion dollars at 12 billion pre 14 billion post. It's a very large round that seems to be very tied to another big news that you guys did formally announce which is Project

Horizon. What is Project Horizon?

>> Yeah. So, so Project Horizon is us building out one of the largest data center complexes in the United States. And this comes back to something I said earlier. We we talked about intelligence becoming a commodity, right? Our our view for pretty much since starting the

company over 2 and a half years has been that there's three layers of the stack that fundamentally are going to matter. It's energy, it's comput, and it's the intelligence built on top. And within this world, if you think that intelligence is going to become less

distinguishable between the companies building it and becomes a commodity, a commodity probably more like oil or cloud comput like bread at the bakery, is there's two things that matter. Your ability to scale it and the cost at which you deliver it to your end user.

Now the ability to scale it was frankly the primary motivating driver for project horizon but cost as well and it's important to think about what you could do 12 months ago versus what you could do today at the scale of compute that we were talking about you know 2

years ago in our industry you could call up a data center colo and say hey I want this much compute in 6 months and there would be space like physical space where you could deploy it or there'd be someone who has the capacity available to you today we're talking about a scale

of compute that gets counted in the hundreds of megawatts and soon gigawatt but there's no one you can call >> right these are built to suit data centers that are so large that no one is building them before having a tenant and so as you're approaching the frontier

and your models are getting more capable and you want to serve that intelligence to the world and you want to scale up your training as well over time now your lead time from deciding to do that to being able to do that is no longer calling up a hyperscaler or calling up

another partner and having it, you know, in months. Now, we're talking 12 months, 14 months, 18 months with huge capital numbers attached to it. >> And so, my co-founders say this thing to each other and probably not really for for podcast material, but it's it's a if

if you're a foundation model company and you're not building physical infrastructure, you're cosplaying your business. >> Mhm. >> And and this is kind of the the really fundamental nature. It's not that we

went out and said it'll be cool to build infrastructure. It was a necessity for the mission to be able to achieve it. So you think you just get boxed out, right?

So obviously OpenAI is a whole target project and they just announced like I think a fourth data center that they're about to build. Enthropic has a special relationship with AWS. I think AWS is building one for them. That's I guess that's what you were saying. You need a

tenant in that case. The scal is willing to do it because they have anthropic as a tenant. So as an indie lab for lack of a better term like you have to own your own destiny and that it involves building your own data center right just to play it back.

>> Yeah. I think as a foundation model company you can go two paths right you can choose to to deeply partner with a hyperscaler and kind of you know become uh you know have them become an owner in you and and really go all the way uh and I think that's one direction but at the

same time the world is getting to a point where I don't think anyone has any doubts anymore that we're now on track to to reach human level capabilities and and intelligence and in that world $29 trillion of knowledge work rewrites itself right scientific progress starts

pushing beyond levels that we've ever seen And all of it is intelligence on compute. And so we are far more in a in a foundation model company is far more in a physical infrastructure business than most people realize because the

ability to scale that compute, right, and and bring it to end users and frankly do so cost effectively, right?

The cost of your tokens are going to matter more and more. as our as our intelligences all become closer to each other, the ability to scale this up and and do so to end users where you know the cents per token are kind of a determinant if someone's going to buy it

is is critical. So we already started several years ago uh asking ourselves the question like what would it take to move towards this and then over time you know we learned more we observed more and and then we started acting towards it and and the acting towards it is

project horizon. So project horizon in a nutshell is today announced a 2 gawatt campus we can actually go far beyond 2 gawatts on the site that we're in. It's a partnership with an incredible family out of Texas called the Mitchells. The Mitchells have one of the single largest

parcels of land in the United States. is half a million acres. >> And it always blows my mind because for context, LA is 300,000 acres. And on that land is renewables being built. There's grid connection. There's water, but there's also a 20-in main gas

line. And that offers us a possibility to start scaling up energy that powers data centers incredibly fast and incredibly large scale. >> Mhm. And so what you're seeing from us is that before the end of this year, we're starting construction on a 250

megawatt data center. It's natural gas powered uh with grid connection as a backup and it will house an incredible amount of compute, but it will be the first phase of many that we're building out. >> Great. So just to unpack some of the

things you said, you mentioned the impact of owning your physical infrastructure on the economics of Frontier AI labs. to help us understand understand that obviously one key question in the world of AI has been gross margin where do you think owning

your own uh physical infrastructure lead you towards in terms of like overall structure >> I'm not yet convinced that gross margins in our industry will look like a SAS company like 80% plus I think when we're talking about um a commodity as

intelligence that we build value added services on top right as a foundation model company on one hand you you sell your tokens right your barrels of oil, your raw material. And on the other hand, you're building up value added services, your products that you bring

to end users and customers that you know unlock value for their businesses or for consumers in the case of others. And in that world, I think it looks a lot more like a cloud company. So I think when we're going to look at the scale of this, uh, it's going to look like cloud

companies with a zero behind that. from a margin perspective probably pretty similar because if you think about what is an AWS or GCP or an Azure it's effectively virtualization of hardware with services on top. I don't think foundation models are are that

different. So I think from a margin profile will probably sit far closer towards that like 40% mark that you see in in cloud companies than you do like the 80% on the software side. And now when you start thinking about what sits in the stack of cost of intelligence,

you effectively you've got the land, um you've got the energy, you have the data center, you have the chips inside the data center and and just for context, if we for instance break this down with illustrative numbers that are roughly in the right ballpark, uh to energize 250

megawatt is about half a billion dollars worth of physical infrastructure. Mhm. >> So in the case of gas turbines, it's gas turbines, but there's different paths to it. Building a data center like the power shell that brings together all of the equipment where the compute can run

inside, you're talking kind of north of $2 billion, $2 to $2.5 billion. Now the compute that goes inside today would be about $5.5 billion. So kind of all in all, when we're like adding this together, you're talking >> being the chips,

>> being the chips and the networking and and and everything that kind of is what we'd refer to as the fit out of of a data center. And so you got about an $8 billion like cost of 250 megawatt. So when you hear in the in the industry you might say a gigawatt is 40 or $50

billion. That's what people are referring to, right? It's it's the breakdown of those costs. Now the life of a chip has a certain amount of years to it. Uh and in our industry there's there's lots of you know argumentation about how long they last and it's a

demand and supply question. A data center has kind of traditionally, you know, a 15-year life, but it needs retrofitting as as new generations of chips come online. And your power infrastructure can last a very long time, but requires maintenance.

>> Mhm. >> Now, in that $8 billion project of 250 megawatt, you're annually spending somewhere north of 300 to $350 million of opex. And so, you can split that about 50/50 between the cost of financing and the cost of energy and

operations. energy like being the most. And this is where there's already an interesting insight. All of a sudden, you realize that the energy part today is not the biggest part of the stack, right? You're talking $160 million a year in energy. And of course, we're I'm

taking numbers that are West Texas numbers. So energy can be, you know, double that in other places in the country, but still compared to the total, you know, cost TCO of of of compute, uh, it's relatively, you know, minor. Uh, numbers are still very big.

Now the reason I I mentioned is that what is going to happen over time right like what's always happened in capitalism is that margins compress everywhere now margins have already compressed for energy we already you know try to make energy as cheaply as

possible in the world data centers effectively have already been increasingly more commoditized over the last 20 30 years GPU compute today is still very high margin business and over time we'll find a similar level of compression

and as the chips become more compressed the other parts of the stack are more important as well. And when you end to end own all of this and build all of this, everywhere where previously margin sat in between, you know, falls away.

>> If that's the person who you were paying for, you know, the cost of land, if you're talking about the cost of like the energy, the cost of the building, the data center, kind of all the layers in the stack. This adds up to a very large amount because the foundation

model itself is, you know, it has capex for building it. that's your R&D cost and you you know you advertise it over time. Uh but the physical infrastructure is really frankly where the majority of your cost sits and so I know I went super detailed straight away but I think

it's useful for people to kind of get this sense of where it is. So when you take all those margins out all of a sudden you can start seeing that you can serve your tokens 20 30 40% cheaper than someone else. And this is going to matter because it's we're shifting from,

you know, human labor that leads to economic output to intelligence on compute combined with human labor. And we've barely started this in the world. And so we're going to be in a world where this is a commodity that will look far larger than we've seen cloud compute

be. Frankly, is often the largest bills of companies, right? What they're what they're paying in that world. Yeah. Controlling that cost is a big impact. But cost is really only part of the answer here.

You have to be able to scale it. You have to be able to say if I next quarter need more compute, can I bring it online? Because there's no shortage of chips. There's just a clip yesterday from Satcha uh on a podcast that I saw where he said, "Look, it's it's energy

and and data centers that are my bottleneck. It's not chips." And it's true, right? Nvidia does an incredible job at supplying the market with chips. and and TSMC and everything in the stack and scale up to to huge levels but physical space bringing power online and

building data centers well that's when we kind of in tech all of us start realizing the real world hits us >> and you mentioned cost of energy and cost of financing uh just quickly this is finance how this is project finance kind of a setup

>> exactly so so if you think about traditional data center business and this is not unlike it you you have an amount of equity that goes into that business you have a loan to cost ratio and the loan own to cost ratio really depends on the lease, the tenant that

you have on that data center. You got a 15-year lease contract traditionally and and then it's traditional project financing uh that is highly effective. The world has been doing this for a long time. It is relatively, you know, low rate financing. Uh and uh this is not

new. This is not you know new to tech. It's been around for a long time. >> Part of the idea is that Pside will be the anchor tenant but you'll be renting the facility out to others as well. what is going to be the revenue stream.

>> We are not yet at a place where we can be signing 15-year leases on massive multi-billion dollar, you know, like buildouts. Uh and so, but we are ramping up rapidly where we see a path to being able to scale up compute uh at those levels. And so, we found a really great

partnership here with Cororeweave that was announced. Corewave frankly is second to none in terms of operating uh Nvidia's compute, right? first have GB200s online at scale uh now coming with the GB300s and and have been this great partner like for us in in the

space and so we found a really great hybrid solution with them. So they are the anchor tenant on our data center. Jointly we can scale up our compute inside that data center and any capacity that we choose not to take or not able to take in our space. Uh they can bring

to other customers >> and and this was really an important thing because we're we're not a hyperscaler who can make you know multi-billion dollar bets for two or three years out. So, we needed to find our way of of having great partnerships

that were kind of win-win situations where we could scale up our compute but didn't need to make a lead time decision years ahead of it. And and this has been a fantastic setup because uh it kind of brings the best of both worlds to to to the site. as you explain all of this

fascinating in terms of ambition. It's also fascinating in terms of like what that means for ultimately a young software AI company to become you know a major physical instructor company. So the core partnership helps but presumably you have to hire all sorts of

people that's a whole different skill set. How did you think about that and um also the the financial aspect of it?

Yeah, we followed an algorithm. I would probably say most of my career by now. Uh at least I remember going this back as far as 9 years is you have to by all accounts avoid the Dunning Krueger effect as a founder, right? Uh because when you get into something new like it

has been on the physical infrastructure side, you start with learning, right?

You start with reading, you start with like meeting the experts, learning and learning and as you learn more about a topic, you fall in this point where you start thinking you really know it. But you don't really know it because you haven't hit the real world or you

haven't done it before. And what I found to be one of the best advice I got a very long time ago whenever you find yourself in that situation, well, this is when you want to start finding the experts to join you. But how do you find the experts? Because you yourself

aren't. And here's when you just start interviewing 100 plus people for every role and you build a distribution. You start understanding who sits on the right tail end outlier of that distribution. And it turns out, you know, even just with relatively I would

I wouldn't say our knowledge at this point was shallow because we'd spent years like, you know, understanding the space and uh getting close to it, but with definitely not the level of like an expert, we got to the point where we started being able to identify who were

the outliers and those are the people you hire and you bring on, right? And you empower, you give the right autonomy to and that in combination with the values that we care about. So at Poolside since day zero, we've always cared about low ego, kind-hearted,

extremely hardworking, like in our space, you work way too many hours, a lot of intellectual curiosity and horsepower, and deeply care about the work that you deliver. >> Uh, and we've recently added a six because we found that's really been true

is like people are very explicit and direct in their communications, right?

Or it can be very clear >> and and so we started with that and you start from the top, right? uh we found an incredible hire in Lance who's our VP of data centers uh and and from there we've been building out the team. What was particularly interesting to learn

about the infrastructure space over time is that it's an incredibly small world. Everybody knows each other. It makes tech feel like a big world. Uh right, the the construction of data centers at scale has really only been done by quite small number of players. And so

everybody knows each other across the entire like endto-end supply chain. So you also have an ability to very quickly understand who's considered a good actor and who's considered a bad actor, right?

Who has built up a reputation. Small industries are very reputation driven. And so it all starts with people um empowering them, learning from them like continuously and and knowing where you know you you trust and knowing where you go deep and you ask questions and and

try to challenge people. And that has actually not just led us to building a an infrastructure company with an incredible team, but also to find the people in the industry who were able to start rethinking some of the ways data centers have been built. So we haven't

just gone and said, "Oh, we're doing the exact traditional thing everyone else is doing." We've actually taken a quite new approach uh to data center construction as well. >> That sounds great. Unpack that. So I think the industry has seen the XAI data

center being built in record speed which I think has reset the bar for for everyone. How do you guys go about building that data center? So if you think about data centers, there's kind of been a traditional way of building them and I wouldn't say new way because

it's been around for for over a decade. But on one end you have kind of stickuilt buildings. Everything is done on site, right? So all the materials are brought there. Thousands of tradesmen who who are assembling the data center and on the other hand of kind of like

Verdive here in uh in the west or players like Day One and their parent company out of China who've gone full modular. to think of like your your data center units are rolling out of a factory. And what was interesting in in that both of those approaches uh have

their limitations, right? The moment you're bringing thousands of people to site uh and you're assembling everything and and especially in what is effectively the Peran basin, the desert, right, in in West Texas comes with its challenges both on mobilizing

workforces, but also the weather, the dust, uh the logistics, the supply chain. But then the factory side has not yet scaled up at a level in the west. I would say this is different in China uh where you can actually you know bring out gigawatts like over the course of

you know like couple of years uh and so we we kind of decided to meet in the middle uh and took an approach where we said we're building the actual building stick built so that's being built on site uh think of it as an extremely long corridor uh with leaves attached to it

but let's take a modular approach but one that fits on the back of a flatbed truck. So how do we kind of do 2 and a half megawatt computeworthy skids for think of this for electrical for mechanical and cooling and for compute.

Those are kind of the three elements that sit inside a data center and do it with manufacturing partners here in the United States. And what you start finding here is and once you start breaking the problem down you realize that the electrical modular components

can be done by lots of great manufacturers. This is not unique. This is so you actually have great players including very large ones uh that can help you there. when you can kind of get to the cooling and mechanical part. uh it's a smaller number of companies but

again have been doing this for a long time highly reputable uh and you can work with and then when it gets to compute side there's recently been an unlock uh because on the compute mixed with cooling side what you're finding is that now we have you know direct liquid

to chip cooling the designs are changing there's a lot more that you can do in a far more contained manner and so we decided we said we'd rather have a little bit more redundancy on every individual kind of room that we build so that we can bring them incrementally

completely online. So where most of the data center is being built for AI factories today, you'll see them bringing them online at kind of 40 megawatt chunks. We said what if you designed it in a way where you could bring 2.5 megawatts of compute online,

you know, as you're doing construction. Think of it again on this hallway as you're bringing on like units of kind of a thousand GPUs or depending on the GPU type can be less in the future. uh you're bringing this online and the combination of those things that we

refer to as incrementally delivered hybrid modular data centers is kind of bringing the best of both worlds. It usually derisks your ability to deliver. Uh it drisks your ability to bring compute online. It also means that if all of a sudden next month you need more

compute, you can make decisions on 2 and a half megawatts like I just want a little bit more. On the other hand, you're taking advantage of the fact that for the kind of bigger construction, the building and stuff, you take the traditional approach and on the

manufacturing side, you can partner with multiple manufacturers and you can do so actually very geographically advantageous as well. Most of the things that we are bringing to our site are actually manufactured in Texas or very nearby.

>> And so now you're actually reducing as well like your supply chain risk and your time like to sight risk. The biggest advantage though of all of this is not just speed, but it's also your workforce that you can mobilize because we're in a quite remote area and that

remote area offers all of these advantages, right? Like pretty much infinite land so we can, you know, build a single level horizontally uh an incredible amount of source of energy, but also being remote meaning that mobilizing workforces there is more

challenging. So all of a sudden, you know, when we're building 250 megawws, we're doing it with a less than 450 person on-site workforce with very skilled tradesmen and and and and construction workers that were bringing over there and we're housing over there

versus a world where you might need traditionally 2,000 plus people to build the exact same thing. So it's all about de-risking and speed. And you kind of see a common theme. We think about lead time and we do the same by the way on our model building site. We think you

know how can quickly can you go from decision to having something online uh and how can you make sure the capital that you have to deploy does not have to sit for 12 to 18 months because that's when you as a smaller company can all of a sudden scale up right because the big

numbers really have more to do with the fact that these decisions need to be made years ahead of time we can make them months ahead of time >> what's the target delivery date for like the first part to come online >> so um at the beginning of Q4 end of Q3

so next year we've got the first compute coming online uh and then we're finalizing in Q1 2027 the first uh you know 250 megawatt but already next summer the next 250 megawatt starts construction it's a staggered build uh and this means that we will always have

a horizon on you know essentially three times a year an additional 250 megawatts coming online >> last question on this and then we'll transition over to the more familiar software and AI side of the conversation the uh environmental impact that's

obvious viously a question that the whole data center industry grapples with. How do you guys think about it?

>> Let's first be honest about this, right?

We're using natural gas for the generation of power and in the world that we are right now at the scale that data centers are being built out. Uh the sun, you know, doesn't shine all day, the wind doesn't blow all the time. And so renewables only in combination with

the battery capacity that you would need to power a data center if you weren't renewable only is just not economically viable yet. But the combination of like natural gas uh with renewables with batteries. We've got a big best being built out uh on the site. So a big

battery system. Uh allows for kind of an an optimal point in the middle. Uh now when you talk about natural gas generation, you want to make sure that you have your emissions in check. So what you're adding is SCTRs. Uh and so you're an SCR is essentially cataly

reduction. So what you're doing is you are filtering out some of the more harmful particles that come out of natural gas generation and you do this all within federal standards. You you file for air permits that you have that are approved by like at the federal

level. And so this is a critical thing to do as a as a business. And so you're you're combining those things. So that's on the energy side. The second side of environmental concerns around data centers are usually water, right? Water is a depending on where you are scarce

resource. in other places. It's an abundant resource. It can impact like a local community. And so we're very privileged because of where we sit and the sheer scale of land that our partners have. Water is a both a resource that is there at the levels

that's required for us without actually taking away from like the local community. And this is not the case everywhere else in the United States or in the world. So picking this site was not just about like how large it can scale because it can scale into

incredibly large size but also can you do so in a way that's you know responsible and community what goes beyond the environmental topic is incredibly important right you're building something out even though we are building out in a relatively remote

area at the end of the day this would not be possible without the support that we get from the local community so we're in a place called POS county which is just some of the best people that I've ever met like it's I honestly like if you want to go and and just walk into a

bar somewhere and spend time with some of like the greatest people in the United States, go to Pekos County because we've been amazed with like how welcoming like the community has been. And so this goes hand inhand with how do we invest in you know job programs to

bring people there. I don't mean to start sounding like a politician Matt because I think this is almost when they talk about community it feels like you are but these are you know these are projects that are not one-off right when we talk about this big multi-gawatt

project. We've got about 8 years worth of construction that we are planning out there. Uh and potentially as nuclear, you know, becomes a viable option in the coming years and we can build out more power potentially in nuclear over there.

This can be construction that happens out there for as long as we can imagine because the space is close as infinite as you're going to get. And so bringing kind of everyone along uh has been kind of a critical like point for us. We just try to be good actors and try to be

transparent about you know the positives and also the negative externalities and how to mitigate them. >> Fascinating and very different from the world of RL. But let's precisely um switch to RL now since we chatted a couple of years ago. You mentioned that

the world has changed and um everybody was very pre-training focused uh for the last few years and sort of feels like in 2025 the combination of pre-training and RL now has become sort of like the the both the state-of-the-art in what what a lot of people do. Walk us through your

approach. So when we did this podcast two years ago, you know, we spoke about our approach of of reinforcement learning from code execution feedback. Yes. I think at the time I called out we had tens of thousands of these environments where the models would go

in they would be given synthetic tasks explore solutions and then we rewarded on the solutions that >> and maybe remind people what what that is. So you have a whole layer where you have those uh execution environment that you built.

>> Exactly. Yeah. So, so what we have done is uh at the time it was about 10,000 or uh I believe 20,000 code bases like real world repositories that you know have complex software in them where we define tasks and we would look for often singleshot solutions. So here's a task

model can write some code it gets tested it might have additional signal from reward models and others in there and it will be able to provide you know feedback was the code correct was it wrong was it rightly styled etc. uh over time that has grown a lot across two

axis. One is right now we have over a million of those environments uh and tens of millions of revisions putting us in like the 30 million range. So just yeah two orders of magnitude larger over the last 2 years and that's because as you increase the diversity of the type

of problems that the models need to learn in you can increase the capabilities of the models. The second axis is that the models got so capable in their reasoning and longer time horizon capabilities multi-step like complex planning and execution that RL

moved from kind of singleshot reinforcement learning to agentic RL. So meaning that now we give a much higher level task. We send in an agent that agent goes and you know tries to solve for the problem by using its tools. So editing code, executing commands,

updating packages, like doing everything that a software developer would do. And when you see it, you know, you see it today in coding agents, it looks even quite natural to how we do things, right? Increasingly more like how we do it. And then it gets rewarded along the

way as well. So what we're seeing is that we're entering a world where the reasoning capabilities of models and their longerterm horizon planning and execution is now getting so capable that the tasks we provide them are no longer simple tasks like the questions at the

end of a textbook but now it's like the projects that you you know you're made to do in school uh where you have to do a lot more steps and so we've continued to scale on those axis today I believe everyone is scaling amongst those axes um and it's true where 2 years ago this

was a contrarian opinion that no one was doing to date is absolutely consensus where we have again found ourselves in a point where we now have again a contrarian opinion on the future. Uh and it's really not by by trying to be contrarian. It's just that we I think

have had a pretty had a couple of you know a long time thinking about this. Right. I started thinking about this first in 2016 when I was building source and did the first RL with with LSTMs with language models and now the last two and a half years with with these

larger LLMs and our view now has become that the world is going in a direction again that we actually might not fully agree with on so what you're hearing a lot about at the moment is the scaling up of environments with human expert rubrics.

So it's not just going into coding where we've got verified rewards. Now we're entering into areas if that's you know how to operate on a spreadsheet or if that's how to do a complicated task as a as a chemist or you know marketing uh professional where you use kind of a

rubric created by an expert uh to make it part of what a model can use to judge the answers and it's highly effective uh and by the way we do it as well. It is a highly effective way to to get models singular capabilities on singular tasks uh to push it up. And I think what we'll

see in the next couple of years is a lot of model companies ourselves as well. We go into these enterprises. We find high value use cases. We use our agents to get to 60 or 70%, you know, of of quality and then use additional reinforcement learning with, you know,

domain experts knowledge to get it to 80 or 90 or potentially even 100%. But the narrative that that's going to scale us to AGI, yeah, we don't agree with. We think this is the right thing to do in the awkward teenage years ahead of AGI. It's because

it allows you to create really economically valuable outcomes for your customers. At the end of the day, you know, we're we're we're businesses, right? We we need to be able to fund our economic engine towards our mission to Mars, towards reaching, you know, human

level intelligence and beyond. But the singularly collecting of skills this way by creating environments and experts, we don't think will scale to artificial general intelligence. Our view is that reinforcement learning will have a moment similar like we've seen in

language modeling. So, what makes language modeling so beautiful? It's the fact that by predicting the next token on the web, you have this general self-supervised objective, right? You just can throw the entire internet at it and of course continue to filter it into

more quality and improve it with synthetic, all the things we spoke about last time, but it forces the model to learn. But the reason that never got us to AGI is because well, the internet never actually included the data set of the thoughts and actions that created

it. >> It's the final piece of code. It's the final article you write uh but not the thoughts that you had and actions that led to it. >> Not the trial and error. >> Exactly. Uh and so our view is that

there is an generalized objective for reinforcement learning that doesn't require you know human judges or environments or reward models or anything external but can generalize over language. Uh the same way that we have seen you know next token prediction

generalize. A way of thinking about it is if you have traditionally we've taken the web and we've used synthetic techniques to reformulate it to improve it. We've moved forward like we taken the web and generated forward. What if you could generate backwards? So what I

mean by that is what if you could reverse engineer the web or decompress the web into the thoughts and actions that created it. Is there such a general objective for RL that can be found? This is where we've done a lot of our research in the last year and a half and

we have now started seeing a lot of promise towards that being possible. Not yet too ready yet to talk more openly about it because we're still at a stage of the research where we've got a lot of things to prove out.

>> Does it have a name as an approach or >> Yeah. So we call it reinforcement learning to learn RL. >> Okay. >> This is the first time I'm probably publicly saying this. Uh and so >> and just to play it back and make sure

that it's understandable by everyone. So there's one approach which is reinforcement learning where you create environments and reinforcement learning does trial and error uh gets rewarded not rewarded and the path to scaling reinforcement learning is to just uh

expand reinforcement learning to all sort of different tasks. So that's option A. What you're saying is that you you're working on an option B or maybe that's option A and the first one was option uh B but you're working on a on a different approach where you're going

back to internet data reverse engineer the the thoughts that went in reaching that conclusion in the first place is that >> it's a perfect way of framing it and and we don't think they're mutually exclusive right we think that you want

to have a model learn how to think and reason as early as possible in its training and then you want to actually have it learn in the environments to sharpen its skill sets, right? And it's not unlike us, right? Like it's we'll have learned a lot coming out of

university, but then we're put in the job and we're actually doing it and we're learning from experiences. So the learning from experiences, uh the reinforcement learning from environments, right, and from experience is effectively like a renewable energy.

The density of information in those tokens is not as high as like in a physics book is, right, where huge amount of density of like within a small amount of tokens. In experience, it's less, but it's highly valuable. So our view is is that you know we pre-training

and predicting the next token on the web is an incredible bootstrap of of understanding language and and helping us get you know to a level of intelligence. Reinforcement learning to learn RL internally also refer to as the Bondi techniques. It's kind of our our

our code name for it. We think will push models to a level of reasoning and thought that will happen far earlier in their training than it does today. And then you have reinforcement learning from code execution feedback and and from other verified environments that

help learn really what is you know to sharpen those skill sets in simulated environments. And then the fourth stage of training over time increasingly will become learning continuous learning from real world experiences of these agents.

And so those are kind of the four stages that we think training will go to. If you will talk about uh that fourth stage a little bit like continuous learning is something that people may have heard about that keeps coming back as one of the next avenues for the whole AI

systems to progress. What is it and how does that work currently and how does that work in the context of poolside >> today? There there's one thing about foundation models that we have yet to really optimize almost at all like over all of this time which is the ability to

learn from a single you know sample of data. Foundation models today can do it and we don't yet have a path in in them successfully doing so in a way that like really improves it. Internally we call it the hot stove problem. If you're a kid and you touch a hot stove once,

you're never touching it again. Single sample and you're good, right?

Foundation models because of the the underlying technology gradient descent, right, is it's such a it's such a data hungry algorithm. It requires so many samples to be able to navigate that higher dimensionality space to a place where it does something more optimal.

And so we've got this big hole towards AGI of like how can we get a model to learn continuously from a smaller number of experiences like you and I can. Now there's a question if that's required to reach AGI. I would actually tend to say that if we take the definition of AGI to

be that you know foundation models are as capable as you and I to do the vast majority of economically valuable tasks maybe first behind a laptop and over time embodied in robotics. I would tend to say that it might not be necessary.

Uh but it of course is an incredible thing to add to intelligence because it will massively make it more compute efficient and will also make it more valuable over time. And what we do have already a path towards is when we've got a large number of users or the real

world giving feedback on what an agent is doing, what a model is doing. Models and agents to me are effectively the same thing. we can incorporate that feedback in improving the models and and this is really the learning from real world experiences. This is when you've

got you know maybe hundreds of thousands or millions of agent trajectories right tasks that were done where some feedback was provided and that can be by the final by the person who instigated it or can also be by some form of environment or system. And so it's kind of taking it

from in our clusters as foundation model companies to where it actually touches the users uh and that feedback will come back. And so is that necessary to reach AGI?

Not so sure yet. It's a very honest answer. But will it be valuable uh in the journey towards it to make our models more capable and and more directed to what they want them to be?

Like absolutely. In all of this, you know, I have a bit of an issue started to use the term more because it's just more commonly used, you know, AGI, but it treats intelligence. We often treat it as this like singular spot that once we're there, it encompasses everything.

But intelligence is so multi-dimensional, >> right? You've got people are incredible creative writers who can create, you know, incredible works of art at DStoski. On the other hand, you've got incredible mathematicians who can prove

themselves. on our hands. You have incredible people who are, you know, amazing at designing a factory. And it is not yet obvious that that is all a singular thing, right? There's uh and and we see this very clearly with modalities, right? Someone who is not

able to see can still be an incredible software developer, but it's unlikely to be able to be an incredible race car driver, >> right? Or unlikely, it's called impossible to be an incredible race car driver. And so intelligence

is so multi-dimensional that we can get to a point where it's going to be as capable of doing all the knowledge work that we do behind a laptop and we're still not going to agree that it's AGI and we still won't have solved every technical challenge. And that's also

okay. Like I think we've got a long road ahead of us to truly get to the incredible like versatile like features that we have as humans. But will we get to a point in the next couple of years where economically valuable knowledge work can be done by I? I think

absolutely. >> You just mentioned something interesting about the model and the agent being more or less the same thing. So to to weave that question into the you know current sort of debate topics a few days ago said that agents were 10

years away. So what is your take on where we are with agents as it relates to AGI and um is there an opportunity to build agents without being a foundation model company? So I haven't yet listened to the kopathy interview and I really want to because I don't think you

noticed but I got into the space because of karapathy. So in 2015, Andreopathy wrote an article called the unreasonable effectiveness of recurrent neural networks, a blog post about language models. And that singular blog post let me start my company sourced and got me

down this obsession for now a decade of like what you know language models and and can do. And so I have an immense amount of respect for for his opinion. And so while I haven't listened yet to the interview, I can't comment specifically on on what he said. But

what I can say is this, right? What is today the definition that we use of an agent? It's a model running in a loop inside an environment with access to a set of tools, right? And it's and it's doing longer trajectory work. And how are we training agents, right? Agent

capabilities as foundation model companies is that we take that agent, right, the binary, the piece of code that that runs this in a loop and has access to could be a container or could be access to a bunch of tools and we train it together with the model. And so

this is why you see the most capable coding agents, right, really coming out of people who are training with the models because and this by the way will likely hold true for lots of domains. But when you already have a model that

has all of the intelligence and all of the capabilities to do an agentic task, right? So it doesn't require more intelligence. It doesn't need to be pushed further in like reinforcement learning. Well, then the question, what's the differentiation in the agent?

Well, differentiation in the agent is whoever is building it needs to have access to some form of proprietary data, some form of proprietary environment, something that allows that, you know, that loop that the model is running in to to be, you know, more advantageous

than another competitor. And there's lots of opportunity there. But what I am careful about and we see this in coding, right? Maybe we see this ourselves is if you decide to start a company to build a coding agent where you are not able to improve the model, right? The agent is

not able to train together with the model and foundation model companies like like ourselves are deeply focused on doing so. You you don't have that edge and so the cat-and- mouse game that you're playing becomes a lot more difficult. It's not impossible. I've

seen incredible agents being built for specific things because no one can singly focus on everything. Not any foundation model company and including ourselves. But we have a phrase at poolside which is over time everything collapses into the models. And I think

increasingly that's what we're seeing. Frameworks or agents or products that were 2 years ago you know around today are have you know either gone or they've sometimes their UIs have collapsed right? So you used to see these products with lots of bells and whistles and lots

of things behind the scenes and you would see lots of UI options and today it's just like a screen that says agent and you talk to it and it goes off and does the work >> and so I think you have to ask yourself the question like where do you sit like

what's your unfair advantage that you have and if a foundation model company decides to focus on it will that advantage you know still sustain or will it fall away because they're able to train the models to be further and and more capable in combination with their

agent and yours. >> Yeah, it's it's funny how we went from talking about thin rappers 2 years ago, 3 years ago, to thick rappers last year, and it feels like in 2025 going into 2026, we're starting to think about agents that are really thin rappers.

Again, it's it's a constant. >> We're all ignoring something, right? And it's maybe it's the wrong way of putting it. Uh, I think we have a hard time holding the point of view that models will reach the same level of intelligence and capabilities that the

world's most capable people in every field have. And when you take a step back and don't take the next 12 month view, but just take the next I think it's in the next 36 months for knowledge work. Maybe it's the next someone else might think it's the next 5 years. I

think few people today would argue that it's not going to happen in the next decade. And when you're building a business, you're not building it for the next 5 years. you're building it for the next 20 or 30, right? You're building a company that can sustain and and be at

the level of success, at least particularly when it's venturebacked businesses. In that world, what does your business look like if AI is as capable as the most capable person in the field? And that means certain things still exist like my Uber Eats app still

exists because I want to look at my food and etc. But a whole bunch of vertical software or vertical agents are probably gone. And finding out where you sit in that world, I think, is is critical. I think we have a tendency all of us and and and myself included to get so caught

up in what's right in front of us that we don't take a step back and and come back to the big things and and I think the big things really here are is that we are going to reach those levels of capabilities and it rewrites the world really like that pre and post

electricity moment far more than we I think have even factored in today. I think not even in the financial markets or not even in the way that we operate our businesses. It's uh it's truly living on an exponential to a point that I don't think we have

ever seen before. >> Everything we've created till now was because of intelligence. But now intelligence itself is something we will be able to create and scale and I think it becomes a very different world afterwards.

>> So what do you tell people that argue that AI progress is actually plateauing?

>> The frankly the same thing. I've I've had this question for two and a half years now since starting poolside. This is not the first podcast where you know there was a are we hitting a wall? We are I think at a point where we are continuing to see

with every new generation of chips an ability to make the model larger. Uh and it's important that this links to every new generation of chips because it is not a world where you can throw infinite dollars at scaling. It's that's a false narrative and you can think of

it very easily yourself because if I take the size of a model uh which is still effectively the biggest determinant of how much compute it takes to train it because of the limitations on the networking and because of the limitations on the flops on those chips.

It is not that I can linearly add more GPUs and train increasingly a more larger model. If I do so the time it takes becomes exponentially longer. So if tomorrow someone said I wanted to train a 300 trillion parameter model completely out of the realm of anything

that anyone's doing today, it just wouldn't be possible. No matter how much money you have, no matter how many chips that you put up, it would become exponentially more expensive and and longer to train that. So and this is because of the current hardware

limitations. But every 2 years and now the chip cycle is even getting less. We have an incredible new chip and an incredible new networking stack that is improving that all of a sudden makes the next generation of size model possible.

Now I don't think this will scale infinitely. Uh I look at intelligence as compression and you know at some point you're you're you can compress further uh or at least not to a point where it's worth it. It's diminishing returns but we still have an ability to do that. So

on one hand we have a a free lunch like as as new generations of chips come out we can build and train larger models uh and it as continues to show that it improves the model capabilities on the other hand and I think far more interesting as we're now getting into

the world of scaling reinforcement learning we're able to train models for longer duration with more data right because I want to have this the size axis and the other thing we have to duration axis how much data we're showing the model and every single year.

We've also become multiples more efficient with the data we had to make models become more capable in a lesser compute budget. It's kind of mind-blowing to see where even if I think about our pre-training at the beginning of the year or we have a new

run that's that's at going on at the moment versus now like you can really see incredible improvements because you're refining the data better. You're creating it cleaner. You're creating a better curriculum. I know there's a long-winded answer to your question, but

I think it's important to understand that it is not an infinite dollar you can just keep throwing intelligence. It's kind of bounded by physics of every chip generation. As reinforcement learning is becoming increasingly more scaling axis, we are able to improve

models within those generations far more. But we have not yet found a generalized infinitely scalable, you know, dollars we can throw at reinforcement learning either. And so both on increasing model size and on increasing data from RL, there's still

limitations. And if those limitations didn't exist, Pulside wouldn't have been able to catch up, right? I think this is a really important part because we we're now at a point where we're starting like we should definitely spend some time with the models after this. They've

gotten really darn good. We're now scaling up compute to make it to the frontier. That wouldn't have been possible if it was just a world of dollars. But it is definitely a place where yeah I think right now I don't see any limitations. I see more than

limitations. I see opportunities in some of the research we're doing in reinforcement learning to learn and others that might completely open open up new scaling access that can go even further and then every generation of chip that comes out we'll see a stepwise

function that goes up in our industry. >> You're very focused on software development. Uh how do you think about the bitter lesson? Is that something that you guys worry about?

>> If you go back to our first podcast or we put a post on our website on day zero of the company, we always said the path for AGR runs through software. It is not software. And we laid out this three-step master plan which was step one assist people in coding. It's kind

of very early days. Step two, allow people, anyone to build software. I think the world is clearly there right now. And then step three, generalize to all domains. And we're now really in that step three moment. And so already our models today have become generally

capable across the board. But because we had kept our blinders on for those first couple of years on really software development capabilities as a proxy for intelligence, it allowed us to go really far allowed us to push the things that mattered and our view. What was really

missing was pushing reasoning like improving knowledge. You can improve knowledge outside the model, right? You can give it access to the right sources of information, but improving this kind of complex reasoning is what was missing. So we're kind of converged on

the same point. So today I use our models to the other day I had to write a sci-fi book while I was reading it which was a lot of fun right uh and uh my brother uses it in his in his growth marketing job and and prefers it over you know using uh using other models and

so we're already in that domain now but software development is still with our deep focus on enterprise and and government often the most highest value like value or highest driver of cost and also impact of knowledge work in a lot of businesses. It's not the only one,

but it's a big one. So, it's been an amazing place to enter into organizations with and and now we're starting to increasingly go beyond that. So, we had a big announcement this week with company called Red Panda. We're big fans of and we just integrated their 300

plus enterprise data connectors inside it. And the demo that we were showing at the New York Stock Exchange was one of an AML process in finance that our agents are endtoend together with these data connectors like doing. And so we're we're starting to open up much more

broader outside of software development and come back to what we've always said like we want to be able to to power all of knowledge work in the world. >> For the last part of this conversation, let's uh go back down in the weeds and talk about some of the stuff that you

just uh mentioned. I think I and and and people may be curious about the current reality of of poolside both from a model standpoint, product standpoint and commercial. So starting with the model. So you have you have three products on your website. You have a Malibu, you

have a point, and then you have assistant. >> What's the current status of those products and what do what do they do product models?

>> Yeah. So if you go back 2 and a half years ago, right? One of the first things was point which was like the code completion models. >> Today they're table stakes. We have them, but it's not where the intelligence sits, right? So our Malibu

was our first big family of models. And so within the Malibu models, they were really oriented towards originally to be very capable coding assistants. Now they've become incredibly capable coding agents. Uh and now they've also become incredible knowledge work agents. And so

within that uh those models are right now for their their size and weight class, best-in-class. They have become incredibly capable coding, but they're not yet at what I would define as the frontier. So frontier today is OpenAI, Anthropic, Google, and XAI. And that's

why we have all of this compute coming online to scale up those model sizes. That's our upcoming family of models, Laguna, where we have a small, a medium, and a large. So, small is finishing training in a couple of weeks. Medium actually starts training this week. Uh,

and and large starts training as our 41,000 GB300s come online. And >> you have benchmarks for those. Do you have benchmark?

>> We do have benchmarks. Yeah. So, so if you think about our benchmarks today, um, and we primarily share them privately with the organizations, the commercial part that we work with, but if you think about the Malibu agent as a coding agent for instance, right now, it

sits at a level of like Sweetbench verified for instance, where Gemini 2 and a half pro was when it came out. Uh, and so it's a much smaller model. Uh, but it's really pushed those capabilities because of our work in reinforcement learning.

>> And so we should do a a demo after. I think we're actually doing a demo in New York uh publicly because we've only done it privately with enterprises in a couple of weeks at the AI engineer summit. >> Uh and so if anybody's interested, they

can come and see it there and I think that will be recorded as well. >> It's an interesting uh thought from a go to market standpoint because you you guys have been both public but kind of stealth in a way for the last couple of years and we're in a world of just like

massive noise and like everybody's like struggling for attention. How do you think about about that tension working with enterprises on the one hand but like on the other hand like everybody's uh you know all developers want those products to be in their hands. So our

view has been quite simple actually is that we will make models publicly available when we are clearly best on on a very valuable axis right and I think that's that's important it's not just about having a hype on Twitter for a couple of weeks and and having something

out you need to bring something in that can scale and that is valuable to the world so before and this gets to your commercial part of the question we weren't at the frontier and now we've are getting increasingly closer and closer and uh and that's al also opened

up our market. So before we were at the frontier, we picked a market where no one else could go, which was the defense industrial base and government because one of the things that we're willing to do uh kind of from the enterprise DNA we have is take our entire model weights

and the full stack all the way with the agents anywhere the customer needs it to be. So we today operate on workstations that go into skiffs. We operate on servers that are you know onrem or even as far as airgapped. We operate in commercial cloud like AWS in private VPC

environments. Uh we also oper in more commercial just cloud regular environments and then on GVC cloud top secret cloud kind of the places where the the weights have to travel to the customer. And we did this because it wasn't just about the model is because

we knew that as the world was moving towards agents doing increasingly more work in the enterprise it was going to become incredibly important to build out everything around the model. So today we can rattle off you know a very long list of enterprise features that we've built

over time so that these agents and models can actually operate in complex regulated environments. And so that goes everything from a data access layer towards the managing of assigning of agents across role-based access control deep integrations with all of the kind

of active directory-l like systems that people have all of the monitoring audit logging like that associates with observability you know go on and on and we battle tested that in the defense industry because they're extremely large organizations they're highly complex

they're highly regulated and they often need segregated deployments for different missions that that they're on and so we've been scaling that up in that industry. Now, as the models have gotten to a point where we say, "Okay, now we feel that we can compete, we're

going to wider enterprise." And so, you'll see us increasingly more showing up in kind of the large enterprise names amongst financial services, industrials, like technology companies. And we treat our business as kind of two business models. On one hand, we want to make our

models publicly available and allow anyone to use them. But we want to do so when we say, "Hey, we have something here that sets us apart." >> Mhm. And that's the scaling up of comput is necessary for that.

>> Laguna the Laguna family will be public. And then we have the part of business where we go very value ad. And here we started with product. >> So we started with a coding assistant that sits in VS code and IntelliJ now also sits outside of it. Then we started

with kind of the web interfaces that you expect uh to be able to chat and use the models you know connected with data sources. But we found something over and over again, which was that a lot of these enterprises and organizations, they have incredibly high valuable

problems that can be solved with AI today. Hell, in many cases with AI capabilities of 2 years ago or a year ago, but they're not successfully doing so. Uh and a lot of that has to do with the gap between what the intent of the project is versus being able to bring it

all together, the right data sources, the right context engineering, often additional reinforcement learning that's needed. And here we started building up a very strong forward deployed motion. At Poolside, we have former Palunteerians and who came over and

really kind of instilled some of that DNA, the DNA of what it means to find a high value problem and come in with high agency and help a customer solve that. Yeah. >> You even have like FD R you call them FDR.

FD is for chumps like you forward deployed research >> engineers. Yeah. And and look what we found is that there's a there is a gap between the skill set of doing traditional forward deployed engineering right where you're focused on uh a high

value problem and using kind of piping data sources together and and building you know interfaces on top which is by the way incredible thing. I have a lot of respect for everyone who has done this. The research engineering side is interesting because what you're looking

for is the people in that who also have the experience and the natural tendency to work very well with models and with agents and can who work on additional reinforcement learning if it's necessary. And so we're taking these highly capable research engineers and

we're putting them inside our customers. And that's for us something you're going to see us scale up increasingly more. It's why we've uh going to be opening an office in the UK actually next week. Uh we're going to be opening an office here in New York and really kind of scaling

up that motion of forward deployed research engineering. >> On that topic of geography uh you guys started mostly in Europe I believe but it feels like over the last couple of years you've recentered the company in large part towards the US. Is that a

fair sentiment? And if so what drove it?

>> Yeah so we've always been an American company. We've been incorporated headquartered US since day zero and and I think at any given moment the balance of people will have always sat somewhere between 4060 or 6040 one way or another like throughout the life of the company.

So building it as a global company was critical for us. But the decision we made early on and I think we spoke about it on the podcast 2 years ago is to hire researchers outside of the Bay Area and particularly originally very centered on Europe. And still today if we look at

where like researchers for pool sites sit they sit predominantly you know 95% outside of Silicon Valley. And this offered an incredible opportunity for us. It offered the opportunity to find highly capable like highly motivated people who were not in the echo chamber

that is the Bay Area. It's one of the most valuable echo chambers in the world. I I personally love it. I I spent a fair amount of time there. But you've seen and probably realize that there's a bunch of things we've done that were quite contrarian to the belief of of

that echo chamber. And I think that was partially unlocked by the fact that we sat outside of it and continue to do so. So we hire all across the United States. We hire all across Europe. Uh we're hiring increasingly more in Asia as well. Uh as we're scaling up and so but

we have and continue to to try to avoid as much of an echo chamber as possible because I think the world we're very early in all of this still and the path towards AGI is is one that deserves deserves to be built by opinions and people that aren't all the same. And we

have just found that there is no shortage of incredible talent uh in the world. it just might not be as obvious on a on a CV or a LinkedIn as it is in the Bay Area. >> So, zooming out the next 12 to 18 months, what happens at Poolside? What

can we expect?

>> Models reach the frontier that that's uh that's right now I think we see a straight line towards this. Uh you'll see the the scaling up of physical infrastructure to both empower training even more larger and capable models but also serving them to the world. uh

you'll see us uh have forward deployed research engineers and increasingly larger number of enterprises uh globally you'll see us just continue to work towards the mission right I mean for for us we never want to lose sight of the fact that we are building this company

and and and the economic engine associated with it because we think that the world that lives after this is one where a lot of abundance can be created abundance through scientific progress that's going to happen but also abundance through the fact that costing

of goods and services ultimately are dependent on the cost of what it takes to create them. And as we shift intelligence to compute, we can find ourselves in a world where we can drive that cost down. And if you look back at the last 100 years, there's no moment

that I think any of us rather live than today, right? And so that's it core mission. It always has been. And you'll you'll see more of that. Uh and uh yeah, looking forward to probably being here in 12 months to to tell you about what happened.

>> Looking forward to it. Uh ISO great to catch up. Thank you so much. Congrats on everything uh that you guys have accomplished in the last couple of years. Excited to see the data center and like the new models. And thank you for spending time with us.

>> Appreciate it. >> Thank you, Matt. >> Hi, it's Matt Turk again. Thanks for listening to this episode of the MAD podcast. If you enjoyed it, we'd be very grateful if you would consider subscribing if you haven't already, or

leaving a positive review or comment on whichever platform you're watching this or listening to this episode from. This really helps us build a podcast and get great guests. Thanks, and see you at the next episode.
